\chapter{Solver implementations}
    In this chapter we'll discuss various implementation aspects of the \emph{ab
    initio} solvers discussed in \autoref{chap:hf} through \autoref{chap:ci} and
    \autoref{chap:cc}.

    \section{Hartree-Fock}
        In \autoref{sec:hf} we showed how the variational principle of the
        Hartree-Fock ansatz led to the canonical Hartree-Fock equations, where
        the molecular orbitals are the primary unknowns.
        Here we will demonstrate how we can find solvable equations for the
        molecular orbitals starting from an initial basis of atomic orbitals.
        We will in the following demonstrate three different procedures related
        to the restrictions put on the spin-orbitals as discussed in
        \autoref{subsec:restrictions-on-spin-orbitals}.
        First we'll discuss a general Hartree-Fock method which puts no
        restrictions on the molecular orbitals.
        This method leads to the molecular orbitals being described as general
        spin-orbitals as shown in \autoref{eq:general-spin-orbital}.
        The second method is known as the \emph{restricted Hartree-Fock} method
        as it limits the molecuar orbitals to restricted spin-orbitals as
        in \autoref{eq:restricted-spin-orbital}.
        Finally, we'll demonstrate the \emph{unrestricted Hartree-Fock method}
        yielding unrestricted spin-orbitals for the molecular orbitals as
        shown in \autoref{eq:unrestricted-spin-orbital}.

        \subsection{Hartree-Fock with general spin-orbitals}
            \label{subsec:ghf}
            Given a basis of $K$ known non-orthonormal atomic orbitals
            $\brac{\chi_{\alpha}}$, we wish to find an orthonomal basis of $L$
            molecular orbitals $\brac{\phi_{p}}$ satisfying the canonical
            Hartree-Fock equations.
            We can transform from the known atomic orbital basis to the unknown
            molecular orbital basis by
            \begin{align}
                \ket*{\phi_p} = C_{\alpha p}\ket{\chi_{\alpha}},
            \end{align}
            where $\vfg{C} \in \mathbb{C}^{K\times L}$ is now our unknown
            coefficient matrix.
            The orthonormality condition of the molecular orbitals can now be
            formulated as
            \begin{align}
                \braket*{\phi_p}{\phi_q}
                &= C^{*}_{\alpha p} C_{\beta q}
                \braket*{\chi_{\alpha}}{\chi_{\beta}}
                = C^{*}_{\alpha p} \overlapten_{\alpha \beta}
                C_{\beta q}
                = \delta_{pq},
            \end{align}
            where $\overlapten_{\alpha\beta}$ is the matrix elements of the
            overlap matrix $\overlapmat \in \mathbb{C}^{K\times K}$ of the
            atomic orbitals.
            In the case of an orthonormal basis of atomic orbitals, the overlap
            matrix $\overlapmat \in \mathbb{C}^{K \times K}$ reduces to the identity
            matrix.
            By left-projecting with a state from our atomic orbital basis onto
            the canonical Hartree-Fock equations, we obtain
            \begin{gather}
                \mel*{\chi_{\alpha}}{\fock}{\phi_q}
                = \epsilon_{q} \braket*{\chi_{\alpha}}{\phi_q}
                \implies
                \fockten_{\alpha\beta}
                C_{\beta q}
                = \epsilon_q C_{\beta q} \overlapten_{\alpha \beta}
                \implies
                \fockmat \vfg{C} = \overlapmat \vfg{C} \vfg{\epsilon},
                \label{eq:roothan-hall-general}
            \end{gather}
            where we've denoted the matrix elements of the Fock operator in the
            atomic orbital basis by
            \begin{align}
                \mel*{\chi_{\alpha}}{\fock}{\chi_{\beta}}
                \equiv \fockten_{\alpha \beta},
                \label{eq:ghf-fock-mel}
            \end{align}
            and where $\fockmat \in \mathbb{C}^{K \times K}$ is the \emph{Fock
            matrix} consisting of the matrix elements defined in
            \autoref{eq:ghf-fock-mel}.
            The diagonal matrix $\vfg{\epsilon} = \diag(\epsilon_1, \dots,
            \epsilon_L)$ is the matrix with the orbital eigenenergies from the
            canonical Hartree-Fock equation.
            The orbital eigenenergies are indeed energies, but they are
            \emph{not} the eigenenergy of the total Hamiltonian.
            Rather they are similar to the eigenergies of the one-body
            Hamiltonian, but with a the Fock operator representing a one-body
            Hamiltonian containing a potential built from the mean-field
            interaction in the many-body problem.
            The equations in \autoref{eq:roothan-hall-general} are known as the
            \emph{Roothan-Hall} equations \cite{roothan, hall}.
            The Roothan-Hall equations constitute a generalized eigenvalue
            equation.\footnote{%
                The grammar sounds highly speculative as we talk about the
                Roothan-Hall \emph{equations} reducing to a single
                \emph{equation}.
            }
            These equations represent a computational improvement to the
            integro-differential equations that come from the canonical
            Hartree-Fock equations.

            In Python we solve the Roothan-Hall equations using SymPy's linear
            algebra routine \pyth{scipy.linalg.eigh} which solves both ordinary
            and generalized eigenvalue equations for symmetric and Hermitian
            matrices \cite{sympy}, viz.
            \begin{python}
epsilon, C = scipy.linalg.eigh(fock_matrix, s)
            \end{python}
            where \pyth{s} is the overlap matrix.
            Our solution does not discriminate whether the atomic orbitals are
            orthornomal or not.
            We always solve the generalized eigenvalue equation, and therefore
            pass in the idenity matrix as the overlap matrix in case of an
            orthonormal atomic orbital basis.

        \subsection{Constructing the general Fock matrix}
            An important point to note is that the Fock matrix elements depends
            on both the atomic and the molecular orbitals.
            That is,
            \begin{align}
                \fockten_{\alpha\beta}
                &= \mel*{\chi_{\alpha}}{\fock}{\chi_{\beta}}
                = \mel*{\chi_{\alpha}}{\onehamil}{\chi_{\beta}}
                +
                \mel*{\chi_{\alpha}\phi_j}{\twohamil}{\chi_{\beta}\phi_j}_{AS},
            \end{align}
            where $j$ only sums over the occupied indices in the molecular
            orbital basis.
            We notice that only the antisymmetric two-body elements depend on
            the molecular orbitals.
            Formulating the matrix elements in terms of he known atomic orbitals
            and the coefficient matrix we get
            \begin{align}
                \mel*{\chi_{\alpha}\phi_j}{\twohamil}{\chi_{\beta}\phi_j}
                &=
                C^{*}_{\gamma j} C_{\delta j}
                \mel*{\chi_{\alpha}\chi_{\gamma}}{\twohamil}{\chi_{\beta}\chi_{\delta}},
                \\
                \mel*{\chi_{\alpha}\phi_j}{\twohamil}{\phi_j\chi_{\beta}}
                &=
                C^{*}_{\gamma j} C_{\delta j}
                \mel*{\chi_{\alpha}\chi_{\gamma}}{\twohamil}{\chi_{\delta}\chi_{\beta}}.
            \end{align}
            The product of the coefficient matrices inspires the introduction of
            the density matrix of the occupied orbitals
            \begin{align}
                D_{\delta\gamma} \equiv
                C^{*}_{\gamma j} C_{\delta j}
                \implies
                \vfg{D} = \vfg{C}_{o}\vfg{C}^{\dagger}_{o},
                \label{eq:ghf-density-matrix}
            \end{align}
            where $\vfg{D} \in \mathbb{C}^{K \times K}$, and we've denoted the
            occupied coefficient matrices by $\vfg{C}_o \in \mathbb{C}^{K \times
            N}$.
            We can compute the density matrix in Python by
            \begin{python}
o = slice(0, n)
density_matrix = C[:, o] @ C[:, 0].conj().T
            \end{python}
            where \pyth{n} is he number of occupied particles and \pyth{o} is a
            slice with the indices of the occupied states.
            We can then write the matrix elements of the Fock operator in terms
            of the atomic orbitals and the density matrix as
            \begin{align}
                \fockten_{\alpha\beta}
                &= \mel*{\chi_{\alpha}}{\onehamil}{\chi_{\beta}}
                +
                D_{\delta\gamma}
                \mel*{\chi_{\alpha}\chi_{\gamma}}{
                    \twohamil
                }{\chi_{\beta}\chi_{\delta}}_{AS}.
                \label{eq:ghf-fock-matrix}
            \end{align}
            We use NumPy's tensor contraction routine \pyth{np.tensordot} to
            contract the density matrix and the two-body antisymmetric matrix.
            The Fock matrix can thus be constructed by
            \begin{python}
fock_matrix = (
    h + np.tensordot(density_matrix, u, axes=((0, 1), (3, 1)))
)
            \end{python}
            where \pyth{h} is the one-body matrix elements, \pyth{u} the
            antisymmetric two-body elements with the memory ordered by reading
            the indices from left to right, and \pyth{density_matrix} the
            density matrix.

            \subsubsection{General Hartree-Fock energy}
                The Hartree-Fock energy can be found by inserting the expansion
                of the molecular orbitals in terms of the atomic orbitals and
                the coefficient matrices into the energy functional from
                \autoref{eq:energy_func_hf}.
                This yields
                \begin{align}
                    \energy
                    &=
                    D_{\beta\alpha}
                    \mel*{\chi_{\alpha}}{\onehamil}{\chi_{\beta}}
                    + \half
                    D_{\gamma\alpha} D_{\delta\beta}
                    \mel*{\chi_{\alpha}\chi_{\beta}}{
                        \twohamil
                    }{\chi_{\gamma}\chi_{\delta}}_{AS},
                    \label{eq:general-hartree-fock-energy}
                \end{align}
                where we've contracted the occupied coefficient matrices into
                density matrices.
                In Python we compute the energy by
                \begin{python}
energy = np.trace(np.dot(density_matrix, h))
term = 0.5 * np.tensordot(
    density_matrix, u, axes=((0, 1), (2, 0))
)
energy += np.trace(np.dot(density_matrix, term))
                \end{python}
                where \pyth{term} is used as a temporary storage for the
                contraction of one of the density matrices with the
                antisymmetric two-body elements.

            \subsubsection{General Hartree-Fock one-body density matrix}
                The one-body density matrix elements is given by
                \begin{align}
                    \densityten^{q}_{p}
                    = \mel*{\slat}{
                        \ccr{p}
                        \can{q}
                    }{\slat}
                    = \delta_{p \in o}
                    C^{*}_{\alpha p} \overlapten_{\alpha\beta}
                    C_{\beta q}
                    = \delta_{p \in o}
                    \delta_{pq},
                \end{align}
                where we have labelled the set of occupied indices in the Slater
                determinants by $o = \brac{1, \dots N}$.
                We can represent the one-body density matrix as a block matrix
                by
                \begin{align}
                    \vfg{\densityten}
                    = \begin{pmatrix}
                        \1_{N \times N} & \vfg{0}_{N \times M} \\
                        \vfg{0}_{M \times N} & \vfg{0}_{M \times M}
                    \end{pmatrix},
                \end{align}
                where $M = L - N$ is the number of virtual basis states.
                We compute the one-body density matrix by
                \begin{python}
o = slice(0, n)
rho_qp = np.zeros_like(h)
rho_qp[o, o] = C[:, o].conj().T @ s @ C[:, o]
                \end{python}
                where \pyth{n} is the number of occupied particles and \pyth{o}
                is a slice with the occupied indices.


        \subsection{Self consistent field procedure}
            Now, in order for us to solve the Roothan-Hall equations, we need an
            expression for the Fock matrix.
            However, the Fock matrix depends on the coefficients found from
            solving the Roothan-Hall equations.
            In order to get around this pickle, we use \emph{self consistent
            field iterations} to gradually converge towards a solution to the
            Roothan-Hall equations.
            We denote matrices at a specific step $i$ by a superscript $(i)$,
            e.g., the Fock matrix at step $i$ is denoted $\fockmat^{(i)}$.
            If there are no superscripts that means the matrix is independent of
            the self consistent iterations.
            The self consistent field method for the Roothan-Hall equations is
            then given by,
            \begin{align}
                \fockmat^{(i)}\vfg{C}^{(i + 1)}
                = \overlapmat \vfg{C}^{(i + 1)}\vfg{\epsilon}^{(i + 1)}.
                \label{eq:ghf-scf-roothan-hall}
            \end{align}
            Here $\fockmat^{(i)}$ is built from the coefficient matrices at the
            previous time step, i.e., $\vfg{C}^{(i)}$.
            The generalized eigenvalue equation is solved in the same manner as
            described in \autoref{subsec:ghf} yielding the coefficient matrix
            and the orbital eigenenergies for the next time step.

            To start the self consistent iterations we need an initial value for
            the Fock matrix.
            We choose $\fockmat^{(0)} = \vfg{\oneten}$, i.e., we set the initial
            Fock matrix to be the one-body Hamiltonian matrix.
            The self consistent procedure can now be explained by the following
            steps:
            \begin{enumerate}
                \item Construct the Fock matrix $\fockmat^{(i)}$ from
                    \autoref{eq:ghf-fock-matrix}, or from the initial state if
                    $i = 0$.
                \item Solve \autoref{eq:ghf-scf-roothan-hall} to find
                    $\vfg{C}^{(i + 1)}$ and $\vfg{\epsilon}^{(i + 1)}$.
                \item Build the density matrix $\vfg{D}^{(i + 1)}$ from the
                    occupied coefficient matrices as in
                    \autoref{eq:ghf-density-matrix}.
                \item Check for convergence.
            \end{enumerate}
            The convergence of the self consistent iterations are determined by
            the change in energy and the change in the density matrix between
            two consecutive steps.
            That is, for two given tolerances $\delta_E$ and $\delta_D$, we say
            that the iterations have converged when both
            \begin{gather}
                \Delta \energy
                = \energy^{(i + 1)} - \energy^{(i)} < \delta_E,
                \\
                \norm{\Delta \vfg{D}}_{F}
                = \norm{\vfg{D}^{(i + 1)} - \vfg{D}^{(i)}}_{F}
                < \delta_D.
            \end{gather}
            are satisfied.
            The matrix norm is given by the Frobenius norm.

        \subsection{Convergence acceleration}
            The self-consistent field iterations are not guaranteed to converge.
            By utlizing the convergence acceleration techniques discussed in
            \autoref{sec:convergence} we can often remedy some of these
            problems.
            We define $\fockmat^{(i)}$ as the measured quantity, and the newly
            built $\fockmat^{(i + 1)}$ as our predicted quantity.
            The error vector is contstructed from
            \begin{align}
                \vfg{e}_i
                = \fockmat^{(i)} \vfg{D}^{(i)} \vfg{S}
                - \vfg{S} \vfg{D}^{(i)} \fockmat^{(i)}.
            \end{align}
            The error vector is only used for the DIIS acceleration and ignored
            in the alpha filter.
            We use the estimated Fock matrix from the alpha filter or DIIS as
            our new Fock matrix before starting the next round in the
            self-consistent field iterations.


        \subsection{The restricted Hartree-Fock method}
            \label{subsec:rhf}
            In the restricted Hartree-Fock method we make the assumption that
            each spin-direction is doubly occupied by an orbital.
            This can be a valid assumption if the Hamiltonian is
            spin-independent\footnote{%
                We write \emph{can} as there are situations where the
                Hamiltonian is spin-independent, but subject to conditions where
                the spin-symmetry of the restricted spin-orbitals break.
            }.
            To be even more specific, we will look at the \emph{closed-shell
            restricted Hartree-Fock} method, i.e., each spin-orbital \emph{must}
            be doubly occupied and each energy shell must be completely filled.
            This corresponds the spin-restricted spin-orbitals from
            \autoref{eq:restricted-spin-orbital}.
            For a basis of $L$ spin-orbitals we get $L/2$ orbitals, where $L$
            must be an even number.
            We label the states by
            \begin{align}
                \phi_{P}(x) = \varphi_p(\vf{r}) \sigma(m_s)
                \implies
                \ket*{\phi_P} = \ket*{\varphi_p\sigma},
            \end{align}
            where $P \in \brac{1, \dots, L}$ and $p \in \brac{1, \dots, L / 2}$.
            That is, we use capital letters to refer to composite indices and
            lowercase letters for the orbitals.
            We write the ground state Slater determinant as
            \begin{align}
                \ket*{\slat} = \ket*{\phi_1 \phi_2 \dots \phi_{N - 1} \phi_N}
                = \ket*{
                    (\varphi_1 \alpha)
                    (\varphi_1 \beta)
                    \dots
                    (\varphi_{N / 2} \alpha)
                    (\varphi_{N / 2}\beta)
                },
            \end{align}
            where $N$ is an even number of particles.
            The requirement that the molecular orbitals should be orthonormal is
            kept in the restricted Hartree-Fock method.
            As a consequence both the spin basis functions and the orbitals are
            orthonormal,
            \begin{align}
                \braket*{\phi_P}{\phi_Q}
                = \braket*{\sigma}{\tau}
                \braket*{\varphi_p}{\varphi_q}
                = \delta_{\sigma \tau}
                \delta_{pq}
                = \delta_{PQ}.
            \end{align}
            Inserting the restricted spin-orbitals into the canonical
            Hartree-Fock equation we get,
            \begin{align}
                \fock\ket*{\phi_P} = \epsilon_P\ket*{\phi_P}
                \implies
                \fock\ket*{\varphi_p\sigma}
                = \epsilon_{p}\ket*{\varphi_p\sigma}.
            \end{align}
            By projecting onto a molecular orbital we demonstrate how we can
            construct the Fock matrix elements when the Hamiltonian is
            spin-independent.
            This gives
            \begin{align}
                \mel*{\phi_P}{\fock}{\phi_Q}
                = \mel*{\phi_P}{\onehamil}{\phi_Q}
                + \mel*{\phi_P\phi_J}{\twohamil}{\phi_Q\phi_J}_{AS}.
                \label{eq:mo-fock-elements}
            \end{align}
            For the one-body Hamiltonian part of the Fock matrix elements we get
            \begin{align}
                \mel*{\phi_P}{\onehamil}{\phi_Q}
                = \delta_{\sigma\tau}\mel*{\varphi_p}{\onehamil}{\varphi_q}.
            \end{align}
            For the two-body part, we split up the antisymmetric elements into
            its constituent parts and show the spin-dependence in each terms
            separately.
            We start with the Coulomb operator giving
            \begin{align}
                \mel*{\phi_P\phi_J}{\twohamil}{\phi_Q\phi_J}
                &= \braket*{\sigma}{\tau}\braket{\nu}{\nu}
                \mel*{\varphi_p\varphi_j}{\twohamil}{\varphi_q\varphi_j}
                = 2 \delta_{\sigma\tau}
                \mel*{\varphi_p}{\hat{J}}{\varphi_q},
            \end{align}
            where we've introduced the restricted Coulomb operator from
            \autoref{eq:coulomb-operator}, and summed over the spin-dependence
            $\ket{\nu}$ from the two occupied orbitals in the two-body elements.
            That is,
            \begin{align}
                \braket*{\nu}{\nu} = \delta_{\nu\nu} = 2.
            \end{align}
            The second term in the antisymmetric two body elements yields the
            exchange operator to be
            \begin{align}
                \mel*{\phi_P\phi_J}{\twohamil}{\phi_J\phi_Q}
                &= \braket*{\sigma}{\nu}\braket{\nu}{\tau}
                \mel*{\varphi_p\varphi_j}{\twohamil}{\varphi_j\varphi_q}
                = \delta_{\sigma\tau}
                \mel*{\varphi_p}{\hat{K}}{\varphi_q},
            \end{align}
            where we've used the completness relation for the spin of the
            occupied molecular orbitals, viz.
            \begin{align}
                \dyad*{\nu}
                = \1 \in \mathbb{R}^{2 \times 2}.
            \end{align}
            Collecting the terms, we get the Fock matrix elements
            \begin{align}
                \mel*{\phi_P}{\fock}{\phi_Q}
                &=
                \delta_{\sigma\tau}
                \brak{
                    \mel*{\varphi_p}{\onehamil}{\varphi_q}
                    +
                    2
                    \mel*{\varphi_p}{\hat{J}}{\varphi_q}
                    -
                    \mel*{\varphi_p}{\hat{K}}{\varphi_q}
                }
                \\
                &=
                \delta_{\sigma\tau}
                \mel*{\varphi_p}{\fock}{\varphi_q},
            \end{align}
            where we see that the spin-dependence has been removed from the
            orbital integrals.
            We can therefore restrict ourselves to the orbital integrals for the
            Fock matrix elements.
            This means that we only need to look for coefficients for the
            molecular orbitals in terms of a known atomic orbital basis without
            spin.
            That is,
            \begin{align}
                \ket*{\varphi_p} = C_{\alpha p} \ket*{\chi_{\alpha}},
            \end{align}
            where $\brac{\chi_{\alpha}}$ is our spin-independent basis of $K$
            known atomic orbitals.
            By projecting the atomic orbitals onto the canonical Hartree-Fock
            equations we will again be left with the Roothan-Hall equations as
            seen in \autoref{eq:roothan-hall-general}.

        \subsection{Constructing the restricted Fock matrix}
            The difference between the restricted and the general Hartree-Fock
            methods lies in our calculation of the Fock matrix elements in the
            atomic orbital basis.
            We now have that
            \begin{align}
                \fockten_{\alpha\beta}
                &\equiv \mel*{\chi_{\alpha}}{\fock}{\chi_{\beta}}
                =
                \mel*{\chi_{\alpha}}{\onehamil}{\chi_{\beta}}
                +
                2
                \mel*{\chi_{\alpha}}{\hat{J}}{\chi_{\beta}}
                -
                \mel*{\chi_{\alpha}}{\hat{K}}{\chi_{\beta}},
            \end{align}
            where the Coulomb and the exchange operator depends on the full
            spin-dependent molecular orbitals.
            Looking at these two operators separately we have
            \begin{gather}
                \mel*{\chi_{\alpha}}{\hat{J}}{\chi_{\beta}}
                =
                C^{*}_{j\gamma} C_{j\delta}
                \mel*{\chi_{\alpha}\chi_{\gamma}}{
                    \twohamil
                }{\chi_{\beta}\chi_{\delta}}
                =
                C^{*}_{j\gamma} C_{j\delta}
                \twotensym^{\alpha\gamma}_{\beta\delta},
                \\
                \mel*{\chi_{\alpha}}{\hat{K}}{\chi_{\beta}}
                =
                C^{*}_{j\gamma} C_{j\delta}
                \mel*{\chi_{\alpha}\chi_{\gamma}}{
                    \twohamil
                }{\chi_{\delta}\chi_{\beta}}
                =
                C^{*}_{j\gamma} C_{j\delta}
                \twotensym^{\alpha\gamma}_{\delta\beta},
            \end{gather}
            where we've introduced the tensor notation for the elements of the
            two-body operator to be
            \begin{align}
                \twotensym^{\alpha\beta}_{\gamma\delta}
                \equiv
                \mel*{\chi_{\alpha}\chi_{\beta}}{
                    \twohamil
                }{\chi_{\gamma}\chi_{\delta}},
            \end{align}
            where the greek letters indicate that the elements are expressed in
            the atomic orbital basis.
            Note that these elements are not antisymmetric as opposed to
            $\twoten^{\alpha\beta}_{\gamma\delta}$.
            We also introduce the restricted density matrix
            \begin{align}
                D_{\beta \alpha}
                \equiv 2 C^{*}_{\alpha i} C_{\beta i},
            \end{align}
            where $i \in \brac{1, \dots, N / 2}$, and $N$ is the number of
            particles.
            In Python we compute the density matrix in the same way as done for
            the general Hartree-Fock method, but now we choose the slice over
            the occupied indices to be \pyth{o = slice(0, n // 2)}, and multiply
            the density matrix by a factor $2$.
            The restricted Fock matrix elements in the atomic orbital basis can
            now be written
            \begin{align}
                \fockten_{\alpha\beta}
                &=
                \oneten_{\alpha\beta}
                + D_{\delta\gamma}
                \brak{
                    \twotensym^{\alpha\gamma}_{\beta\delta}
                    -
                    \half
                    \twotensym^{\alpha\gamma}_{\delta\beta}
                },
                \label{eq:atomic-fock-rhf}
            \end{align}
            In Python we compute the restricted Fock matrix by
            \begin{python}
fock_matrix = (
    h
    + np.tensordot(density_matrix, I, axes=((0, 1), (3, 1)))
    - 0.5 * np.tensordot(
        density_matrix, I, axes=((0, 1), (2, 1))
    )
)
            \end{python}
            where \pyth{density_matrix} is now the restricted density matrix and
            \pyth{I} are the two-body elements.
            Furthermore, the one-body Hamiltonian \pyth{h} is now
            spin-independent.
            By proceeding with the self-consistent field iterations solving the
            Roothan-Hall equations with \autoref{eq:atomic-fock-rhf} as the
            definition of the Fock matrix elements, we find the orbital
            coefficient matrix $\vfg{C} \in \mathbb{C}^{K \times L/2}$ which we
            use to transform to the restricted molecular orbitals.
            Once transformed, we are at liberty to introduce spin-redundancy to
            open up for non-restricted post Hartree-Fock methods, e.g., the
            coupled-cluster method.

            \subsubsection{Restricted Hartree-Fock energy}
                We can compute the ground-state restricted Hartree-Fock energy
                by inserting our expression for the restricted molecular
                orbitals into the energy functional in
                \autoref{eq:energy_func_hf}.
                This yields
                \begin{align}
                    \energy
                    &=
                    D_{\beta\alpha}
                    \brac{
                        \oneten_{\alpha\beta}
                        + \half D_{\delta\gamma}
                        \para{
                            \twotensym^{\alpha\gamma}_{\beta\delta}
                            - \half
                            \twotensym^{\alpha\gamma}_{\delta\beta}
                        }
                    }.
                    \label{eq:rhf-energy}
                \end{align}
                We compute the energy in Python by the snippet shown in
                \autoref{alg:rhf-energy}.
                \begin{algorithm}
                    \begin{python}
# term_{ab} <- D_{dc} I^{ac}_{bd}
term = np.tensordot(
    density_matrix, I, axes=((0, 1), (3, 1))
)
# term_{ab} <- term_{ab} -0.5 * D_{dc} I^{ac}_{db}
term -= 0.5 * np.tensordot(
    density_matrix, I, axes=((0, 1), (2, 1))
)

# term_{ab} <- h_{ab} + 0.5 term_{ab}
term = h + 0.5 * term

# energy = D_{ba} term_{ab}
energy = np.trace(np.dot(term, density_matrix))
                    \end{python}
                    \caption{In this snippet we demonstrate how to compute the
                    restricted Hartree-Fock energy from
                    \autoref{eq:rhf-energy}.}
                    \label{alg:rhf-energy}
                \end{algorithm}
                The one-body density matrix in the restricted Hartree-Fock
                method is computed in the same way as for the general
                Hartree-Fock method, but with the restricted coefficients and an
                extra factor from the double occupancy.

        \subsection{The unrestricted Hartree-Fock method}
            The unrestricted Hartree-Fock method allows the molecular orbitals
            to have independent orbitals for each spin-direction.
            Hence, we assume that the molecular orbitals can be described by
            spin-unrestricted spin-orbitals as seen in
            \autoref{eq:unrestricted-spin-orbital}.
            Introducing indices for the different molecular orbitals, we denote
            the spin-unrestricted molecular orbitals by
            \begin{align}
                \phi_P(x)
                &=
                \varphi^{\sigma}_{p}(\vf{r})
                \sigma(m_s)
                \implies
                \ket*{\phi_P}
                = \ket*{\varphi^{\sigma}_{p}\sigma},
            \end{align}
            where $P \in \brac{1, \dots, L}$, $\sigma \in \brac{\alpha, \beta}$,
            and $p \in \brac{1, \dots, L_{\sigma}}$.
            We have that $L = L_{\alpha} + L_{\beta}$, and we have refrained
            from labelling the lower case orbital indices as they always occur
            with a spin index.
            Note that there is no implicit sum over the label $\sigma$ in the
            orbital $\varphi^{\sigma}_{p}$ and the spin-function $\sigma(m_s)$.
            We can collect the orbitals in two sets
            $\bigl\{\varphi^{\sigma}_{p}\bigr\}$, one for each spin-direction.
            The ground state Slater determinant can then be written
            \begin{align}
                \ket*{\slat}
                &=
                \ket*{\phi_1 \phi_2 \dots \phi_{N - 1} \phi_N}
                =
                \ket*{
                    (\varphi^{\alpha}_{1}\alpha)
                    \dots
                    (\varphi^{\alpha}_{N_{\alpha}}\alpha)
                    (\varphi^{\beta}_{1}\beta)
                    \dots
                    (\varphi^{\beta}_{N_{\beta}}\beta)
                },
            \end{align}
            where $N_{\sigma}$ is the number of particles with spin
            $\sigma(m_s)$.
            Note the ordering of the spin-orbitals in the determinant.
            As the two sets of orbitals can be of different sizes, we can no
            longer be sure that there is an even number of spin states.
            We therefore stack the spin-orbitals after one another instead of
            interlacing them by odd and even positions.
            The orthonormality of the molecular orbitals is given by
            \begin{align}
                \braket*{\phi_P}{\phi_Q}
                &= \delta_{PQ}
                = \braket*{\sigma}{\tau}
                \braket*{\varphi^{\sigma}_{p}}{\varphi^{\tau}_{q}},
            \end{align}
            where the overlap between two orbitals with differing spin is not
            necessarily zero.
            However, if the two spin-directions are the same, i.e., $\sigma =
            \tau$, we get
            \begin{align}
                \braket*{\varphi^{\sigma}_{p}}{\varphi^{\sigma}_{q}}
                = \delta_{pq}.
            \end{align}
            Inserting the unrestricted spin-orbitals into the canonical
            Hartree-Fock equations yield
            \begin{align}
                \fock\ket*{\phi_P}
                = \epsilon_P\ket*{\phi_P}
                \implies
                \fock\ket*{\varphi^{\sigma}_{p}\sigma}
                = \epsilon^{\sigma}_{p}\ket*{\varphi^{\sigma}_{p} \sigma},
            \end{align}
            which demonstrates how each spin-component yields a different
            equation as the Fock eigenenergies $\epsilon^{\alpha}_{p}$ is in
            general different from $\epsilon^{\beta}_{p}$.
            By projecting onto another molecular orbital as in
            \autoref{eq:mo-fock-elements} we demonstrate how the spin yields two
            separate Fock matrices, one for each spin-direction.\footnote{%
                Note that this assumes a spin-independent Hamiltonian.
            }
            The one-body elements in the molecular orbital basis is given by
            \begin{align}
                \mel*{\phi_P}{\onehamil}{\phi_Q}
                = \delta_{\sigma\tau}
                \mel*{\varphi^{\sigma}_{p}}{
                    \onehamil
                }{\varphi^{\tau}_{q}},
            \end{align}
            while the Coulomb operator from the two-body elements is given by
            \begin{align}
                \mel*{\phi_P\phi_J}{
                    \twohamil
                }{\phi_Q\phi_J}
                &=
                \delta_{\sigma\tau}
                \sum_{\rho \in \brac{\alpha, \beta}}
                \mel*{\varphi^{\sigma}_{p}}{
                    \hat{J}^{\rho}
                }{\varphi^{\sigma}_{q}}.
            \end{align}
            We note that this term provides a coupling between the orbitals in
            both spin-directions as we get a sum over the two spin-directions.
            This is not the case for the exchange operator from the
            antisymmetric two-body elements.
            We have
            \begin{align}
                \mel*{\phi_P\phi_J}{
                    \twohamil
                }{\phi_J\phi_Q}
                =
                \delta_{\sigma\tau}
                \mel*{\varphi^{\sigma}_{p}}{
                    \hat{K}^{\sigma}
                }{\varphi^{\tau}_{q}}.
            \end{align}
            Collecting all the terms in order to find the Fock matrix, we get
            \begin{align}
                \mel*{\phi_P}{\fock}{\phi_Q}
                &=
                \delta_{\sigma\tau}
                \Biggl[
                    \mel*{\varphi^{\sigma}_{p}}{
                        \onehamil
                    }{\varphi^{\tau}_{q}}
                    +
                    \sum_{\rho \in \brac{\alpha, \beta}}
                    \mel*{\varphi^{\sigma}_{p}}{
                        \hat{J}^{\rho}
                    }{\varphi^{\tau}_{q}}
                    -
                    \mel*{\varphi^{\sigma}_{p}}{
                        \hat{K}^{\sigma}
                    }{\varphi^{\tau}_{q}}
                \Biggr].
            \end{align}
            This demonstrates how the spin yields two different Fock matrices
            from the canonical Hartree-Fock equations.
            That is,
            \begin{align}
                \mel*{\varphi^{\sigma}_{p}}{
                    \fock^{\sigma}
                }{\varphi^{\sigma}_{q}}
                &=
                \mel*{\varphi^{\sigma}_{p}}{
                    \onehamil
                }{\varphi^{\sigma}_{q}}
                +
                \sum_{\rho \in \brac{\alpha, \beta}}
                \mel*{\varphi^{\sigma}_{p}}{
                    \hat{J}^{\rho}
                }{\varphi^{\sigma}_{q}}
                -
                \mel*{\varphi^{\sigma}_{p}}{
                    \hat{K}^{\sigma}
                }{\varphi^{\sigma}_{q}},
            \end{align}
            where the spin label on the Fock matrix corresponds to the spin
            label of the exchange operator.
            We now look for a set of coefficients for the orbitals in each
            spin-direction in terms of our original atomic orbital basis,
            \begin{align}
                \ket*{\varphi^{\sigma}_{p}}
                &= C^{\sigma}_{\kappa p} \ket*{\chi_{\kappa}},
            \end{align}
            where we use the greek letters $\kappa$, $\lambda$, $\mu$, and $\nu$
            for the atomic orbitals to avoid confusion with the spin-functions
            $\alpha(m_s)$ and $\beta(m_s)$.
            Before we demonstrate how we can generate a set of equations in
            order to find the coefficient matrices $\vfg{C}^{\sigma} \in
            \mathbb{C}^{K \times L_{\sigma}}$, we motivate the spin-labelling of
            the Fock matrices in the atomic orbital basis.
            We define
            \begin{align}
                \fockten^{\sigma}_{\kappa\lambda}
                &\equiv
                \mel{\chi_{\kappa}}{\fock^{\sigma}}{\chi_{\lambda}}
                %=
                %\oneten_{\kappa\lambda}
                %+ \sum_{\rho \in \brac{\alpha, \beta}}
                %\mel{\chi_{\kappa}}{
                %    \hat{J}^{\rho}
                %}{\chi_{\lambda}}
                %- \mel{\chi_{\kappa}}{
                %    \hat{K}^{\sigma}
                %}{\chi_{\lambda}}
                %\\
                %&=
                %\oneten_{\kappa\lambda}
                %+ \sum_{\rho \in \brac{\alpha, \beta}}
                %(C^{\rho}_{\mu j})^{*}
                %C^{\rho}_{\nu j}
                %\twotensym^{\kappa\mu}_{\lambda\nu}
                %-
                %(C^{\sigma}_{\mu j})^{*}
                %C^{\sigma}_{\nu j}
                %\twotensym^{\kappa\mu}_{\nu\lambda}
                %\\
                %&=
                =
                \oneten_{\kappa\lambda}
                + \sum_{\rho \in \brac{\alpha, \beta}}
                D^{\rho}_{\nu\mu}
                \twotensym^{\kappa\mu}_{\lambda\nu}
                -
                D^{\sigma}_{\nu\mu}
                \twotensym^{\kappa\mu}_{\nu\lambda},
            \end{align}
            where the density matrix along a certain spin-direction is defined
            similarly to the density matrices in the general Hartree-Fock
            method.
            Left-projecting the atomic orbital basis on the canonical
            Hartree-Fock equations acting on an orbital in the unrestricted
            regime yields
            \begin{gather}
                \mel*{\chi_{\kappa}}{\fock^{\sigma}}{\varphi^{\sigma}_{p}}
                = \epsilon^{\sigma}_{p}
                \braket*{\chi_{\kappa}}{\varphi^{\sigma}_{p}}
                \implies
                C^{\sigma}_{\lambda p}
                \mel*{\chi_{\kappa}}{\fock^{\sigma}}{\chi_{\lambda}}
                = C^{\sigma}_{\lambda p} \epsilon^{\sigma}_{p}
                \braket*{\chi_{\kappa}}{\chi_{\lambda}}
                \\
                \implies
                \fockten^{\sigma}_{\kappa \lambda}
                C^{\sigma}_{\lambda p}
                =
                \overlapten_{\kappa\lambda}
                C^{\sigma}_{\lambda p}
                \epsilon^{\sigma}_{p}
                \implies
                \vfg{F}^{\sigma}
                \vfg{C}^{\sigma}
                =
                \overlapmat
                \vfg{C}^{\sigma}
                \vfg{\epsilon}^{\sigma}.
                \label{eq:pople-nesbet}
            \end{gather}
            These coupled equations constitute the \emph{Pople-Nesbet
            equations}.
            They resemble the Roothan-Hall equations seen in the two previous
            methods in that they are generalized eigenvalue equations, but now
            we solve two sets of eigenvalue equations simultaneously.
            Note that the coupling between the two spin-directions occurs in the
            Fock matrix via the Coulomb operator.
            The self-consistent field iterations for the unrestricted
            Hartree-Fock method remains the same, but now we iterate equations
            for both spin-directions at the same time.

            \subsubsection{The unrestricted Hartree-Fock energy}
                Inserting our expression for the molecular orbitals into the
                energy functional we find the unrestricted Hartree-Fock energy,
                \begin{align}
                    \energy
                    %&=
                    %\mel{\slat}{\hamil}{\slat}
                    %=
                    %\mel{\phi_I}{\onehamil}{\phi_I}
                    %+
                    %\half
                    %\mel{\phi_I\phi_J}{
                    %    \twohamil
                    %}{\phi_I\phi_J}_{AS}
                    %\\
                    %&=
                    %\sum_{\sigma \in \brac{\alpha, \beta}}\brac{
                    %    \mel{\varphi^{\sigma}_{i}}{
                    %        \onehamil
                    %    }{\varphi^{\sigma}_{i}}
                    %    + \half\brak{
                    %        \sum_{\tau \in \brac{\alpha, \beta}}
                    %        \mel{\varphi^{\sigma}_{i}}{
                    %            \hat{J}^{\tau}
                    %        }{\varphi^{\sigma}_{i}}
                    %        -
                    %        \mel{\varphi^{\sigma}_{i}}{
                    %            \hat{K}^{\sigma}
                    %        }{\varphi^{\sigma}_{i}}
                    %    }
                    %}
                    %\\
                    &=
                    \sum_{\sigma \in \brac{\alpha, \beta}}
                    \Biggl\{
                        D^{\sigma}_{\lambda\kappa}
                        \oneten_{\kappa\lambda}
                        + \half
                        \sum_{\tau \in \brac{\alpha, \beta}}
                        D^{\sigma}_{\mu\kappa}
                        D^{\tau}_{\nu\lambda}
                        \twotensym^{\kappa\lambda}_{\mu\nu}
                        - \half
                        D^{\sigma}_{\mu\kappa}
                        D^{\sigma}_{\nu\lambda}
                        \twotensym^{\kappa\lambda}_{\nu\mu}
                    \Biggr\}.
                \end{align}

        \subsection{Time-evolution}
            Due to time limitations we have only implemented a time-dependent
            general Hartree-Fock method.
            From \autoref{eq:tdhf} we know that we can write the time-evolution
            of the molecular orbitals by,
            \begin{align}
                i\hslash\dpd{}{t}\ket*{\phi_i(t)}
                &= \fock(t)\ket*{\phi_i(t)}.
            \end{align}
            We can now insert the linear combination for the molecular orbitals in
            terms of the known atomic orbital basis.
            The time-evolution of the molecular orbitals is kept in the
            coefficients leaving the atomic orbital basis static in time.
            This gives
            \begin{align}
                i\hslash\dpd{}{t}C_{\alpha i}(t)\ket*{\chi_{\alpha}}
                &= \fock(t)C_{\alpha i}(t)\ket*{\chi_{\alpha}}.
            \end{align}
            Left projecting with another state from the atomic orbitals we can rewrite
            the previous equation to
            \begin{gather}
                i\hslash\dpd{}{t}C_{\alpha i}(t)\braket*{\chi_{\beta}}{\chi_{\alpha}}
                =
                C_{\alpha i}(t)\mel*{\chi_{\beta}}{\fock(t)}{\chi_{\alpha}}
                \\
                \implies
                i\hslash \dot{C}_{\alpha i} \overlapten_{\beta\alpha}
                = C_{\alpha i}(t)\fockten_{\beta\alpha}(t)
                \implies
                i\hslash \vf{S}\dot{\vf{C}}
                = \fockmat(t)\vf{C}(t),
                \label{eq:tdhf-equations}
            \end{gather}
            where the time-dependent Fock matrix $\fockmat(t)$ contains the
            matrix elements
            \begin{align}
                \fockten_{\beta\alpha}(t)
                = \mel*{\chi_{\beta}}{\fock(t)}{\chi_\alpha}.
            \end{align}
            We restrict ourselves to orthonormal atomic orbital basis sets as we
            start the time evolution after performing a ground state
            Hartree-Fock calculation and transforming to the orthonormal
            molecular orbital basis.
            Using atomic units we can then write \autoref{eq:tdhf-equations} as,
            \begin{align}
                \dot{\vf{C}} = -i\fockmat(t)\vf{C}(t).
                \label{eq:tdhf-orthogonal}
            \end{align}
            In case of non-orthogonal atomic orbitals, the orthonormalization
            procedure described in \autoref{subsec:basis-transformation} can be
            used to make the orbitals orthonormal prior to starting the
            time-evolution.
            As discussed in \autoref{sec:numerical-integration} we need to
            convert $\dot{\vfg{C}}$ to a vector.
            Using NumPy we store the result from the right-hand side as a
            two-dimensional array and we can use the function \pyth{np.ravel} to
            convert the two-dimensional array to a contiguous, flattened,
            one-dimensional array.
            We use the member function \pyth{reshape} to go from the flattened
            array back to the two-dimensional form.

        \subsection{Time-dependent energy}
            The time-dependent energy is computed in exactly same way as for the
            general Hartree-Fock method.
            We therefore reuse this functionality, but use the time-dependent
            coefficients for the density matrices.
            % TODO: Consider adding normalization

        \subsection{Time-dependent overlap}
            From \autoref{subsec:determinant-overlap} we an expression for the
            overlap of two Slater determinants.
            Due to the orthonormality of the atomic orbital basis, the
            time-dependent overlap is given by
            \begin{align}
                P(t, t_0)
                =
                \abs{\braket*{\Phi(t)}{\Phi(t_0)}}^2
                &= \abs{\determinant(\vf{C}^{\dagger}(t)\vf{C}(t_0))}^{2},
            \end{align}
            where $\vf{C}(t)$ is the coefficient matrix of the time evolved states.


    \section{Configuration interaction}
        As the main goal of this thesis has been to implement coupled-cluster
        solvers, the configuration interaction solver has not been worked at to
        such a large degree.
        The motivation for implementing a full configuration interaction solver
        for small systems is to compare our approximate methods to an -- within
        the computational space -- exact method.
        We have therefore implemented a ``naÃ¯ve'' configuration interaction
        solver where we create the full Slater determinant space and store it in
        memory.
        From this we also create the full Hamiltonian matrix $\hamilmat$.
        Our implementation thus quickly absorb too much memory and therefore
        limits the number of particles and basis functions that can be explored.
        To improve on the current scheme, an implementation of the \emph{direct
        CI} methods \cite{helgaker-molecular, olsen2012full} along with only
        storing non-zero elements in $\hamilmat$ will yield a more powerful
        method supporting more particles and basis functions.

        \subsection{Constructing the Slater determinant basis}
            We represent the Slater determinants as NumPy-arrays \cite{numpy} of
            bit strings using unsigned integers.
            The reason for choosing NumPy-arrays is that we are ble to use Numba
            \cite{numba} to speed up much of the explicit for-loops.
            Unfortunately this means that we have to the bit-twiddling manually.
            The default choice is to use \pyth{np.uint64}, i.e., 64-bit unsigned
            integers with room for 64 single-particle states, but other options
            such as 32-bit and 16-bit unsignd integers are available.
            If we have a system with $L > 64$ we add more integers in the array
            thus allowing for an integer mutiple of $64$ single-particle states at
            a time.
            Let $b$ be the number of bits in an integer, then the number of
            integers needed for a single Slater determinant $N_i$ is given by
            \begin{align}
                N_i = \left\lfloor\frac{L}{b}\right\rfloor
                + q,
            \end{align}
            where $q$ is either one or zero,
            \begin{align}
                q = \begin{cases}
                    1 & L \mod b > 0, \\
                    0 & L \mod b = 0,
                \end{cases}
            \end{align}
            where $L \mod b$ is the remainder of the integer division.
            The number of Slater determinants $N_s$ is given by a recursive
            function $N_s(S)$ depending on the order $S$ of the truncation,
            \begin{align}
                N_s(S) = \begin{cases}
                    1, & S = 0, \\
                    N_s(S - 1) \frac{(N - [S - 1])(M - [S - 1])}{S^2}, & S > 0.
                \end{cases}
            \end{align}
            Here $N$ is the number of particles, $M = L - N$ is the number of
            virtual states, and we've denoted the order $S$ as an integer where
            $1$ represents singles, $2$ doubles, and so forth.
            This formula counts the number of ways $N$ particles can be
            distributed among $M$ positions moving $S$ particles at a time.
            For a given truncation level, e.g., singles-and-doubles (CISD), the
            number of Slater determinants is then
            \begin{align}
                N_s = N_s(2) + N_s(1) + N_s(0),
            \end{align}
            where $N_s(0) = 1$ counts the reference state.
            In \autoref{tab:num-slater-determinants} we demonstrate how the
            number of Slater determinants increase as a function of truncation
            for a fixed number of particles.
            The storage cost of the Hamiltonian matrix $\hamilmat$ is uncanny
            going from CIS to CISDTQ as the storage increases by $7$ orders of
            magnitude.
            \begin{table}
                \centering
                \caption{In this table we demonstrate how the number of
                Slater determinants $N_s$ increase as a function of truncation
                level for $N = 4$ and $L = 80$.
                We've also included the number of bytes needed to store the
                Slater determinants using \pyth{np.uint64}, i.e., 64-bit
                unsigned integers to represent the determinants, and the size of
                the Hamiltonian matrix in bytes where we assume 128-bit complex
                numbers as elements.
                The storage cost of the Hamiltonian matrix for CIS was
                $\SI{0.001}{\giga\byte}$, which does not show up in the
                designated one decimal point.}
                \renewcommand{\arraystretch}{1.3}
                \begin{tabular}{@{}lrrr@{}}
                    \toprule
                    Truncation & $N_s$ & Determinant storage $[\si{\byte}]$
                    & Hamiltonian storage $[\si{\giga\byte}]$ \\
                    \midrule
                    CIS & $305$ & $2440$ & $0.0$ \\
                    CISD & $17405$ & $139240$ & $4.5$ \\
                    CISDT & $298605$ & $2388840$ & $1328.7$ \\
                    CISDTQ & $1581580$ & $12652640$ & $37273.7$ \\
                    \bottomrule
                \end{tabular}
                \label{tab:num-slater-determinants}
            \end{table}

            In our code we construct the Slater determinant basis by creating
            the reference determinant where we set the $N$ first bits in the
            array of unsigned integers and then create $N_s - 1$ copies of this
            state.
            The setting of a single-particle state in a bit string is done using
            the binary OR command.
            An example of the setting of single-particle states represented as
            bits is shown in \autoref{alg:set-state-68}.
            \begin{algorithm}
                \inputpython{implementation/solvers/determinants.py}{0}{13}
                \caption{An example of how set the single-particle state $68$ in
                a binary state array \pyth{state} using \pyth{np.uint64}
                integers to represent determinants.}
                \label{alg:set-state-68}
            \end{algorithm}
            Other options are to use the binary XOR operation, but whichever one
            is chosen some care must be shown as bugs can arise if the
            single-particle state is already set.
            In the case of the OR operation this does not change the state, but
            the XOR operation will remove the state.
            For this reason we use the XOR operation in order to unset a bit,
            i.e., remove a single-particle state.

            The higher excited determinants are created by exciting the
            reference determinant in a recursive fashion.
            The excitation operator for a single Slater determinant is shown in
            \autoref{alg:excite-state}.
            \begin{algorithm}
                \inputpython{implementation/solvers/determinants.py}{16}{23}
                \caption{Function used to represent a series of excitation
                operators $\hat{X}^{a}_{i}$, neglecting the sign.}
                \label{alg:excite-state}
            \end{algorithm}
            This function excites all single-particle states in the array
            \pyth{o_remove} to the single-particle states in \pyth{v_insert}.
            Note that the Slater determinants are interpreted as being in
            canonical ordering and we ignore the sign handling when creating the
            basis of determinants.
            The signs are thus handled when computing matrix elements.
            To populate the \pyth{o_remove}- and \pyth{v_insert}-arrays, we have
            a function which recursively adds an occupied index into
            \pyth{o_remove} and then proceeds to add all the virtual indices in
            order into \pyth{v_insert}, before calling the excitation function
            defined in \autoref{alg:excite-state}.
            The full recursive procedure is shown in
            \autoref{alg:create-excited-states}.
            \begin{algorithm}
                \inputpython{implementation/solvers/determinants.py}{26}{56}
                \caption{Function creating all excited determinants of a given
                order \pyth{order}.}
                \label{alg:create-excited-states}
            \end{algorithm}
            The entire implementation of the configuration interaction method is
            uniquely defined by the basis of Slater determinants.
            This means that after a truncation order has been chosen and the
            basis of Slater determinants has been constructed, everything that
            follows will be solved in the same manner independently of the
            truncation level.

        \subsection{Constructing the Hamiltonian matrix}
            The arguably most effective ``first order'' optimization that can be
            performed for the configuration interaction method is to implement
            the Slater-Condon rules when evaluating matrix elements of operator
            strings, as opposed to brute force evaluation of the action of the
            second quantized operators on a determinant.
            When constructing the Hamiltonian matrix $\hamilmat$ with one- and
            two-body operators, we wish to evaluate the matrix elements
            \begin{align}
                \hamilten_{IJ}
                = \mel*{\Phi_{I}}{\hamil}{\Phi_J}
                =
                \oneten^{p}_{q}
                \mel*{\Phi_{I}}{\ccr{p}\can{q}}{\Phi_J}
                +
                \frac{1}{4}
                \twoten^{pq}_{rs}
                \mel*{\Phi_{I}}{
                    \ccr{p}
                    \ccr{q}
                    \can{r}
                    \can{s}
                }{\Phi_J},
            \end{align}
            using the Slater-Condon rules defined in
            \autoref{lemma:slater-condon-one-body} and
            \autoref{lemma:slater-condon-two-body}.
            Given two Slater determinants $\ket*{\slat_I}$ and $\ket*{\slat_J}$
            which we represent as two occupation number states $\ket*{\vfg{n}}$
            and $\ket*{\vfg{m}}$, respectively, we need ways to evaluate the
            following:
            \begin{itemize}
                \item The sign given by the phase,
                    \begin{align}
                        (\Gamma_{-})^{\vfg{n}}_{p}
                        = \prod_{i = 1}^{p - 1}(-1)^{n_i},
                    \end{align}
                    as defined in \autoref{def:creation_1}.
                \item The Kronecker-Delta $\delta_{p \in \vfg{n}}$ checking if
                    the single-particle state $p$ is an occupied state in
                    $\ket{\vfg{n}}$.
                \item The difference $\abs{\vfg{n} - \vfg{m}}$, between the two
                    determinants $\ket{\vfg{n}}$ and $\ket{\vfg{m}}$.
                \item The position of a set bit to a single-particle index $p$
                    in order to find the correct matrix elements in
                    $\oneten^{p}_{q}$ and $\twoten^{pq}_{rs}$.
            \end{itemize}
            For the sign calculation we use the product for the phase
            $(\Gamma_{-})^{\vfg{n}}_i$ defined in \autoref{def:creation_1} by
            counting the number of set bits $k$ at positions below $i$, and
            computing $(-1)^k$.
            An implementation of this sign calculation is shown in
            \autoref{alg:gamma-phase}.
            \begin{algorithm}
                \inputpython{implementation/solvers/determinants.py}{59}{72}
                \caption{Function computing the sign of the action of a creation
                or annihilation operator for index \pyth{p} on a determinant
                \pyth{state}.
                This is the binary implementation of the phase defined in
                \autoref{def:creation_1}.}
                \label{alg:gamma-phase}
            \end{algorithm}
            The implementation of the Kronecker-Delta is shown in
            \autoref{alg:kronecker-delta}.
            \begin{algorithm}
                \inputpython{implementation/solvers/determinants.py}{75}{81}
                \caption{Implementation of the Kronecker-Delta $\delta_{p \in
                \vfg{n}}$.}
                \label{alg:kronecker-delta}
            \end{algorithm}
            To compute the difference between the two determinants we start by
            using the XOR operation to find the specific bits that are set in
            either $\vfg{n}$ or $\vfg{m}$, but not both.
            Next, we count the number of set bits in this difference state.
            The counting of set bits in an integer is a topic which has been
            explored in some depth in the field of computer science and has led
            to some very efficient algorithms.
            We use the population count algorithm \cite{popcount} shown in
            \autoref{alg:popcount_64}.
            \begin{algorithm}
                \inputpython{implementation/solvers/determinants.py}{84}{97}
                \caption{Implementation of the popcount algorithm for 64-bit
                integers.}
                \label{alg:popcount_64}
            \end{algorithm}
            Now, counting the number of set bits in the difference-state from
            $\ket{\vfg{n}} \text{XOR} \ket{\vfg{m}}$ yields the difference
            $\abs{\vfg{n} - \vfg{m}}$.
            The function in \autoref{alg:state-diff} computes the difference
            between two determinants.
            \begin{algorithm}
                \inputpython{implementation/solvers/determinants.py}{100}{108}
                \caption{Function counting the difference in the number of
                single-particle states in two Slater determinants.}
                \label{alg:state-diff}
            \end{algorithm}
            To compute the index of a set bit in a bitstring we iterate over all
            bit positions in the bitstring and right-shift the bits before using
            the binary AND operation to check if the rightmost bit is set.
            Once we encounter a bit, the iteration counter contains the index of
            the bit.
            An implementation of this scheme is shown in
            \autoref{alg:get-index}.
            \begin{algorithm}
                \inputpython{implementation/solvers/determinants.py}{111}{125}
                \caption{Function computing the index of a set bit in Slater
                determinant.
                The parameter \pyth{index_num} decides if we should find the
                first (\pyth{0}), second (\pyth{1}), or higher, set bits.}
                \label{alg:get-index}
            \end{algorithm}
            % TODO: Add the implementation of the Slater-Condon rules?


        \subsection{Diagonalization}
            Having constructed the full Hamiltonian matrix $\hamilmat \in
            \mathbb{C}^{N_s \times N_s}$ the next step is to diagonalize the
            matrix in order to get the eigenenergies $\vfg{\energy} = \diag(E_1,
            \dots, E_{N_s})$ and the eigenvectors, i.e., the coefficients for
            the eigenstates, $\vfg{C} \in \mathbb{C}^{N_s \times N_s}$.
            As stated in \autoref{chap:ci}, we restrict our attention to
            orthonormal single-particle states and hence orthonormal Slater
            determinants.
            We therefore need to solve the eigenvalue equation
            \begin{align}
                \hamilmat\vfg{C} = \vfg{E}\vfg{C}.
            \end{align}
            The Hamiltonian matrix is Hermitian and we can therefore use the
            function \pyth{np.linalg.eigh} \cite{numpy}, which uses the
            LAPACK-routines \cite{laug} \pyth{_syevd} and \pyth{_heevd} for
            symmetric and Hermitian matrices respectively, to solve the
            eigenvalue equation.
            This will yield the full spectrum of $\hamilmat$ and will often
            prove a limiting factor in terms of computational complexity as the
            number of FLOPS required to solve this equation scales as
            $\mathcal{O}(N_s^3)$.

            As an alternative to the full spectrum we can use a sparse
            eigenvalue solver.
            We use the function \pyth{scipy.sparse.linalg.eigsh} from SymPy
            \cite{sympy} which is a wrapper around the ARPACK routines SSEUPD
            and DSEUPD \cite{arpack} implementing the Implictly Restarted
            Lanczos Method with a theoretical complexity of $\mathcal{O}(N_s^2)$.
            This lets us specify how many eigenpairs $k$ we wish to compute,
            which is then found iteratively.

            The eigensolvers given by NumPy \cite{numpy} and SciPy \cite{sympy}
            sorts the eigenvalues in an ascending order with the eigenvectors
            sorted in the same fashion, and with the eigenvectors unitary.
            This means that the ground state energy is found as the first
            element of the eigenvalues.

        \subsection{One-body density matrix}
            Having diagonalized the Hamiltonian matrix we can compute the one-body
            density matrix of the system.
            For a given eigenstate $\ket*{\Psi_I}$ we compute the one-body
            density matrix by
            \begin{align}
                {\rho_I}^{q}_{p}
                &= \mel*{\Psi_I}{\ccr{p}\can{q}}{\Psi_I}
                = C_{JI}^{*}C_{KI}
                \mel*{\slat_J}{\ccr{p}\can{q}}{\slat_K}.
                \label{eq:ci-one-body-density}
            \end{align}
            As matrix elements of the one-body density matrix is given by a pair
            of creation and annihilation operators we can use the Slater-Condon
            rules for one-body operators to evaluate the overlap.
            This results in virtually the same implementation as for the
            one-body Hamiltonian, but with a new one-body operator given by the
            coefficient vector $\vfg{c}_I$ for a specific eigenstate
            $\ket*{\Psi_I}$.
            Do note that $\vfg{c}_I \in \mathbb{C}^{N_s}$ and that the indices
            into this vector is given by the same indices as for the Slater
            determinants, unlike the single-particle indices used for the
            one-body density matrix and the one-body Hamiltonian.

        \subsection{Time-evolution}
            Having solved the ground state problem, we move on to the dynamics
            of the configuration interaction method.
            Choosing an initial state with a given coefficient vector
            $\vfg{c}(0) \in \mathbb{C}^{N_s}$, either from the ground state
            problem or from some other method, we proceed to solve the
            differential equation demonstrated in \autoref{eq:tdci}.
            In atomic units this corresponds to solving
            \begin{align}
                \dot{\vfg{c}}(t) = -i\hamilmat(t)\vfg{c}(t),
            \end{align}
            which we see is already a vector and therefore be directly fed into
            the numerical integrators described in
            \autoref{sec:numerical-integration}.
            Our task is now to construct the time-dependent Hamiltonian matrix.
            In general we can construct the time-dependent matrix elements from
            \begin{align}
                \hamilten_{IJ}(t)
                &=
                \oneten^{p}_{q}(t)
                \mel*{\slat_I}{\ccr{p}\can{q}}{\slat_J}
                + \frac{1}{4}\twoten^{pq}_{rs}(t)
                \mel*{\slat_I}{
                    \ccr{p}\ccr{q}\can{s}\can{r}
                }{\slat_J}.
            \end{align}
            and reuse the Slater-Condon rules to evaluate the one- and two-body
            operators.
            Our implementation does programmatically support a time-dependent
            two-body operator, but this is not something that we will use.
            Therefore, by only including time-dependent one-body operators, an
            optimization is to store two copies of the full Hamiltonian
            matrix,\footnote{%
                This might seem a little odd as we've already argued at length
                of how the Hamiltonian matrix is the bottleneck of the
                implementation, but remember that our focus is on the dynamics
                of quantum-mechanical systems and we are therefore quite limited
                in the size of the systems we can explore.
                This means that we will seldom look at very large systems and we
                can often store the full Hamiltonian matrix, and copies, in
                memory.
            }
            and only re-compute the time-dependent one-body contributions.
            We define the two sets of matrix elements for the Hamiltonian matrix
            by
            \begin{gather}
                (\hamil_1)_{IJ}(t)
                \equiv
                \mel*{\slat_I}{\onehamil(t)}{\slat_J}, \\
                (\hamil_2)_{IJ}(t)
                \equiv
                \mel*{\slat_I}{\twohamil(t)}{\slat_J}.
            \end{gather}
            Setting $\twohamil(t) = \twohamil$ we now compute the Hamiltonian
            matrix from
            \begin{align}
                \hamilmat(t)
                = \hamilmat_1(t) + \hamilmat_2,
            \end{align}
            where we only construct a new $\hamilmat_1(t)$ -- using the
            Slater-Condon rules for the one-body operator -- at every time step.

        \subsection{Time-dependent energy}
            For a time-evolved state $\ket*{\Psi(t)}$ with a time-evolved
            Hamiltonian $\hamil(t)$, we can compute the energy of the state at a
            certain time $t$ by
            \begin{align}
                E(t) = \frac{
                    \mel*{\Psi(t)}{\hamil(t)}{\Psi(t)}
                }{
                    \braket*{\Psi(t)}
                },
            \end{align}
            where we've included an explicit normalization due to potential
            drift in the coefficients from the time-evolution using numerical
            integrators.
            Expanding the time-evolved state in the static basis of Slater
            determinants with time-dependent coefficients, $\vfg{c}(t)$, we find
            the time-dependent energy to be
            \begin{align}
                E(t)
                &=
                \frac{
                    c^{*}_I(t)\mel*{\slat_I}{\hamil(t)}{\slat_J}c_J(t)
                }{
                    c^{*}_I(t)c_I(t)
                }
                = \frac{
                    \vfg{c}^{\dagger}(t)\hamilmat(t)\vfg{c}(t)
                }{
                    \vfg{c}^{\dagger}(t)\vfg{c}(t)
                }.
            \end{align}


        \subsection{Time-dependent overlap}
            We compute the time-dependent overlap by
            \begin{align}
                P(t, t_0)
                =
                \frac{
                    \abs{\braket*{\Psi(t)}{\Psi(t_0)}}^2
                }{
                    \braket*{\Psi(t)}\braket*{\Psi(t_0)}
                }
                = \frac{
                    \abs{\vfg{c}^{\dagger}(t)\vfg{c}(t_0)}^2
                }{
                    \abs{\vfg{c}(t)}^2
                    \abs{\vfg{c}(t_0)}^2
                },
            \end{align}
            where we again include an explicit normalization term in case of
            drift in the normalization due to the integrator.

    \section{Coupled cluster}
        \label{sec:cc-solver}
        In this thesis we have implemented a set of coupled-cluster solvers,
        they are:
        \begin{itemize}
            \item The coupled-cluster doubles (CCD) and the coupled-cluster
                singles-and-doubles (CCSD) methods with static orbitals.
                This includes both time-independent and time-dependent solvers.
            \item The non-orthogonal coupled-cluster doubles (NOCCD) ground
                state solver.
                Note that we often call this solver for the orbital-adaptive
                coupled-cluster doubles method (OACCD).
                This solver was written by \citeauthor{rolf-nocc}
                \cite{rolf-nocc} and given to use as part of an ongoing article
                on the stability of time-dependent coupled-cluster methods.
                We still describe the implementation in some detail as we had to
                integrate the solution into our library.
                % TODO: Cite ongoing article
            \item The orbital-adaptive time-dependent coupled-cluster doubles
                (OATDCCD) method.
                This method uses the OACCD method as an initial ground state
                solver.
        \end{itemize}
        Our coupled-cluster library tries in as large degree as possible to
        reuse routines for the different solvers as this makes optimization
        more efficient, that is, we only need to optizime a specific function
        once, instead of once per solver.

        \subsection{Ground state solvers with static orbitals}
            \label{subsec:gs-cc}
            From \autoref{chap:cc} we know that the projected amplitude
            equations from \autoref{eq:cc_amp_sim} should be satisfied once we
            have found the optimal $\clustamp_{\mu}$-amplitudes.
            However, in order to find the optimal amplitudes we employ an
            iterative \emph{quasi-Newton} method which avoids the need for
            computing the Jacobian matrix and solving a linear equation as in
            Newton's method \cite{helgaker-molecular}.
            We define the left-hand side of \autoref{eq:cc_amp_sim} with the
            normal-ordered Hamiltonian to be
            \begin{align}
                \Omega_{\mu}(\vfg{\clustamp}^{(i)})
                = \mel*{\slat_{\mu}}{
                    \exponential(-\clust^{(i)})
                    \hamil_N
                    \exponential(\clust^{(i)})
                }{\slat},
            \end{align}
            where $\vfg{\clustamp}^{(i)}$ is the collection of cluster
            amplitudes at iteration $i$ and $\mu$ denotes an excitation level.
            Note that we treat $\Omega_{\mu}$ as a tensor of rank $\mu$ in
            order to collect all elements from a cluster of rank $\mu$.
            In \autoref{app:cc-tau-amplitudes} the $\clustamp$-amplitude
            equations are listed for the doubles (CCD) and singles-and-doubles
            (CCSD) truncation levels.

            In the quasi-Newton method we now solve \cite{bartlett-purvis,
            helgaker-molecular}
            \begin{align}
                \Delta \clustamp^{(i)}_{\mu}
                = \frac{\Omega_{\mu}(\vfg{\clustamp}^{(i)})}{
                    D_{\mu}
                },
                \label{eq:delta-tau}
            \end{align}
            in order to find the change in the amplitudes between two
            iterations.
            In the former equation $\mu$ serves as a label and should not be
            summed.
            The division is also interpreted as an element-wise division.
            Now, $D_{\mu}$ is a tensor of rank $\mu$ and serves as an
            approximation to the Jacobian matrix in case of a full fledged
            Newton's method \cite{helgaker-molecular}.
            For the singles and doubles amplitudes we have the tensors
            \begin{gather}
                D^{a}_{i} \equiv \epsilon_i - \epsilon_a, \\
                D^{ab}_{ij} \equiv \epsilon_i + \epsilon_j
                - \epsilon_a - \epsilon_b,
            \end{gather}
            where $\epsilon = \diag(\epsilon_1, \dots, \epsilon_L)$ are the
            diagonal elements of the normal-ordered Fock matrix.
            In order for \autoref{eq:delta-tau} to be well-defined we require
            that none of the elements in $D_{\mu}$ are zero.
            This is ensured as long as we have a well-defined single-reference
            problem as \cite{cramer2004computational}
            \begin{align}
                \epsilon_i \neq \epsilon_a,
            \end{align}
            for all $i \in \brac{1, \dots, N}$ and $a \in \brac{N + 1, \dots,
            L}$ when the single-reference assumption holds.
            However, do note that the lack of infinite precision on a computer
            means that we can get instabillities if we approach a system that is
            almost degenerate across the Fermi vacuum, that is, if we are almost
            in a multireference situation.
            % TODO: This sounds kinda sketchy!
            We are now able to compute the next iteration of the cluster
            amplitudes by
            \begin{align}
                \vfg{\clustamp}^{(i + 1)}
                = \vfg{\clustamp}^{(i)}
                + \Delta \vfg{\clustamp}^{(i)},
            \end{align}
            where we collect all the cluster amplitudes together when computing
            the improved estimate.
            We order the $\clustamp$-amplitudes with the virtual indices first,
            for example the doubles amplitudes is given by
            \begin{align}
                \clustamp^{ab}_{ij}
                \to \text{\pyth{t_2[a, b, i, j]}},
            \end{align}
            as NumPy arrays.
            To determine if we have found a converged set of cluster amplitude
            we compute the Frobenius norm of each set of amplitudes using
            NumPy, and checking whether or not we have a value below a set
            threshold.
            An alternative is to compute the energy in every iteration of the
            cluster amplitudes and determining convergence based on the
            difference in energy between each step.

            Having found converged $\vfg{\clustamp}$-amplitudes we turn our
            attention to the Lagrange multipliers, also known as the
            $\vfg{\clustlamp}$-amplitudes.
            The procedure is virtually the same as for the
            $\vfg{\clustamp}$-amplitudes, but now we solve for the stationary
            condition in \autoref{eq:cc-lagrangian-lambda}.
            That is,
            \begin{align}
                \Omega_{\mu}(\vfg{\clustamp}, \vfg{\clustlamp}^{(i)})
                = \mel{\slat}{
                    (\1 + \clustl^{(i)})
                    \exponential(-\clust)
                    \com{\hamil}{\hat{X}_{\mu}}
                    \exponential(\clust)
                }{\slat}
            \end{align}
            where $\vfg{\clustamp}$ are now the converged
            $\vfg{\clustamp}$-amplitudes.
            The equations for CCD and CCSD are listed in
            \autoref{app:cc-lambda-amplitudes}.
            Reusing the quasi-Newton method we compute
            \begin{align}
                \Delta\clustlamp_{\mu}^{(i)}
                = \frac{
                    \Omega_{\mu}(\vfg{\clustamp}, \vfg{\clustlamp}^{(i)})
                }{
                    D_{\mu}
                },
            \end{align}
            where the $D_{\mu}$ is the same tensor as for the
            $\clustamp$-amplitudes, but with the occupied and the virtual
            indices reversed.
            That is, we order the indices of the $\clustlamp$-amplitudes
            in the opposite order from the $\clustamp$-amplitudes, e.g., the
            doubles amplitudes are
            \begin{align}
                \clustlamp^{ij}_{ab}
                \to \text{\pyth{l_2[i, j, a, b]}},
            \end{align}
            using NumPy arrays.
            We compute the next iteration of the $\clustlamp$-amplitudes from
            \begin{align}
                \vfg{\clustlamp}^{(i + 1)}
                = \vfg{\clustlamp}^{(i)}
                + \Delta\vfg{\clustlamp}^{(i)}.
            \end{align}
            The convergence criteria for $\vfg{\clustlamp}$ is the same as for
            the $\vfg{\clustamp}$-amplitudes, i.e., we check if the Frobenius
            norm of each amplitude set is below a certain threshold.


        \subsection{Convergence acceleration}
            As discussed in \autoref{sec:convergence} the quasi-Newtion method
            can suffer from convergence instabillities.
            We can therefore use the alpha filter from
            \autoref{subsec:alpha-filter} and the DIIS technique from
            \autoref{subsec:diis} to accelerate the convergence.
            In order to use these techniques as they stand we need to define the
            predicted estimate $z_i$ to use in the alpha filter and the error
            vectors in DIIS.

            \subsubsection{Alpha filter}
                For the alpha filter we define our predicted estimate to be
                \begin{align}
                    \vfg{z}^{(i)} = \vfg{\clustamp}^{(i)}
                    + \Delta\vfg{\clustamp}^{(i)},
                \end{align}
                for the $\clustamp$-amplitudes.
                The measurement is thus $\vfg{\clustamp}^{(i)}$.
                Moving to the $\clustlamp$-amplitudes we define the predicted
                estimate similarly, viz.
                \begin{align}
                    \vfg{z}^{(i)} = \vfg{\clustlamp}^{(i)}
                    + \Delta\vfg{\clustlamp}^{(i)},
                \end{align}
                and with the measurement as $\clustlamp^{(i)}$.
                This means that we need to represent our amplitude tensors as
                vectors, and in the case of more than one set of amplitudes we
                need to concatenate these vectors on top of one another.
                By letting $\alpha \to 1$ we include less and less of the next
                iteration.
                Setting $\alpha = 0$ we remove the convergence acceleration
                entirely.

            \subsubsection{DIIS}
                For the DIIS algorithm described in \autoref{subsec:diis} we
                define our error vectors to be
                \begin{align}
                    \vfg{e}_{i} = \vfg{\Omega}^{(i)},
                \end{align}
                for both amplitude sets.
                This choice is made as we know that $\vfg{\Omega} \to 0$ when we
                reach convergence and have found the optimized amplitudes.
                Thus the least squares minimization of the error vectors should
                yield the optimal converged amplitudes.
                The measured vector $\vfg{p}_{i + 1}$ is given by
                \begin{align}
                    \vfg{p}_{i + 1}
                    = \vfg{\clustamp}^{(i)}
                    + \Delta\vfg{\clustamp}^{(i)},
                \end{align}
                for the $\clustamp$-amplitudes and similarly for the
                $\clustlamp$-amplitudes.
                The extrapolated value from the DIIS algorithm is chosen as the new
                amplitude before starting the next iteration of the quasi-Newton
                method.

        \subsection{Non-orthogonal ground state solver}
            The non-orthogonal coupled-cluster doubles (NOCCD/OACCD) method has been
            implemented by \citeauthor{rolf-nocc} as part of his article
            \citetitle{rolf-nocc} \cite{rolf-nocc} and has been implemented into
            our framework by the author.
            We wish to use the quasi-Newton method for the orbital rotations
            $\noccten$ in a similar fashion as for the cluster amplitudes in
            \autoref{subsec:gs-cc}.
            From the stationary conditions in \autoref{eq:nocc-kappa-up-rhs} and
            \autoref{eq:nocc-kappa-down-rhs}, we can compute the change in the
            orbital rotations by \cite{ugur-occ}
            \begin{gather}
                \Delta (\noccten^{u})^{(i)}
                = -\frac{1}{D^a_i}
                \left.
                \dpd{
                    L(\vfg{\clustamp}, \vfg{\clustlamp},
                    \noccten^{u}, \noccten^{d})
                }{
                    {\noccten^{d}}
                }
                \right\rvert_{\noccten = 0},
                \\
                \Delta (\noccten^{d})^{(i)}
                = -\frac{1}{D^i_a}
                \left.
                \dpd{
                    L(\vfg{\clustamp}, \vfg{\clustlamp},
                    \noccten^{u}, \noccten^{d})
                }{
                    {\noccten^{u}}
                }
                \right\rvert_{\noccten = 0},
            \end{gather}
            where the index placement of the $\vfg{D}$-matrix should be noted.
            This lets us compute the next iteration of the rotations in the
            quasi-Newton method by
            \begin{gather}
                (\noccten^{u})^{(i + 1)}
                = (\noccten^{u})^{(i)} + \Delta(\noccten^{u})^{(i)}, \\
                (\noccten^{d})^{(i + 1)}
                = (\noccten^{d})^{(i)} + \Delta(\noccten^{d})^{(i)}.
            \end{gather}
            As discussed in \autoref{subsec:nocc} the ground state algorithm for
            the NOCCD method can be formulated as \cite{ugur-occ, rolf-nocc}:
            \begin{enumerate}
                \item Compute the orbital transformation matrices by
                    \begin{gather}
                        \vfg{S} = \exponential(\noccmat),
                        \qquad
                        \tilde{\vfg{S}} = \exponential(-\noccmat),
                    \end{gather}
                    where we use the matrix exponential function
                    \pyth{scipy.linalg.expm} from SciPy \cite{scipy} to compute
                    the exponential of the $\noccmat$-matrices.
                    Initially we set $\noccmat = \1$.
                \item Change from the original atomic orbital basis to the new
                    non-orthogonal basis using $\vfg{S}$ and $\tilde{\vfg{S}}$
                    as coefficient matrices for the ket and bra states
                    respectively, as discussed in \autoref{sec:change-of-basis}.
                \item Solve the $\clustamp$ and $\clustlamp$ equations as
                    described in \autoref{subsec:gs-cc}.
                    Note that as we have transformed to the new non-orthogonal
                    basis we need to recompute the $D_{\mu}$-matrices.
                \item Compute the stationary conditions for the orbital
                    rotations from \autoref{eq:nocc-kappa-up-rhs} and
                    \autoref{eq:nocc-kappa-down-rhs}, and build the next
                    $\noccmat$ from the new matrices $(\noccten^{u})^{(i + 1)}$
                    and $(\noccten^{d})^{(i + 1)}$ found from the quasi-Newton
                    method.
            \end{enumerate}
            We start the initial minimization using $\noccmat = \1$.
            This means that the initial iterations of the $\clustamp$- and
            $\clustlamp$-amplitudes are the same as for the CCD-method, and if
            we are looking at a system where the CCD-method has ``extreme''
            convergence issues, \footnote{%
                By ``extreme'' convergence issues we mean so severe problems
                that the first iteration of the cluster amplitudes yield
                residuals that blow up and clearly diverges.
            } the NOCCD-method will also follow the same diverging trend.

            Now, the convergence criteria for the NOCCD method is slightly
            different than in the static ground state solvers.
            The method uses a semi-adaptive convergence scheme where we
            initially let the cluster amplitudes converge to a very low
            tolerance, i.e., a non-precise convergence.
            We then compute the new orbital rotations using the quasi-Newton
            method with optional convergence acceleration, e.g., DIIS.
            The convergence critera of the orbital rotations are found from the
            Frobenius norm of $\Delta(\noccten^{u})^{(i)}$ and
            $\Delta(\noccten^{d})^{(i)}$ as these should be zero in the optimal
            basis.
            Now we update the convergence criteria of the cluster amplitudes by
            multiplying the smallest orbital residual and multiplying it with
            some pre-defined factor.
            Choosing the largest tolerance criterion, either the newly computed
            tolerance from the orbital rotations, or some pre-defined
            termination tolerance we repeat the steps numerated above.
            Once the residuals of the orbital matrices are below the termination
            tolerance, we say that the method has converged.

        \subsection{Computing the coupled-cluster energy}
            In the case of ``pure'' ground state calculations for CCD and CCSD
            where we are only interested in the energy, we can compute the
            projected coupled-cluster energy from
            \autoref{eq:cc-energy-equation}.
            However, for the NOCCD-method we compute the full Lagrangian from
            \autoref{eq:nocc-lagrangian}.
            As we continually transform to the new basis in the NOCCD-method
            this is equivalent to the coupled-cluster Lagrangian in
            \autoref{eq:cc-energy-functional}.
            This Lagrangian can also be used for the CCD- and CCSD-methods once
            we have found the optimal values for $\vfg{\clustlamp}$.
            % TODO: Refer to full expression in appendix

        \subsection{Density matrices}
            Having found the converged $\clustamp$- and $\clustlamp$-amplitudes
            we can compute the one- and two-body density matrices as discussed
            in \autoref{subsec:cc-density-matrices}.
            These values only depend on the cluster amplitudes which means that
            NOCCD can reuse the density matrices from the static CCD-method.
            We have listed the tensor contractions involved for the one- and
            two-body density matrices in the CCD-method, and the one-body
            density matrix for the CCSD-method in
            \autoref{app:cc-density-matrices}.

        \subsection{Time-evolution with static orbitals}
            \label{subsec:tdcc-implementation}
            From \autoref{eq:time-evolution-tau} and
            \autoref{eq:time-evolution-lambda} we find the equations of motion
            for the amplitudes.
            To evolve the amplitudes in time using a numerical integration
            scheme we compute
            \begin{gather}
                \partial_t \clustamp_{\mu}
                = -i\mel*{\slat_{\mu}}{
                    \exponential(-\clust(t))
                    \hamil(t)
                    \exponential(\clust(t))
                }{\slat},
                \\
                \partial_t \clustlamp_{\mu}
                = i\mel*{\slat}{
                    (\1 + \clustl(t))
                    \exponential(-\clust(t))
                    \com{\hamil(t)}{\hat{X}_{\mu}}
                    \exponential(\clust(t))
                }{\slat},
            \end{gather}
            where we've set $\hslash = 1$ in atomic units.
            Sans the imaginary number and sign these are just the $\clustamp$-
            and $\clustlamp$-amplitude equations as used in the time-independent
            case.
            However, the Hamiltonian and the amplitudes themselves are the
            time-evolved states.
            The explicit tensor contractions for CCD and CCSD are listed in
            \autoref{app:cc-tau-amplitudes} and
            \autoref{app:cc-lambda-amplitudes}.

            Now, the integration schemes assume a vector of derivatives instead
            of a set of rank $2$ and rank $4$ tensors.
            We therefore ravel the amplitude tensors using \pyth{np.ravel} from
            NumPy to represent them as a one-dimensional array.
            We then concatenate the arrays as a single long one-dimensional
            array.
            However, we've formulated all our amplitude equations using the rank
            $2$ and $4$ tensors and we must transform the input vector with
            amplitudes from the integration schemes to the correct shape of the
            amplitudes when solving the right-hand sides.
            To do this we've created a Python class called
            \pyth{AmplitudeContainer} which defines convenience functions
            handling the reshaping of the amplitudes.
            This class was originally intended to provide operators such as
            \pyth{__add__}, \pyth{__mul__}, etc, to allow direct manipulation of
            the amplitudes inside the containers when calling differential
            equation solvers.
            This worked relatively right out of the box for the Runge-Kutta 4
            scheme, but once we included the Gauss-integrator with more complex
            manipulations of the solution vector we were forced to move to the
            -- in hindsight -- much smarter solution of making all the
            amplitudes into a single vector.

            Using the Python dunder method \pyth{__iter__} we make
            \pyth{AmplitudeContainer} into a generator object, viz.
            \begin{python}
class AmplitudeContainer:
    # Code removed for clarity

    def __iter__(self):
        yield self._t
        yield self._l

    def unpack(self):
        yield from self._t
        yield from self._l
            \end{python}
            where \pyth{self._t} and \pyth{self._l} are Python lists with the
            $\clustamp$- and $\clustlamp$-amplitudes respectively.
            The latter method \pyth{unpack} allows us to iterate over all the
            amplitudes in a for-loop.
            For CCSD \pyth{self._t} will be a list of three amplitudes,
            $\clustamp_0$, $\clustamp^{a}_{i}$ and $\clustamp^{ab}_{ij}$, where
            the $\clustamp_0$ is the phase defined in \autoref{subsec:cc-phase}.
            The \pyth{self._l}-list will contain the two amplitudes
            $\clustlamp^{i}_{a}$ and $\clustlamp^{ij}_{ab}$.
            The method for stacking all the amplitudes into a vector is now
            given by \autoref{alg:cc-asarray}.
            \begin{algorithm}
                \begin{python}
class AmplitudeContainer:
    # Code removed for clarity

    def asarray(self):
        np = self.np

        amp_vec = np.zeros(self.n)
        start_index = 0
        stop_index = 0

        for amp in self.unpack():
            start_index = stop_index
            stop_index += amp.size

            try:
                amp_vec[start_index:stop_index] += amp.ravel()
            except TypeError:
                amp_vec = amp_vec.astype(amp.dtype)
                amp_vec[start_index:stop_index] += amp.ravel()

        return amp_vec
                \end{python}
                \caption{Function in \pyth{AmplitudeContainer} building a single
                vector with all coupled-cluster ampltides stacked on top of one
                another.}
                \label{alg:cc-asarray}
            \end{algorithm}
            The inverse operation where we go from a vector of amplitudes to two
            sets of amplitudes is demonstrated in \autoref{alg:cc-from_array}.
            Note that this is a static method which means that the function is
            available outside an instance of the container object.
            This lets us build the container by comparing with an existing
            container denoted \pyth{u} where we assume that the dimensionality
            are the same in both containers.
            \begin{algorithm}
                \begin{python}
class AmplitudeContainer:
    # Code removed for clarity

    @staticmethod
    def from_array(u, arr):
        np = u.np

        args = []
        start_index = 0
        stop_index = 0

        for amps in u:
            inner = []

            if type(amps) == list:
                for amp in amps:
                    start_index = stop_index
                    stop_index += amp.size

                    inner.append(
                        arr[start_index:stop_index].reshape(
                            amp.shape
                        )
                    )
            else:
                start_index = stop_index
                stop_index += amps.size
                inner = arr[start_index:stop_index].reshape(
                    amps.shape
                )

            args.append(inner)

        return type(u)(*args, np=np)
                \end{python}
                \caption{Function in \pyth{AmplitudeContainer} building lists
                amplitudes and reshaping them into the correct rank.}
                \label{alg:cc-from_array}
            \end{algorithm}

        \subsection{Time-dependent energy}
            The time-dependent energy for the coupled-cluster method is computed
            in exactly the same way as for the variational ground state solvers,
            that is, by computing the Lagrangian from
            \autoref{eq:cc-energy-functional}.

        \subsection{Time-dependent overlap}
            Due to the dual state of $\ket{\Psi}$ in coupled-cluster not being
            the adjoint, we have that
            \begin{align}
                \braket*{\tilde{\Psi}}{\Psi}
                \neq \braket*{\Psi}{\tilde{\Psi}}^{*}.
            \end{align}
            This means that there is an inherent ambiguity in the calculation of
            the autocorrelation as defined \autoref{eq:autocorrelation} where
            $A(t, t_0) \neq A^{*}(t_0, t)$.
            To get around this we force hermiticity by computing the
            autocorrelation from \cite{pedersen2018symplectic}
            \begin{align}
                A(t, t_0)
                \equiv \half\para{
                    \braket*{\tilde{\Psi}(t)}{\Psi(t_0)}
                    + \braket*{\tilde{\Psi}(t_0)}{\Psi(t)}^{*}
                }.
            \end{align}
            The time-dependent overlap can then be found by
            \begin{align}
                P(t, t_0)
                = \abs{A(t, t_0)}^2.
            \end{align}
            In \autoref{app:cc-autocorrelation} we've added explicit tensor
            contractions for both CCD and CCSD in order to compute the
            coupled-cluster autocorrelation.

        \subsection{Orbital-adaptive time-evolution}
            \label{subsec:oatdcc-implementation}
            We have in this thesis implemented the doubles truncation of the
            orbital-adaptive time-dependent coupled-cluster family; the
            OATDCCD-method.
            This method uses the NOCCD-method as its ground state solver due to
            the lack of an existing ground state OACCD-method
            \cite{kvaal2012ab}.
            Thus, after we have computed the ground state using the
            NOCCD-method, we change to this basis using the converged $\vfg{S}$
            and $\tilde{\vfg{S}}$ as coefficient matrices with the latter
            serving as the left-hand coefficients.

            % TODO: Explain that D_0 is zero for OATDCCD
            To propagate the cluster amplitudes and the orbitals in time we
            start by noticing that
            \begin{align}
                \mathcal{E}[
                    \vfg{\clustamp},
                    \vfg{\clustlamp},
                    \slat,
                    \tilde{\slat}
                ]
                = \mel*{\tilde{\Psi}}{
                    \para{
                        \hamil
                        - i\hslash\hat{\eta}
                    }
                }{\Psi}
                = \mel*{\tilde{\Psi}}{
                    \hamil
                }{\Psi},
            \end{align}
            in the doubles approximation.
            This is a consequence of the gauge condition imposed on the orbital
            rotations with the occupied-occupied and virtual-virtual blocks of
            $\eta^{p}_{q}$ being zero, viz.
            \begin{align}
                \eta^{a}_{b} = \eta^{i}_{j} = 0,
            \end{align}
            and from the CCD one-body density matrix.
            In \autoref{app:ccd-density-matrices} we demonstrate that
            \begin{align}
                \densityten^{a}_{i} = \densityten^{i}_{a} = 0,
            \end{align}
            which means that
            \begin{align}
                \mel*{\tilde{\Psi}}{\hat{\eta}}{\Psi}
                =
                \eta^{p}_{q}
                \mel*{\tilde{\Psi}}{\biccr{p}\bican{q}}{\Psi}
                = \eta^{p}_{q}\densityten^{q}_{p}
                = 0.
            \end{align}
            This means that the right-hand side evaluation in the OATDCCD-method
            now consists of evaluating the $\clustamp$- and
            $\clustlamp$-amplitudes similarly as in the time-dependent static
            orbital coupled-cluster methods and we can reuse the machinery from
            the TDCCD-method to propagate the amplitudes.
            We then move on to evolving the orbitals in time.
            This means that we must solve the $P$-space and $Q$-space equations
            as discussed in \autoref{sec:oatdcc}.
            The equations of motion for the orbitals can then be expressed as
            \cite{kvaal2012ab}
            \begin{gather}
                \partial_t\ket*{\phi_p}
                = (\hat{P} + \hat{Q})\partial_t\ket*{\phi_p}
                = \eta^{q}_{p}\ket*{\phi_q}
                + \hat{Q}\partial_t\ket*{\phi_p},
                \\
                \partial_t\bra*{\tilde{\phi}_p}
                = \partial_t\bra*{\tilde{\phi}_p}(\hat{P} + \hat{Q})
                = -\eta^{p}_{q}\bra*{\tilde{\phi}_q}
                + \partial_t\bra*{\tilde{\phi}_p}\hat{Q},
            \end{gather}
            where we in the second equation used that
            \begin{align}
                \eta^{p}_{q}
                = \mel*{\tilde{\phi}_{p}}{\partial_t}{\phi_q}
                = -\para{
                    \partial_t \bra*{\tilde{\phi}_{p}}
                }\ket*{\phi_q}.
            \end{align}
            We start by solving the $P$-space equations, which means that we
            need to compute the one- and two-body density matrices.
            These are listed in \autoref{app:cc-density-matrices}.
            From the right-hand sides of \autoref{eq:eta-jb} and
            \autoref{eq:eta-bj} we can construct the two matrices $R^{i}_{a}$
            and $R^{a}_{i}$ respectively, that is,
            \begin{gather}
                R^{i}_{a}
                = \oneten^{p}_{a}\densityten^{i}_{p}
                - \oneten^{i}_{q}\densityten^{q}_{a}
                + \half\twoten^{pq}_{is}\densityten^{as}_{pq}
                - \half\twoten^{aq}_{rs}\densityten^{sr}_{iq},
                \\
                \tilde{R}^{a}_{i}
                =
                \oneten^{p}_{i}\densityten^{a}_{p}
                -
                \oneten^{a}_{q}\densityten^{q}_{i}
                +
                \half
                \twoten^{pq}_{is}\densityten^{sa}_{pq}
                -
                \half
                \twoten^{aq}_{rs}\densityten^{sr}_{iq},
            \end{gather}
            where we note that $\partial_t\densityten^{a}_{i} = 0$ in the
            doubles approximation.
            Next we construct the coefficient matrix $\vfg{A}$ by
            \begin{align}
                A^{ib}_{aj}
                \equiv \delta^{b}_{a}\densityten^{i}_{j}
                - \delta^{i}_{j}\densityten^{b}_{a}.
            \end{align}
            We can now formulate the $P$-space equations quite succintly by
            \begin{gather}
                iA^{ib}_{aj}\eta^{j}_{b} = R^{i}_{a},
                \label{eq:eta-jb-R}
                \\
                -i\eta^{b}_{j}A^{ja}_{bi} = \tilde{R}^{a}_{i},
                \label{eq:eta-bj-R}
            \end{gather}
            where atomic units are assumed.
            Here \autoref{eq:eta-jb-R} and \autoref{eq:eta-bj-R} are two linear
            equations for $\eta^{j}_{b}$ and $\eta^{b}_{j}$ respectively.
            To solve these equations we can either create compound indices such
            that $\vfg{\eta}$, $\vfg{R}$, and $\tilde{\vfg{R}}$ become vectors,
            and similarly for $\vfg{A}$ as a matrix.
            We use NumPy's method \pyth{np.linalg.tensorsolve} \cite{numpy} to
            solve the linear equation more or less as they stand and let NumPy
            handle the dimensionality transformations.
            However, we do need to transpose the axes in the $A^{ib}_{aj}$
            array as we use a fixed ordering of the elements.
            For \autoref{eq:eta-jb-R} we have
            \begin{align}
                A^{ib}_{aj} \to A^{ai}_{jb} \equiv \vfg{A}_r,
            \end{align}
            whereas for \autoref{eq:eta-bj-R} we do
            \begin{align}
                A^{ib}_{aj} \to A^{bj}_{ai} \equiv \vfg{A}_l.
            \end{align}
            As matrices and vectors this then translates to the matrix equations
            \begin{gather}
                i \vfg{A}_r \vfg{\eta} = \vfg{R},
                \\
                -i\tilde{\vfg{\eta}}\vfg{A}_l = \tilde{\vfg{R}},
            \end{gather}
            where we've marked the virtual-occupied block of $\eta^{p}_{q}$ by a
            tilde to distinguish it from merely the transpose of the
            occupied-virtual block.
            Solving both these equations yield the two non-zero blocks of
            $\eta^{p}_{q}$.
            % TODO: Add code example of this? The compute_eta-function maybe?

            Having found $\eta^{p}_{q}$ we can now solve the $Q$-space
            equations found in \autoref{eq:ket-q} and \autoref{eq:bra-q}.
            Recalling that
            \begin{align}
                \hat{Q} \equiv \1 - \hat{P}
                = \1 - \dyad*{\phi_p}{\tilde{\phi}_p},
            \end{align}
            where the sum over the orbitals can be truncated to further lower
            the computational cost of the OATDCCD-method.
            In this thesis we have not included any studies where we truncate
            the orbital basis further.
            The $Q$-space equations therefore simplifies as the right-hand side
            of \autoref{eq:ket-q} and \autoref{eq:bra-q} will be zero.
            A demonstration of this fact is shown in
            \autoref{app:untruncated-q-space}.
            The $Q$-space equations thus reduce to
            \begin{gather}
                i\densityten^{q}_{p}\hat{Q}\partial_t\ket*{\phi_q} = 0
                \implies
                \partial_t\ket*{\phi_q}
                = \eta^{r}_{q}\ket*{\phi_r},
                \\
                -i\densityten^{q}_{p}\para{
                    \partial_t\bra*{\tilde{\phi}_q}
                }\hat{Q}
                = 0
                \implies
                \partial_t\bra*{\tilde{\phi}_q}
                = -\bra*{\tilde{\phi}_r}\eta^{q}_{r}.
            \end{gather}
            We denote the biorthonormal basis of single-particle states from
            NOCCD by $\brac{\chi_{\alpha}}$ where we have
            \begin{align}
                \braket*{\tilde{\chi}_{\alpha}}{\chi_{\beta}}
                = \delta_{\alpha \beta}.
            \end{align}
            The time-dependent orbitals can then be constructed from the ground
            state orbitals by
            \begin{gather}
                \ket*{\phi_p(t)}
                = C_{\alpha p}(t) \ket*{\chi_{\alpha}},
                \\
                \bra*{\tilde{\phi}_p(t)}
                = \tilde{C}_{p \alpha}(t) \bra*{\tilde{\chi}_{\alpha}},
            \end{gather}
            where the time-dependence is kept in the coefficients.
            The equations of motion for the coefficients is then found by
            projecting onto the ground state orbital basis.
            This gives
            \begin{gather}
                \dot{C}_{\alpha q} = C_{\alpha r} \eta^{r}_{q}
                \implies
                \dot{\vfg{C}} = \vfg{C} \vfg{\eta},
                \\
                \dot{\tilde{C}}_{q \alpha}
                = -\eta^{q}_{r} \tilde{C}_{r \alpha}
                \implies
                \dot{\tilde{\vfg{C}}}
                = -\vfg{\eta} \tilde{\vfg{C}},
            \end{gather}
            where we denote the derivative of the coefficient matrices in time
            by a dot.

            In order to use known numerical integration schemes we collect all
            the amplitudes and the coefficients together in a single vector
            as done for the regular time-dependent coupled-cluster methods.
            In fact, we are able to reuse the \pyth{AmplitudeContainer}-class by
            creating a subclass called \pyth{OACCVector} which only needs new
            \pyth{__iter__} and \pyth{unpack} methods thanks to the generality
            of \pyth{AmplitudeContainer}.
            These are listed in \autoref{alg:oaccvector-iter}.
            \begin{algorithm}
                \begin{python}
class OACCVector(AmplitudeContainer):
    # Code removed for clarity

    def __iter__(self):
        yield self._t
        yield self._l
        yield self._C
        yield self._C_tilde

    def unpack(self):
        yield from super().unpack()
        yield self._C
        yield self._C_tilde
                \end{python}
                \caption{Iterator and unpacking methods for \pyth{OACCVector}.}
                \label{alg:oaccvector-iter}
            \end{algorithm}
            We are then able to reuse the \pyth{asarray} and \pyth{from_array}
            methods from \pyth{AmplitudeContainer}.
            Thus we can reuse the already implemented differential equation
            solvers.

        \subsection{Measuring quantities}
            More or less all measurable quantities can be computed in the same
            manner as for the regular time-dependent coupled-cluster method, but
            we must remember to change the basis of the matrix elements as
            discussed in \autoref{sec:change-of-basis}.

        \subsection{Time-dependent overlap}
            A significant drawback of the orbital-adaptive time-dependent
            coupled-cluster family of methods are their inability to compute the
            time-dependent overlap, or the autocorrelation, in polynomial time.
            The main reason for this is that the overlap is computed at two
            separate times which yield two different sets of operators.
            We can see this from
            \begin{gather}
                \ket*{\phi_p(t)}
                = \biccr{p}(t)\ket*{\vac}
                = C_{\alpha p}(t) \ket*{\chi_{\alpha}}
                = C_{\alpha p}(t) \biccr{\alpha}(0)\ket*{\vac}
                \\
                \implies
                \biccr{p}(t) = C_{\alpha p}(t) \biccr{\alpha}(0),
                \\
                \implies
                \bican{p}(t) = \tilde{C}_{p \alpha}(t) \bican{\alpha}(0),
            \end{gather}
            where we've expressed the time-evolved biorthonormal second
            quantized operators as a linear combination of the initial
            operators.
            Now, the biorthonormality condition is only maintained at equal
            times which means that
            \begin{align}
                \acom{\biccr{p}(t)}{\bican{q}(t_0)} \neq \delta_{pq},
            \end{align}
            in general.
            To get around this we express all operators in terms of the initial
            operators and use the coefficient matrices to keep track of the
            time-dependence.
            Looking at the doubles cluster amplitudes we then have
            \begin{align}
                \clust_2(t)
                &= \clustamp^{ab}_{ij}(t)
                \biccr{a}(t)\biccr{b}(t)\bican{j}(t)\bican{i}(t)
                = \clustamp^{\alpha \beta}_{\gamma \delta}(t)
                \biccr{\alpha}(0)\biccr{\beta}(0)
                \bican{\delta}(0)\bican{\gamma}(0),
                \label{eq:transformed-cluster-operator}
            \end{align}
            where we've defined the transformed cluster amplitudes
            \begin{align}
                \clustamp^{\alpha \beta}_{\gamma \delta}(t)
                \equiv
                \clustamp^{ab}_{ij}(t)
                C_{\alpha a}(t)
                C_{\beta b}(t)
                \tilde{C}_{j\delta}(t)
                \tilde{C}_{i\gamma}(t).
            \end{align}
            An imporant point to note in
            \autoref{eq:transformed-cluster-operator}
            is that the sum over the greek indices run over the entire basis set
            of initial orbitals $\brac{\chi_{\alpha}}$.
            This means that the $\clust$-operators no longer are excitation
            operators in the usual sense, and similarly for the
            $\clustl$-operators no longer being relaxation operators.
            At equal times the biorthonormality condition is fulfilled by the
            coefficients.
            As this is not the case for unequal times, the overlap can be found
            by
            \begin{align}
                \braket*{\tilde{\Psi}(t)}{\Psi(t_0)}
                =
                \mel*{\tilde{\slat}(t)}{
                    (
                        \1 + \clustl(t)
                    )
                    \exponential(-\clust(t))
                    \exponential(\clust(t_0))
                }{\slat(t_0)},
            \end{align}
            where the unequal times in the reference determinants remove our
            ability to use the Fermi vacuum formalism as the right and left
            reference determinants are defined in terms of different second
            quantized operators.
            This problem resembles the variational formulation of the
            coupled-cluster method as discussed in
            \autoref{subsec:non-variational-coupled-cluster}.

            This means that we are unable to compute the spectrum of energy
            levels of a system.
            However, we are still able to compute the energy transitions using
            the dipole moment.
