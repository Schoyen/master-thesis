\chapter{Computational aspects}
    Along with this thesis we have implemented several quantum-mechanical
    solvers that are separated into different Github repositiories.\footnote{%
        Due to ongoing publications using the code most of the repositiories are
        at the time of writing private and access are therefore limited to
        collaborators.
        However, please do request access by sending a mail to:
        \href{mailto:o.s.schoyen@fys.uio.no}{o.s.schoyen@fys.uio.no}, and we'll
        set you up.
    }
    \begin{itemize}
        \item Quantum-systems is a Python package containing modules to set up
            matrix elements, time-evolution operators, and single-particle
            states to be used by the many-body solvers.
            Quantum-systems provides interfaces to the PySCF \cite{pyscf} and
            Psi4 \cite{psi4} systems.
            The code is located at
            \url{https://github.com/Schoyen/quantum-systems} with the
            documentation at
            \url{https://schoyen.github.io/quantum-systems/}.
        \item Coupled-cluster is a Python package with modules containing ground
            state and time-dependent coupled cluster solvers.
            Currently this packages contains doubles, singles-and-doubles ,
            non-orthogonal coupled cluster doubles, and the orbital adaptive
            time-dependent coupled cluster doubles methods.
            The module uses quantum-systems to get access to matrix elements.
            The code is located at
            \url{https://github.com/Schoyen/coupled-cluster} with the
            documentation at
            \url{https://schoyen.github.io/coupled-cluster/}.
        \item Configuration-interaction is a package containing ground state and
            time-dependent configuration interaction solvers.
            This code supports arbitrary levels of excitations, e.g.,
            singles-and-doubles, doubles-and-triples, etc, and full
            configuration interaction.
            The module uses quantum-systems to get access to matrix elements.
            The code is located at
            \url{https://github.com/Schoyen/configuration-interaction} with
            the documentation at
            \url{https://schoyen.github.io/configuration-interaction/}.
        \item Hartree-Fock is a package containing ground state and
            time-dependent Hartree-Fock solvers.
            We've implemented Hartree-Fock for general, restricted and
            unrestricted spin-orbitals.
            The module uses quantum-systems to get access to matrix elements.
            The code is located at
            \url{https://github.com/Schoyen/hartree-fock} with
            the documentation at
            \url{https://schoyen.github.io/hartree-fock/}.
    \end{itemize}
    We will in the rest of this chapter discuss various aspects of the
    implementation that we deem important to elaborate on, but we will leave
    specific implementation details and usage of the solvers to the
    repositiories and the documentation.

    \section{Why Python?}
        In working with this thesis we have developed a large computational
        framework for performing real-time quantum mechanics simulations for
        many-body problems in the programming language Python \cite{python}.
        The choice of using Python comes with a list of pros and cons.
        \begin{itemize}
            \item The development time is much lower when using Python as
                opposed to more verbose, but efficient languages such as C/C++,
                and Fortran.
            \item Integration with other Python libraries are relatively easy.
            \item Libraries such as SciPy \cite{scipy}, NumPy \cite{numpy}, and
                SymPy \cite{sympy} provides fast, and efficient interfaces to
                BLAS and LAPACK along with convenient array abstractions.
        \end{itemize}

    \section{Computing tensor contractions}
        Quite a significant amount of computational resources will go into the
        evaluation of tensor contractions\footnote{%
            We tend to call the matrix elements for tensors, but they more
            resemble numerical $N$-dimensional arrays.%
            % TODO: Check this footnote.
        } and we will therefore spend some time discussing how these
        contractions are performed and how we can speed up the contractions.

        Consider the antisymmetric, two-body, Coulomb elements given by
        \begin{align}
            \twoten^{pq}_{rs}
            = \mel{\phi_p\phi_q}{\twohamil}{\phi_r\phi_s}_{AS},
        \end{align}
        which, along with the two-body density matrix, is the largest tensor in
        use.
        This tensor often represents the bottleneck both in terms of memory and
        contraction time.
        When we represent these tensors mathematically, the labelling of the
        indices are in some sense arbitrary and depends on the context that the
        tensors are used.
        On a computer we however need to decide on a specific way of storing
        memory, and often this choice is related to speed concerns where some
        storage schemes show vast improvements in terms of cache hits as opposed
        to other schemes.
        However, tensor contractions are notoriously difficult to handle in
        terms of memory as they often involve change of dimensionality,
        re-ordering of indices resulting in the need of reshapes, and summation
        along axes that are inefficiently laid out in memory.
        % TODO: Add demonstration of this from CCD

        For the sake of generality we therefore ignore much of these problems
        and have decided to use a fixed layout of the memory and absorb the cost
        of reshapes and memory allocations.
        In our programs we read the axis from top-left and moving right before
        starting on bottom-left and going right.
        That is, to access element $\twoten^{pq}_{rs}$ we use the ordering
        \pyth{u[p, q, r, s]} in NumPy-arrays \cite{numpy}.
        This ordering is used consistenly for all tensors in all solver
        implementations.
        Other orderings might be smarter due to efficient usage of cache hits,
        but this clutters much of the implementation and is therefore ignored.

        \subsection{Intermediates}
            It is common in the coupled cluster litterature to talk about
            \emph{intermediates} \cite{hjorth2017advanced, gauss1995coupled}, or
            intermediate computations, as a technique for speeding up tensor
            contractions involving three or more tensors.
            The basics of intermediate computations is to treat a tensor
            contraction as a binary operation and precomputing common factors,
            or one of the contractions.
            As an example, consider the D3c term, sans the permutation opeator,
            from the coupled cluster doubles amplitudes in
            \autoref{tab:ccd-tau-amplitude-terms},
            \begin{align}
                g^{ab}_{ij} = \clustamp^{ab}_{lj} \clustamp^{dc}_{ik}
                \twoten^{kl}_{cd}.
            \end{align}
            The na√Øve solution using explicit for-loops yields a
            $\mathcal{O}(M^4 N^4)$-complexity.
            By first creating the intermediate contraction
            \begin{align}
                W^{l}_{i} = \clustamp^{dc}_{ik} \twoten^{kl}_{cd},
            \end{align}
            and then computing the total result from
            \begin{align}
                g^{ab}_{ij} = \clustamp^{ab}_{lj} W^{l}_{i},
            \end{align}
            we've reduced the complexity to $\mathcal{O}(M^2 N^3)$.
            This incurs a memory penalty from the temporary storage of
            $W^{l}_{i}$, but the gain in reduction of the number of FLOPS far
            exceeds this cost.

            The choice of which terms to use when creating intermediates has
            been explored in some depth, especially in the case of the
            coupled-cluster singles-and-doubles as done by
            \citeauthor{gauss1995coupled} \cite{gauss1995coupled}.
            We will however not employ pre-defined intermediates, but rather use
            the binary operator \pyth{np.tensordot} \cite{numpy} to do
            contractions.
            This forces us to pre-compute terms with three or more tensors thus
            lowering the cost.
            However, some care must be taken as the optimal choice depends on
            which terms are to be contracted.
            By inspection we choose the contractions which will yield the lowest
            amount of computational complexity by counting the number of unique
            indices and the lowest amount of storage cost.


    \section{Convergence acceleration}
        \label{sec:convergence}
        When performing optimization techniques such as the quasi-Newton method
        for the coupled-cluster equations and the self-consistent field
        iterations in Hartree-Fock, we often find that the solutions can have
        trouble converging.
        To alleviate some of these convergence issues we introduce two
        techniques which often lets us converge faster, or in some cases,
        converge at all.

        \subsection{Alpha filter}
            \label{subsec:alpha-filter}
            A first order approximation stems from data estimation theory as an
            alternative to the more sophisticated Kalman filter.
            This is a technique which lets us combine a predicted value and a
            measured value.
            % TODO: Cite slides:
            % https://www.uio.no/studier/emner/matnat/fys/FYS3240/v19/lecturenotes/l12---data-fusion.pdf
            Given a measurement $\vfg{x}^{(i)}$ at a time $i$ we can create an
            updated estimate $\bar{\vfg{z}}^{(i)}$ from a predicted estimate
            $\vfg{z}^{(i)}$ by
            \begin{align}
                \bar{\vfg{z}}^{(i)} = (1 - \alpha) \vfg{z}^{(i)}
                + \alpha \vfg{x}^{(i)},
            \end{align}
            where $\alpha \in \brak{0, 1}$ is a gain parameter.
            We have in our implementations perhaps (mis)named this filter for
            alpha mixing, as we ``mix'' some of the predicted and measured
            values in a new estimate.
            Note that for $\alpha = 0$ we only keep the predicted value
            $\vfg{z}^{(i)}$ whereas for $\alpha = 1$ we keep the raw
            measurements $\vfg{x}^{(i)}$.
            The alpha filter suffers from the fact that finding a good value for
            $\alpha$ is largely decided by trial and error.
            In our code we have dubbed the measurement vector by
            \pyth{trial_vector} and the predicted estimate by
            \pyth{direction_vector}.

        \subsection{Direct inversion of the iterative subspace}
            \label{subsec:diis}
            A more sophisticated acceleration technique is DIIS (direction
            inversion of the iterative subspace) acceleration
            \cite{pulay1980393, pulay1982, helgaker-molecular, shepard-diis}.
            In order to estimate a measured vector $\vfg{p}_{i + 1}$ at a
            certain step $i + 1$ we use the linear combination
            \begin{align}
                \bar{\vfg{p}}_{i + 1} = \sum_{k = 1}^{i} c_k \vfg{p}_{i},
                \label{eq:diis-estimate}
            \end{align}
            where $\bar{\vfg{p}}_{i + 1}$ is the estimated value at step $i +
            1$, and $c_k$ is a set of unknown coefficients subject to the
            constraint that they should sum up to unity at every step $i$.
            In order to find the coefficients, we construct an error vector
            $\vfg{e}_i$ from $\vfg{p}_i$.
            This step is dependent on the solver we are looking at and will be
            postponed to the implementation chapters on Hartree-Fock and
            coupled-cluster.
            For now, consider the extrapolated error vector
            \begin{align}
                \bar{\vfg{e}}_{i + 1}
                = \sum_{k = 1}^{i} c_k \vfg{e}_k,
            \end{align}
            calculated from the measured error vectors.
            We now wish to minimize the error, and we do this using Lagrange's
            method of undetermined multipliers in order to include the
            constraint that the coefficients should sum up to unity, viz.
            \begin{align}
                L &=  \norm{\bar{\vfg{e}}_i}^2
                - 2\lambda\para{
                    \sum_{k = 1}^{i} c_i
                    - 1
                }.
                \label{eq:diis-lagrangian}
            \end{align}
            We see that this is a least squares approach where we minimize the
            error vectors subject to a constraint.
            The squared norm of the error vectors can be expressed by
            \begin{align}
                \norm{\bar{\vfg{e}}_i}^2
                = c_k B_{kl} c_l,
            \end{align}
            where we've defined the matrix elements
            \begin{align}
                B_{kl} \equiv \vfg{e}_k^T\vfg{e}_l.
            \end{align}
            Finding the stationary condition of the Lagrangian in
            \autoref{eq:diis-lagrangian} with respect to the coefficients $c_k$
            we get system of $i$ linear equations.
            This can expressed as matrices by
            \begin{align}
                \begin{pmatrix}
                    B_{11} & \dots & B_{1i} & -1 \\
                    \vdots & \ddots & \vdots & \vdots \\
                    B_{i1} & \dots & B_{ii} & -1 \\
                    -1 & \dots & -1 & 0
                \end{pmatrix}
                \begin{pmatrix}
                    c_1 \\
                    \vdots \\
                    c_i \\
                    \lambda
                \end{pmatrix}
                = \begin{pmatrix}
                    0 \\
                    \vdots \\
                    0 \\
                    -1
                \end{pmatrix}.
            \end{align}
            Solving this equation for the coefficients $c_k$ we are able to
            compute the estimated quantity $\bar{\vfg{p}}_i$ from
            \autoref{eq:diis-estimate}.
            An existing implementation of the DIIS algorithm was given to us by
            \citeauthor{rolf-nocc} as part of his article \citetitle{rolf-nocc}
            \cite{rolf-nocc} and has been integrated by the author into the
            libraries we have created.
            This makes the method available to all Hartree-Fock solvers and all
            coupled-cluster implementations.

            The DIIS method suffers from the fact that we store all $i$ previous
            measurements in memory.
            In the case of large systems this can become a problem as we spend
            spend all our memory in the acceleration.
            We can therefore adjust the number of vectors, i.e., $i$, in order
            to limit the memory occupied by DIIS.

    \section{Numerical integration}
        \label{sec:numerical-integration}
        In this section we'll review a select few time-integration schemes for
        solving time-dependent ordinary differential equations and we'll discuss the
        applicability of each scheme.

        \subsection{Problem statement}
            The problem we are trying to solve can be formulated as the
            two coupled time-dependent Schr√∂dinger equations on the form
            \begin{gather}
                \dpd{}{t}\ket{\psi(t)}
                = -i\hamil(t)\ket{\psi(t)},
                \\
                \bra{\psi(t)}\dpd{}{t}
                = i\bra{\psi(t)}\hamil(t),
            \end{gather}
            where we've assumed atomic units, and moved $\pm i$ to the right-hand
            side.
            In the case of the time-dependent Hartree-Fock and the
            time-dependent configuration interaction methods, we only need to
            evaluate the former Schr√∂dinger equation as the wave functions are
            variational and $\bra{\psi(t)}$ will be the adjoint of
            $\ket{\psi(t)}$.
            For the coupled cluster methods we need to solve both equations as
            the bra and ket sides of the wave function are no longer the adjoint
            of one another.
            Furthermore, in the orbital-adaptive formulation we need to include
            the time-dependence of the coefficients used in the orbital
            rotations.
            However, we can stack all the coefficients and/or the amplitudes
            after one another creating a long vector as each index in this
            vector can be updated individually as long as the coupling is
            handled in the derivatives.
            Thus, we are able to formulate the time-evolution of the
            parameters of the wave function by
            \begin{align}
                \dot{y} = f(y, t),
            \end{align}
            where $f(y, t)$ represents the right-hand side the time-dependent
            Schr√∂dinger equation, the time-dependent Hartree-Fock equation, or
            the right-hand sides of the derivative of the coupled cluster
            Lagrangian.
            Discretizing time with a constant step $h = \Delta t$ such that $t_i
            = t_0 + i h$ for $i \in \brac{0, N_t}$, where $N_t$ is the number of
            time-steps and $t_0$ is the initial time-step.
            Note that we typically choose a time-step $h$ such that $N_t$ can be
            found by
            \begin{align}
                N_t = \floor{\frac{t_f - t_0}{h}} + 1,
            \end{align}
            where $t_f$ is the final time-step.
            We denote $y_0 \equiv y(t_0)$ and $y_i \equiv y(t_i)$, where $y_0$
            is the initial value of the problem.
            Typically we choose the initial value to be the ground state of the
            specific solver.
            This abstraction thus enables us to work with known ordinary
            differential equation solvers formulated in a familiar way.

        \subsection{Runge-Kutta}
            As a first approximation to solving the time-dependent equations,
            we've implemented the Runge-Kutta 4 algorithm.
            This algorithm is given by \cite{wiki:rk4}
            \begin{gather}
                y_{i + 1} = y_i + \frac{1}{6}\para{
                    k_1 + 2 k_2 + 2 k_3 + k_4
                }, \\
                t_{i + 1} = t_i + h,
            \end{gather}
            where the equations for the $k$'s are given by
            \begin{gather}
                k_1 = h f(y_i, t_i), \\
                k_2 = h f(y_i + k_1 / 2, t_i + h / 2), \\
                k_3 = h f(y_i + k_2 / 2, t_i + h / 2), \\
                k_4 = h f(y_i + k_3, t_i + h).
            \end{gather}
            Runge-Kutta 4 is a fourth-order method with a local numerical error
            of the order of $\mathcal{O}(h^5)$ and a total error of
            $\mathcal{O}(h^4)$.

        \subsection{Symplectic integrators}
            Looking back to \autoref{sec:time-evolution-operators} we note that
            for a time-dependent Hamiltonian in the Schr√∂dinger picture, the
            time-evolution of a state $\ket{\psi(t)}$ is described by the
            time-evolution operator shown in \autoref{eq:td-evolution}, where
            the Hamiltonian in general does not commute at different time-steps.
            It is perhaps somewhat na√Øve to hope for a numerical method such as
            Runge-Kutta 4 to yield good results without some extra concern for
            the problem at hand.
            As discussed by \citeauthor{joshua-magnus} \cite{joshua-magnus} the
            solution to the time-dependent Schr√∂dinger equation can be better
            approximated by the Magnus expansion \cite{magnus-expansion}.
            This leads to implicit differential equation integration methods
            that are \emph{symplectic} \cite{joshua-magnus}.
            These methods will in a much large degree conserve the energy as
            opposed to the explicit schemes such as Runge-Kutta 4.
            % TODO: Demonstrate this.
            We will also observe that the symplectic integrator that we have
            used, in a much larger degree preserves the unitarity requirement
            by, i.e., the bi-orthonormality, of the time-dependent coefficients
            in the orbital-adaptive time-dependent coupled-cluster method.
            % TODO: Demonstrate this as well.
            One of the first approximations to the Magnus expansion is the
            Crank-Nicolson algorithm \cite{ullrich2011time}.


        \subsection{Gauss-Legendre}
            In an article on the stability of the time-dependent coupled-cluster
            singles-and-doubles on atomic systems subject to intense laser
            pulses, \citeauthor{pedersen2018symplectic}
            \cite{pedersen2018symplectic} used symplectic, implicit Runge-Kutta
            methods in an attempt to alleviate some of the stability issues they
            faced.
            As part of an ongoing publication we have been given the
            % TODO: Cite publication in progress.
            implementation of the Gauss integrator\footnote{%
                Note that the name ``Gauss integrator'' is a notoriously
                ambigious name as this can refer to a multitude of techniques
                and solvers due to the use of named polynomials such as
                Legendre, and Laguerre polynomials, etc.%
            } as used by \citeauthor{pedersen2018symplectic}
            \cite{pedersen2018symplectic} to include it in our libraries.
