\chapter{Computational aspects}
    Along with this thesis we have implemented several quantum-mechanical
    solvers that are separated into different Github repositiories.\footnote{%
        Due to ongoing publications using the code most of the repositiories are
        at the time of writing private and access is therefore limited to
        collaborators.
        However, please do request access by sending a mail to:
        \href{mailto:o.s.schoyen@fys.uio.no}{o.s.schoyen@fys.uio.no}, and we'll
        set you up.
    }
    These are:
    \begin{itemize}
        \item Quantum systems is a Python library containing modules to set up
            matrix elements, time evolution operators, and single-particle
            states to be used by the many-body solvers.
            Quantum systems provides interfaces to the PySCF \cite{pyscf} and
            Psi4 \cite{psi4} systems.
            The code is located at
            \url{https://github.com/Schoyen/quantum-systems} with the
            documentation at
            \url{https://schoyen.github.io/quantum-systems/}.
        \item Coupled-cluster is a Python library with modules containing ground
            state and time-dependent coupled-cluster solvers.
            Currently this package contains doubles (CCD/TDCCD),
            singles-and-doubles (CCSD/TDCCSD), non-orthogonal coupled-cluster
            doubles (NOCCD), and the orbital adaptive time-dependent coupled
            cluster doubles (OATDCCD) methods.
            The module uses quantum systems to get access to matrix elements.
            The code is located at
            \url{https://github.com/Schoyen/coupled-cluster} with the
            documentation at
            \url{https://schoyen.github.io/coupled-cluster/}.
        \item Configuration interaction is a library containing ground state and
            time-dependent configuration interaction solvers.
            This code supports arbitrary levels of excitations, e.g.,
            singles-and-doubles (CISD/TDCISD), doubles-and-triples
            (CIDT/TDCIDT), etc, and full configuration interaction (FCI/TDFCI).
            The module uses quantum systems to get access to matrix elements.
            The code is located at
            \url{https://github.com/Schoyen/configuration-interaction} with
            the documentation at
            \url{https://schoyen.github.io/configuration-interaction/}.
        \item Hartree-Fock is a library containing ground state and
            time-dependent Hartree-Fock solvers.
            We've implemented Hartree-Fock for general (HF/TDHF), restricted
            (RHF) and unrestricted spin-orbitals (UHF).
            The module uses quantum systems to get access to matrix elements.
            The code is located at
            \url{https://github.com/Schoyen/hartree-fock} with
            the documentation at
            \url{https://schoyen.github.io/hartree-fock/}.
    \end{itemize}
    We will in the rest of this chapter discuss various aspects of the
    implementation that we deem important to elaborate on, but we will leave
    a discussion of the quantum systems and the solvers for the next two
    chapters and the documentation.

    \section{Why Python?}
        In working with this thesis we have developed a large computational
        framework for performing real-time quantum mechanics simulations for
        many-body problems in the programming language Python \cite{python}.
        The choice of using Python comes with a list of pros and cons.
        On the pro-side we have that:
        \begin{itemize}
            \item The development time is much lower when using Python as
                opposed to more verbose, but efficient languages such as C/C++,
                and Fortran.
            \item Integration with other Python libraries is relatively easy.
            \item Libraries such as SciPy \cite{scipy}, NumPy \cite{numpy}, and
                SymPy \cite{sympy} provide fast, and efficient interfaces
                towards -- amongst others -- BLAS and LAPACK, along with
                convenient mathematical abstractions.
        \end{itemize}
        However, the use of Python comes with the price of less memory control
        and less scalability.
        The former is somewhat alleviated by using NumPy as many operations can
        be performed on already allocated arrays.
        The drawback is that this requires a keen eye, and can quickly lead to
        memory leaks which eventually triggers the Python garbage collector.
        The scalability problem is also somewhat alleviated by NumPy as many of
        the operations run in parallel via OpenMP.
        However, large scale computations requires either more sophisticated
        libraries such as Dask \cite{dask} or CuPY \cite{cupy}.

    \section{Computing tensor contractions}
        Quite a significant amount of computational resources will go into the
        evaluation of tensor contractions\footnote{%
            We tend to call the matrix elements for tensors, but they more
            resemble numerical $N$-dimensional arrays.%
            % TODO: Check this footnote.
        } and we will therefore spend some time discussing how these
        contractions are performed and how we can speed up the contractions.

        Consider the antisymmetric, two-body, Coulomb elements given by
        \begin{align}
            \twoten^{pq}_{rs}
            = \mel*{\phi_p\phi_q}{\twohamil}{\phi_r\phi_s}_{AS},
        \end{align}
        which, along with the two-body density matrix, is the largest tensor in
        use.
        This tensor often represents the bottleneck both in terms of memory and
        contraction time.
        When we represent these tensors mathematically, the labelling of the
        indices is in some sense arbitrary and depends on the context in which
        the tensors are used.
        On a computer we however need to decide on a specific way of storing
        memory, and often this choice is related to speed concerns.
        Some storage schemes show vast improvements in terms of cache hits as
        opposed to other schemes.
        However, tensor contractions are notoriously difficult to handle in
        terms of memory as they often involve change of dimensionality,
        re-ordering of indices resulting in the need of reshapes, and summation
        along axes that are inefficiently laid out in memory.

        For the sake of generality we therefore ignore much of these problems
        and have decided to use a fixed layout of the memory and absorb the cost
        of reshapes and memory allocations.
        In our programs we read the axis from top-left and moving right before
        starting on bottom-left and going right.
        That is,
        \begin{align}
            \twoten^{pq}_{rs}
            \to \text{\pyth{u[p, q, r, s]}}
        \end{align}
        where the right-hand side is the representation of the element using
        NumPy-arrays.
        This ordering is used consistenly for all tensors in all solver
        implementations.
        Other orderings might be smarter due to efficient usage of cache hits,
        but this clutters much of the implementation and is therefore ignored.

        \subsection{Intermediates}
            It is common in the coupled-cluster literature to talk about
            \emph{intermediates} \cite{gauss1995coupled, hjorth2017advanced}, or
            intermediate computations, as a technique for speeding up tensor
            contractions involving three or more tensors.
            The basics of intermediate computations is to treat a tensor
            contraction as a binary operation and precomputing common factors,
            or one of the contractions.
            As an example, consider the D3c term, sans the permutation operator,
            from the coupled-cluster doubles $\clustamp$-amplitudes in
            \autoref{tab:ccd-tau-amplitude-terms},
            \begin{align}
                g^{ab}_{ij} = \clustamp^{ab}_{lj} \clustamp^{dc}_{ik}
                \twoten^{kl}_{cd},
            \end{align}
            where we use the Fermi vacuum formalism for the indices as discussed
            in \autoref{subsec:fermi-vacuum} with $N$ the number of occupied
            states, $L$ the number of basis states, and $M = L - N$ the number
            of virtual states.
            The na√Øve solution using explicit for-loops yields an
            $\mathcal{O}(M^4 N^4)$-complexity.
            By first creating the intermediate contraction
            \begin{align}
                W^{l}_{i} = \clustamp^{dc}_{ik} \twoten^{kl}_{cd},
                \label{eq:w_li}
            \end{align}
            and then computing the total result from
            \begin{align}
                g^{ab}_{ij} = \clustamp^{ab}_{lj} W^{l}_{i},
            \end{align}
            we've reduced the complexity to $\mathcal{O}(M^2 N^3)$.
            This incurs a memory penalty from the temporary storage of
            $W^{l}_{i}$, but the gain in reduction of the number of FLOPS far
            exceeds this cost.

            The choice of which terms to use when creating intermediates has
            been explored in some depth, especially in the case of the
            coupled-cluster singles-and-doubles method as done by
            \citeauthor{gauss1995coupled} \cite{gauss1995coupled}.
            We will however not employ pre-defined intermediates, but rather use
            the binary operator \pyth{np.tensordot} from NumPy to do
            contractions.
            This forces us to pre-compute terms with three or more tensors thus
            lowering the cost.
            However, some care must be taken as the optimal choice depends on
            which terms are to be contracted.
            By inspection we choose the contractions which will yield the lowest
            amount of computational complexity by counting the number of unique
            indices and the lowest amount of storage cost.
            As an example of the converse, consider again the D3c term shown above,
            where we chose the intermediate $W^{l}_{i}$ in \autoref{eq:w_li}.
            Another choice for an intermediate would be a contraction with the
            other $\clustamp$-amplitude and the two-body elements, viz.
            \begin{align}
                W^{abk}_{lcd} = \clustamp^{ab}_{lj} \twoten^{kl}_{cd},
            \end{align}
            or even worse, the intermediate constructed from both
            $\clustamp$-amplitudes
            \begin{align}
                W^{abdc}_{ljik} = \clustamp^{ab}_{lj} \clustamp^{dc}_{ik}.
            \end{align}
            The former expression incurs an extra memory penalty of
            $\mathcal{O}_M(M^4 N^2)$ along with a computational complexity of
            $\mathcal{O}(M^4 N^2)$.
            The latter takes up $\mathcal{O}_M(M^4 N^4)$ memory and
            $\mathcal{O}(M^4 N^2)$ computational complexity.
            The tactic is thus to choose intermediates which contain the
            smallest amount of axes and of these as many as possible should be
            occupied axes as we often have $M > N$.


    \section{Convergence acceleration}
        \label{sec:convergence}
        When performing optimization techniques such as the quasi-Newton method
        for the coupled-cluster equations and the self-consistent field
        iterations in Hartree-Fock, we often find that the solutions can have
        trouble converging.
        To alleviate some of these convergence issues we introduce two
        techniques which often lets us converge faster, or in some cases,
        converge at all.

        \subsection{Alpha filter}
            \label{subsec:alpha-filter}
            A first-order approximation stems from data estimation theory as an
            alternative to the more sophisticated Kalman filter
            \cite{brookner1998tracking}.
            This is a technique which lets us combine a predicted value and a
            measured value.
            Given a measurement $\vfg{x}^{(i)}$ at a time $i$ we can create an
            updated estimate $\bar{\vfg{z}}^{(i)}$ from a predicted estimate
            $\vfg{z}^{(i)}$ by
            \begin{align}
                \bar{\vfg{z}}^{(i)} = (1 - \alpha) \vfg{z}^{(i)}
                + \alpha \vfg{x}^{(i)},
            \end{align}
            where $\alpha \in \brak{0, 1}$ is a gain parameter.
            We have in our implementations perhaps (mis)named this filter for
            alpha mixing, as we ``mix'' some of the predicted and measured
            values in a new estimate.
            Note that for $\alpha = 0$ we only keep the predicted value
            $\vfg{z}^{(i)}$ whereas for $\alpha = 1$ we keep the raw
            measurements $\vfg{x}^{(i)}$.
            The alpha filter suffers from the fact that finding a good value for
            $\alpha$ is largely decided by trial and error.
            In our code we have dubbed the measurement vector by
            \pyth{trial_vector} and the predicted estimate by
            \pyth{direction_vector}.

        \subsection{Direct inversion of the iterative subspace}
            \label{subsec:diis}
            A more sophisticated acceleration technique is DIIS (direction
            inversion of the iterative subspace) acceleration
            \cite{pulay1980393, pulay1982, helgaker-molecular, shepard-diis}.
            In order to estimate a measured vector $\vfg{p}_{i + 1}$ at a
            certain step $i + 1$ we use the linear combination
            \begin{align}
                \bar{\vfg{p}}_{i + 1} = \sum_{k = 1}^{i} c_k \vfg{p}_{i},
                \label{eq:diis-estimate}
            \end{align}
            where $\bar{\vfg{p}}_{i + 1}$ is the estimated value at step $i +
            1$, and $c_k$ is a set of unknown coefficients subject to the
            constraint that they should sum up to unity at every step $i$.
            In order to find the coefficients, we construct an error vector
            $\vfg{e}_i$ from $\vfg{p}_i$.
            This step is dependent on the solver we are looking at and will be
            postponed to the implementation chapters on Hartree-Fock and
            coupled-cluster.
            For now, consider the extrapolated error vector
            \begin{align}
                \bar{\vfg{e}}_{i + 1}
                = \sum_{k = 1}^{i} c_k \vfg{e}_k,
            \end{align}
            calculated from the measured error vectors.
            We now wish to minimize the error, and we do this using Lagrange's
            method of undetermined multipliers in order to include the
            constraint that the coefficients should sum up to unity, viz.
            \begin{align}
                L &=  \norm{\bar{\vfg{e}}_i}^2
                - 2\lambda\para{
                    \sum_{k = 1}^{i} c_i
                    - 1
                }.
                \label{eq:diis-lagrangian}
            \end{align}
            We see that this is a least squares approach where we minimize the
            error vectors subject to a constraint.
            The squared norm of the error vectors can be expressed by
            \begin{align}
                \norm{\bar{\vfg{e}}_i}^2
                = c_k B_{kl} c_l,
            \end{align}
            where we've defined the matrix elements
            \begin{align}
                B_{kl} \equiv \vfg{e}_k^T\vfg{e}_l.
            \end{align}
            Finding the stationary condition of the Lagrangian in
            \autoref{eq:diis-lagrangian} with respect to the coefficients $c_k$
            we get a system of $i$ linear equations.
            This can be expressed as matrices by
            \begin{align}
                \begin{pmatrix}
                    B_{11} & \dots & B_{1i} & -1 \\
                    \vdots & \ddots & \vdots & \vdots \\
                    B_{i1} & \dots & B_{ii} & -1 \\
                    -1 & \dots & -1 & 0
                \end{pmatrix}
                \begin{pmatrix}
                    c_1 \\
                    \vdots \\
                    c_i \\
                    \lambda
                \end{pmatrix}
                = \begin{pmatrix}
                    0 \\
                    \vdots \\
                    0 \\
                    -1
                \end{pmatrix}.
            \end{align}
            Solving this equation for the coefficients $c_k$ we are able to
            compute the estimated quantity $\bar{\vfg{p}}_i$ from
            \autoref{eq:diis-estimate}.
            An existing implementation of the DIIS algorithm was given to us by
            \citeauthor{rolf-nocc} as part of his article \citetitle{rolf-nocc}
            \cite{rolf-nocc} and has been integrated by the author into the
            libraries we have created.
            This makes the method available to all Hartree-Fock solvers and all
            coupled-cluster implementations.

            The DIIS method suffers from the fact that we store all $i$ previous
            measurements in memory.
            In the case of large systems this can become a problem as we spend
            all our memory in the acceleration.
            We can therefore adjust the number of vectors, i.e., $i$, in order
            to limit the memory occupied by DIIS.

    \section{Numerical integration}
        \label{sec:numerical-integration}
        In this section we'll review a select few time-integration schemes for
        solving time-dependent ordinary differential equations and we'll discuss the
        applicability of each scheme.

        \subsection{Problem statement}
            The problem we are trying to solve can be formulated as
            \begin{align}
                \dot{\vfg{y}}(t) = \vfg{F}(\vfg{y}, t),
                \label{eq:ode-problem}
            \end{align}
            where $\dot{\vfg{y}}(t)$ is the derivative of the vector
            $\vfg{y}(t)$ with respect to time.
            The function $\vfg{F}(\vfg{y}, t)$ is a vector valued function
            evaluating the right-hand side of the first-order differential
            equation listed above.
            Most implemented integration schemes assume that the problem can
            be formulated as in \autoref{eq:ode-problem}, but as we've seen the
            time-dependent Hartree-Fock method is defined in terms of a matrix,
            and the time-dependet coupled-cluster methods as two sets of
            amplitudes of different sizes and coefficient matrices for the
            orbital-adaptive methods.
            To get around this we let each time-dependent solver class implement
            a Python dunder method\footnote{%
                A ``dunder method'' is a special Python function denoted with a
                double underscore in front and behind of the name.
                These are often magic methods in the sense that they either
                overwrite operators, such as: \pyth{__add__} for addition,
                \pyth{__mul__} for multiplication, etc, or that they define some
                special functionality, e.g., \pyth{__init__} as the constructor
                for a class.
            } \pyth{__call__} which accepts the two
            parameters \pyth{y} and \pyth{t} to represent the right-hand side
            function $\vfg{F}(\vfg{y}, t)$.
            The innards of this method can now be describe by the following
            steps:
            \begin{enumerate}
                \item Make sure the vector $\vfg{y}$ is reshaped to the correct
                    formulation as required by the right-hand sides.
                    For configuration interaction this is handled automatically,
                    but for the time-dependent Hartree-Fock method this means
                    that we need to reshape the vector into a coefficient
                    matrix.
                    For the time-dependent coupled-cluster methods this is even
                    more involved and will be discussed in due time.
                \item Update the time-dependent Hamiltonian to the current time
                    step $t$.
                \item Compute the right-hand sides to get the derivative of the
                    coefficients and amplitudes.
                \item Convert the coefficients and amplitudes to a new vector
                    $\dot{\vfg{y}}$ and return this to the differential
                    equation solver.
            \end{enumerate}
            Now, the process of stacking all the elements in the coefficient
            matrices and amplitudes as a single vector is possible as each index
            is independent of each other in terms of time evolution.
            However, they do coupled when evaluating the right-hand sides and we
            therefore need to recover the original shape.
            Most differential equation libraries \cite{sympy} \cite{julia-diff}
            assume the right-hand side to be a callable function with a
            \pyth{__call__}-method.
            Annoyingly enough, there seems to be no consensus of the ordering of
            the parameters to the function $\vfg{F}(\vfg{y}, t)$ which means
            that each library will require a different definition of the
            right-hand side functions.
            In the current implementation of our programs we have chosen the
            ordering $\vfg{F}(\vfg{y}, t)$, but for us to use differential
            equation solvers from SymPy \cite{sympy} or diffeqpy
            \cite{julia-diff} we need to reverse the order, or add extra
            parameters.
            This process will be discussed more in \autoref{sec:future-work} as
            we've not implemented an interface towards these libraries in this
            work.

            Discretizing time with a constant step $\Delta t$ such that $t_n
            = t_0 + n \Delta t$ for $n \in \brac{0, N_t}$, where $N_t$ is the number of
            time steps and $t_0$ is the initial time step.
            Note that we typically choose a time step $\Delta t$ such that $N_t$ can be
            found by
            \begin{align}
                N_t = \floor{\frac{t_f - t_0}{\Delta t}} + 1,
            \end{align}
            where $t_f$ is the final time step.
            We denote $\vfg{y}_0 \equiv \vfg{y}(t_0)$ and $\vfg{y}_n \equiv
            \vfg{y}(t_n)$, where $\vfg{y}_0$ is the initial value of the
            problem.
            In this thesis we will always choose the initial value to be the
            ground state of the specific solver.
            This abstraction thus enables us to work with known ordinary
            differential equation solvers formulated in a familiar way.

            As a first approximation to solving the time-dependent equations,
            we've implemented the Runge-Kutta 4 algorithm.
            A rendition of this algorithm can be found in
            \citetitle{morken2017notes} \cite{morken2017notes} and will not be
            repeated in this text as all results are achieved using the more
            sophisticated Gauss-Legendre integrator which will be discussed
            shortly.
            Runge-Kutta 4 is a fourth-order method with a local numerical error
            of the order of $\mathcal{O}(h^5)$ and a total error of
            $\mathcal{O}(h^4)$.

        \subsection{Symplectic integrators}
            \label{subsec:symplectic}
            Looking back to \autoref{sec:time-evolution-operators} we note that
            for a time-dependent Hamiltonian in the Schr√∂dinger picture, the
            time evolution of a state $\ket*{\psi(t)}$ is described by the
            time evolution operator shown in \autoref{eq:td-evolution}, where
            the Hamiltonian in general does not commute at different time steps.
            It is perhaps somewhat na√Øve to hope for a numerical method such as
            Runge-Kutta 4 to yield good results without some extra concern for
            the problem at hand.
            As discussed by \citeauthor{joshua-magnus} \cite{joshua-magnus} the
            solution to the time-dependent Schr√∂dinger equation can be better
            approximated by the Magnus expansion \cite{magnus-expansion}.
            This leads to implicit integration methods that are
            \emph{symplectic} \cite{joshua-magnus}.
            These methods will to a much larger degree conserve the energy,
            normalization, and unitarity of the coefficients and amplitudes as
            opposed to the explicit schemes such as Runge-Kutta 4
            \cite{joshua-magnus, pedersen2018symplectic}.
            One of the first approximations to the Magnus expansion is the
            Crank-Nicolson algorithm \cite{ullrich2011time, joshua-magnus}.


        \subsection{Gauss-Legendre}
            \label{subsec:gauss-legendre}
            In an article on the stability of the time-dependent coupled-cluster
            singles-and-doubles method on atomic systems subject to intense
            laser pulses, \citeauthor{pedersen2018symplectic}
            \cite{pedersen2018symplectic} used symplectic, implicit Runge-Kutta
            methods in an attempt to alleviate some of the stability issues they
            faced.
            As part of an ongoing publication on the stability of time-dependent
            coupled-cluster methods \cite{oa-stability} we have been given the
            implementation of the Gauss-integrator\footnote{%
                Note that the name ``Gauss-integrator'' is a notoriously
                ambiguous name as this can refer to a multitude of techniques
                and solvers due to the use of named polynomials such as
                Legendre, and Laguerre polynomials, etc.%
            } as used by \citeauthor{pedersen2018symplectic}
            \cite{pedersen2018symplectic} to include it in our libraries.
            This method is described in depth in their text
            \citetitle{pedersen2018symplectic} \cite{pedersen2018symplectic} and
            will not be renditioned in this work.
            % TODO: Discuss the parameters to the integrator.

\clearemptydoublepage
