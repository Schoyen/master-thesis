\chapter{Quantum mechanics}
    \epigraph{No fair! You changed the outcome by measuring it!}
    {--- Hubert J. Farnsworth, Futurama}

    We start our journey by reviewing parts of quantum mechanics that we deem
    necessary in order to understand the thesis.


    \section{Canonical quantization}
        \subsection{The Schrödinger equation}
            \begin{align}
                i\hslash\dpd{\Psi}{t}
                = -\frac{\hslash^2}{2m}\vfg{\nabla}^2\Psi
                +    v\Psi
                \label{eq:coordinate-tdse}
            \end{align}

    \section{The postulates of quantum mechanics}
        In order to make sure that we have a common understanding of how to
        understand and interpret quantum mechanics we begin by introducing the
        postulates of quantum mechanics.
        The postulates were originally developed by Dirac
        \cite{dirac1981principles} and Neumann \cite{von2018mathematical},
        but have since been subject to interpretation.
        This has lead to many versions of the postulates, both in the number of
        postulates, and in the accuracy of their description.
        We will base our definition of the postulates of quantum mechanics on
        the representation given in the book \citetitle{salasnich2017quantum} by
        \citeauthor{salasnich2017quantum} \cite{salasnich2017quantum}.

        \begin{enumerate}
            \item A state $\ket{\psi}$ is a unitary vector defined on a
                separably complex Hilbert space.
                The state can be expanded in any complete set of basis vectors
                on the Hilbert space by
                \begin{align}
                    \ket{\psi} = c_i\ket{i},
                \end{align}
                where $c_i \in \mathbb{C}$.
            \item An observable is described by a Hermitian operator $\hat{Q}$
                acting on the Hilbert space of state vectors.
            \item The eigenvalue $q$ of the observable $\hat{Q}$ represents its
                measurable values.
                That is,
                \begin{align}
                    \hat{Q}\ket{q} = q\ket{q},
                \end{align}
                where $\para{q, \ket{q}}$ are the eigenpairs of the observable.
            \item We can measure the probability $p$ of finding the normalized
                state $\ket{\psi}$ in the normalized eigenstate $\ket{q}$ by
                \begin{align}
                    p = \abs{\braket{q}{\psi}}^2.
                \end{align}
                As a consequence $p$ also gives the probability of measuring the
                eigenvalue $g$.
                We can also measure the expectation value of the observable
                $\hat{Q}$ for the state $\ket{\psi}$ by
                \begin{align}
                    \expval{Q} = \expval{\hat{Q}}{\psi}.
                \end{align}
            \item The time-evolution of a state $\ket{\psi(t)}$ is defined by
                the Schrödinger equation
                \begin{align}
                    i\hslash \dod[]{}{t}\ket{\psi(t)} = \hamil\ket{\psi(t)},
                \end{align}
                where we work in the Schrödinger picture.
        \end{enumerate}

    \section{Time-independent Schrödinger equation}
        In order to solve the Schrödinger equation in
        \autoref{eq:coordinate-tdse} for a specified time-independent potential
        $v(\vfg{r}, t) = v(\vfg{r})$, we use the method of separation of
        variables for the wave function.
        \begin{align}
            \Psi(\vfg{r}, t) = \psi(\vfg{r})\phi(t),
        \end{align}
        where $\psi(\vfg{r})$ is a spatial function and $\phi(t)$ is purely
        time-dependent.
        Inserted into the Schrödinger equation and dividing through by
        $\Psi(\vfg{r}, t)$ on both sides we get
        \begin{align}
            i\hslash\frac{1}{\phi}\dod{\phi}{t}
            = -\frac{\hslash^2}{2m}\frac{1}{\psi}
            \vfg{\nabla}^2\psi + v.
        \end{align}
        As both sides are functions of their own separate variable, they must be
        connected through a constant, $\energy$.
        This means that
        \begin{gather}
            i\hslash\frac{1}{\phi}\dod{\phi}{t} = \energy, \\
            -\frac{\hslash^2}{2m}\frac{1}{\psi}\vfg{\nabla}^2\psi
            + v = E,
        \end{gather}
        and we can solve the two equations independently.
        The former equation can be solved as a first order, ordinary
        differential equation.
        The solution is given by
        \begin{align}
            \phi(t) = \exp[-\frac{iEt}{\hslash}].
        \end{align}
        This is a solution which we will discuss in more depth in
        \autoref{sec:time-evolution-operators}.
        Multiplying by $\psi(\vfg{r})$ on both sides in the second equation we
        find the \emph{time-indepedent Schrödinger equation},
        \begin{align}
            -\frac{\hslash^2}{2m}\vfg{\nabla}^2\psi + v\psi = \energy\psi.
        \end{align}
        By virtue of the canonical quantization of the momentum operator
        $\momentumvec$ we recognize the quantum mechanical Hamiltonian on the
        left-hand side.
        We therefore write the time-independent Schrödinger equation on the form
        \begin{align}
            \hamil\psi = \energy\psi,
        \end{align}
        and we interpret the constant $\energy$ as the energy of the state
        $\psi$ in the system described by the Hamiltonian $\hamil$.
        The time-independent Schrödinger equation is an eigenvalue equation, and
        by diagonalizing the Hamiltonian we find the spectrum of the system with
        the eigenpairs $(\psi_n, E_n)$.
        Once the spectrum is found it is common to call the problem solved as
        the time-evolution of the wave function can be expressed as a linear
        combination of the eigenstates $\psi_n$.
        However, we will see in this thesis that finding the spectrum of a
        many-body Hamiltonian proves a challenge indeed and therefore
        complicates the matter of the time-evolution.

    \section{Density operators}
        \label{sec:density-operators}
        When working with many-body quantum mechanics, computing expectation
        values can at times prove easier when done using density matrices.
        A general density matrix of a \emph{pure state} is on the form
        \begin{align}
            \densitymatrix = \ket{\psi}\bra{\psi}.
        \end{align}
        A pure state is a quantum state $\ket{\psi}$ containing the maximum
        amount of information about a given system.
        For a \emph{mixed state}, i.e., a linear combination of pure states
        $\ket{\psi_k}$ with a classical probability $p_k$ associated with the
        state, we get a density matrix on the form
        \begin{align}
            \densitymatrix = \sum_{k} p_k \ket{\psi_k}\bra{\psi_k}.
        \end{align}
        Any density operator must satisfy the following
        properties \cite{modern-qm}:
        \begin{enumerate}
            \item Hermiticity, that is
                \begin{align}
                    p_k = p_k^{*} \implies \densitymatrix = \densitymatrix^{\dagger}.
                \end{align}
                This translates to the probabilities being real, $p_k \in
                \mathbb{R}$.
            \item Positivity,
                \begin{align}
                    p_k \geq 0 \implies \bra{\chi}\densitymatrix\ket{\chi} \geq 0.
                \end{align}
                In other words, density matrices are \emph{positive
                semidefinite}.
            \item Normalization of the probabilities,
                \begin{align}
                    \sum_{k} p_k = 1 \implies \tr(\densitymatrix),
                \end{align}
                that is, the probabilities must sum up to one.
        \end{enumerate}
        Furthermore, by squaring the density matrix and taking the trace we can
        infer if the system we are analyzing is in a mixed state or a pure state
        \cite{modern-qm}.
        \begin{align}
            \tr(\densitymatrix^2) = \sum_{k} p_k^2 \leq 1,
        \end{align}
        with equality if, and only if, the system is in a pure state, viz.
        \begin{align}
            \densitymatrix = \ket{\psi}\bra{\psi}
            \implies \densitymatrix^2 = \densitymatrix
            \implies \tr(\densitymatrix^2) = 1.
        \end{align}
        Using density matrices, we can compute the expectation value of any
        operator $\hat{O}$, by \cite{modern-qm}
        \begin{align}
            \expval{O} = \tr(\hat{O}\densitymatrix).
        \end{align}
        % TODO: Work this out further

    \section{The variational principle}
        \label{sec:variational-principle}
        The variational principle tells us that the ``true'' ground state energy
        $\energy_1$, i.e., the lowest energy eigenvalue of the Hamiltonian
        $\hamil$, will always be the lower bound on the energy of the system.
        This means that all approximate wave functions -- that are determined
        variationally -- will serve as an upper bound to the ground state energy
        \cite{griffiths2017introduction}.
        \begin{theorem}
            Given a Hamiltonian $\hamil$ describing the system we are examining,
            and a normalized wave function $\ket{\psi}$, we have that
            \begin{align}
                \energy_1
                \leq
                E[\psi, \psi^{*}]
                = \mel{\psi}{\hamil}{\psi},
                = \int\dd x\psi^{*}(x)\hamil\psi(x),
                \label{eq:variational-principle}
            \end{align}
            where $E[\psi, \psi^{*}]$ is an energy functional dependent on the shape of
            the wave function $\ket{\psi}$ and $x$ is a set of coordinates.
            Here $\energy_1$ represents the true ground state of the
            Hamiltonian.
        \end{theorem}
        The variational principle guarantees that any wave function $\ket{\psi}$
        will overestimate the ground state energy unless we happened upon the
        true ground state.
        \begin{proof}
            The eigenstates of the Hamiltonian will form a complete basis set
            for the Hilbert space they are apart of.
            This means that we can construct any state $\ket{\psi}$ as a linear
            combination of these basis functions
            \begin{align}
                \ket{\psi} = \sum_{i = 1}^{N} c_{i}\ket{\phi_i},
            \end{align}
            where the basis functions $\brac{\ket{\phi_i}}_{i = 1}^{N}$ are
            eigenstates of the Hamiltonian
            \begin{align}
                \hamil\ket{\phi_i} = \energy_i\ket{\phi_i},
            \end{align}
            such that $\energy_1 \leq \energy_2 \leq \dots \leq \energy_N$.
            Furthermore, in our formulation of the variational principle, i.e.,
            for normalized wave functions, we require that the basis functions
            are orthornomal.
            \begin{align}
                \braket{\phi_i}{\phi_j} = \delta_{ij},
            \end{align}
            and that the coefficients yield
            \begin{align}
                \abs{\vfg{c}}^2 = \sum_{i = 1}^{N} c^{*}_{i}c_{i} = 1.
            \end{align}
            Inserting the state $\ket{\psi}$ into the energy
            function\footnote{%
                The energy functional is no longer dependent on a function and
                is therefore realized as a function instead.
            } in \autoref{eq:variational-principle} we find
            \begin{align}
                E(\vfg{c})
                &= \sum_{i = 1}^{N}\sum_{j = 1}^{N}
                c^{*}_i\mel{\phi_i}{\hamil}{\phi_j}c_j
                = \sum_{i = 1}^{N}\sum_{j = 1}^{N}
                c^{*}_i c_j \energy_j\delta_{ij}
                = \sum_{i = 1}^{N}
                \abs{c_i}^2 \energy_i.
            \end{align}
            However, by definition $\energy_1$ is the smallest eigenvalue of the
            Hamiltonian, i.e., $\energy_1 \leq \energy_i$, for all $i \in
            \brac{1, \dots, N}$.
            Whence, we find
            \begin{align}
                E(\vfg{c})
                &= \sum_{i = 1}^{N}
                \abs{c_i}^2 \energy_i
                \geq
                \energy_1 \sum_{i = 1}^{N}
                \abs{c_i}^{2}
                = \energy_1,
            \end{align}
            which shows that $\energy_1$ serves as a lower bound for the energy
            of the system under observation.
        \end{proof}

        An arguably more important result is that the stationary condition of
        the energy functional in the variational principle will be an eigenstate
        of the time-independent Schrödinger equation.
        \begin{proof}
            Consider a small first order variation in the trial wave function
            \begin{align}
                \ket*{\tilde{\psi}} = \ket{\psi} + \ket{\delta\psi},
                \label{eq:wave-variation}
            \end{align}
            and the corresponding adjoint equation with the kets replaced by bras.
            We denote $\delta\psi(x) = \epsilon\eta(x)$, where $x$ represents an
            arbitrary set of coordinates.
            The function $\eta(x)$ is arbitrary and $\epsilon$ is a small
            number.
            Expanding the energy functional in orders of the parameter
            $\epsilon$, i.e., Taylor expanding the energy functional, we can
            write
            \begin{align}
                E[\tilde{\psi}, \tilde{\psi}^{*}]
                = E[\psi, \psi^{*}]
                + \left.
                \dpd{E[\tilde{\psi}, \psi^{*}]}{\epsilon}
                \right\rvert_{\epsilon = 0}
                \epsilon
                + \left.
                \dpd{E[\psi, \tilde{\psi}^{*}]}{\epsilon}
                \right\rvert_{\epsilon = 0}
                \epsilon
                + \dots.
            \end{align}
            Using the method of \emph{functional derivatives}
            \cite{wiki:functional-derivative, kvaal2017notes}, we can write
            the derivatives of the energy functionals, and hence the stationary
            conditions to be
            \begin{gather}
                \left.
                \dpd{E[\tilde{\psi}, \psi^{*}]}{\epsilon}
                \right\rvert_{\epsilon = 0}
                =
                \int\dd x \frac{
                    \delta E[\tilde{\psi}, \psi^{*}]
                }{
                    \delta \psi(x)
                }
                \eta(x)
                = 0,
                \\
                \left.
                \dpd{E[\psi, \tilde{\psi}^{*}]}{\epsilon}
                \right\rvert_{\epsilon = 0}
                =
                \int\dd x \frac{
                    \delta E[\psi, \tilde{\psi}^{*}]
                }{
                    \delta
                    \psi^{*}(x)
                }
                \eta^{*}(x)
                = 0.
            \end{gather}
            Treating $\epsilon \in \mathbb{R}$ but allowing $\eta(x)$ to be
            complex, we have
            \begin{gather}
                \delta\psi(x) = \epsilon\eta(x),
                \qquad
                \delta\psi^{*}(x) = \epsilon\eta^{*}(x).
            \end{gather}
            The variations in the energy functional are given by
            \begin{gather}
                \delta E[\tilde{\psi}, \psi^{*}]
                = E[\tilde{\psi}, \psi^{*}]
                - E[\psi, \psi^{*}], \\
                \delta E[\psi, \tilde{\psi}^{*}]
                = E[\psi, \tilde{\psi}^{*}]
                - E[\psi, \psi^{*}].
            \end{gather}
            We find expressions for the variations by inserting
            \autoref{eq:wave-variation} into the definition of the energy
            functional.
            We have
            \begin{align}
                E[\tilde{\psi}, \psi^{*}]
                = \frac{
                    \mel{\psi}{\hamil}{\psi}
                    + \mel{\psi}{\hamil}{\delta\psi}
                }{
                    \braket{\psi}
                    + \braket{\psi}{\delta\psi}
                },
            \end{align}
            where we no longer require that the wave functions are normalized.
            Looking at the denominator first we can move the inner product with
            the state $\ket{\psi}$ outside.
            This yields
            \begin{align}
                \braket{\psi}\brak{
                    1 +
                    \frac{
                        \braket{\psi}{\delta\psi}
                    }{
                        \braket{\psi}
                    }
                }
                = \braket{\psi}\brak{1 + x},
            \end{align}
            where we've defined $x$ as the second term in the bracket.
            Here $x$ will be small and we expand the denominator in terms of
            $x$, viz.
            \begin{align}
                \frac{1}{1 + x} = 1 - x + \mathcal{O}(x^2).
            \end{align}
            Keeping only first order variations in $\delta\psi$ we can write the
            energy functional as
            \begin{align}
                E[\tilde{\psi}, \psi^{*}]
                &= \frac{
                    \mel{\psi}{\hamil}{\psi}
                    + \mel{\psi}{\hamil}{\delta\psi}
                }{
                    \braket{\psi}
                }\brak{
                    1 - \frac{\braket{\psi}{\delta\psi}}{\braket{\psi}}
                    + \dots
                }
                \\
                &=
                E[\psi, \psi^{*}]
                + \frac{
                    \mel{\psi}{\hamil}{\delta\psi}
                }{
                    \braket{\psi}
                }
                - E[\psi, \psi^{*}]
                \frac{
                    \mel{\psi}{\hamil}{\delta\psi}
                }{
                    \braket{\psi}
                }
                + \dots
                \\
                &=
                E + \frac{
                    \mel{\psi}{\para{\hamil - E}}{\delta\psi}
                }{
                    \braket{\psi}
                },
            \end{align}
            where for brevity we have written $E = E[\psi, \psi^{*}]$.
            The energy functional with the adjoint variation is given by
            \begin{align}
                E[\psi, \tilde{\psi}^{*}]
                &=
                E + \frac{
                    \mel{\delta\psi}{\para{\hamil - E}}{\psi}
                }{
                    \braket{\psi}
                }.
            \end{align}
            Subtracting $E$ from the above equation the functionals yield the
            variations $\delta E[\tilde{\psi}, \psi^{*}]$ and $\delta E[\psi,
            \tilde{\psi}^{*}]$.
            Inserting into the stationary conditions we find
            \begin{align}
                \int\dd x
                \frac{
                    \delta E[\psi, \tilde{\psi}^{*}]
                }{
                    \delta\psi^{*}(x)
                }\eta^{*}(x)
                &=
                \frac{1}{\braket{\psi}}
                \int\dd x
                \frac{
                    \epsilon\eta^{*}(x)\para{
                        \hamil
                        - E
                    }
                    \psi(x)
                }{
                    \epsilon\eta^{*}(x)
                }
                \eta^{*}(x)
                \\
                &=
                \frac{
                    \mel{\eta}{\para{\hamil - E}}{\psi}
                }{
                    \braket{\psi}
                }
                = 0,
            \end{align}
            where we look at the variations over the adjoint wave function to
            recover the time-independent Schrödinger equation instead of the
            adjoint formulation.
            From the fundamental lemma of calculus of variations
            \cite{wiki:fundamental-lemma}, we have that the stationary condition
            is fullfilled for all $\eta^{*}(x)$ as long as
            \begin{align}
                \hamil\ket{\psi}
                = E\ket{\psi},
            \end{align}
            which is just the time-independent Schrödinger equation.
            Looking at the stationary condition for the adjoint wave function we
            will find
            \begin{align}
                \bra{\psi}\hamil = \bra{\psi}E,
            \end{align}
            i.e., the adjoint time-independent Schrödinger equation.
            We have therefore found that the stationary condition for the
            variational energy functional is satisfied as long as $(\ket{\psi},
            E[\psi, \psi^{*}])$ is an eigenpair of the time-independent
            Schrödinger equation.
            This means that we can find the eigenstate $\ket{\psi}$ by
            optimizing the functional $E[\psi, \psi^{*}]$.
        \end{proof}
        The variational principle can then be summarized as follows: the
        stationary conditions of the variational energy functional will yield
        the eigenstates of the Hamiltionian.

        \subsection{The variational method}
            \label{subsec:variational-method}
            We will now look at a formulation of the variational method using
            wave functions that are linear combinations of a known basis set as
            this is a much applicable technique for our work.
            Given a Hermitian model Hamiltonian $\hamil$ and a normalized trial
            wave function $\ket{\psi}$ that is constructed as a linear
            combination of known normalized wave functions
            $\ket{\chi_{\alpha}}$, viz.
            \begin{align}
                \ket{\psi} = c_{\alpha}\ket{\chi_{\alpha}},
            \end{align}
            where $\vfg{c}$ is the set of coefficients $c_{\alpha}$, we
            construct the variational energy function $E(\vfg{c})$ by
            \begin{align}
                E(\vfg{c})
                &= \mel{\psi}{\hamil}{\psi}.
            \end{align}
            Optimizing the energy function is now done by finding the
            stationary points with respect to the variational parameters
            $c_{\alpha}$.
            We do this by demanding that
            \begin{align}
                \dpd[]{E(\vfg{c})}{c_{\alpha}} &= 0,
                \\
                \dmd{E(\vfg{c})}{2}{c_{\alpha}}{}{c_{\beta}}{} &= 0,
            \end{align}
            where the first variation locates the stationary point and the
            second variation gives the Hessian matrix which categorizes the
            point as a minimum, maxmimum, or saddle.
            However, looking at the first variation we have
            \begin{align}
                \dpd{}{c_{\alpha}}\mel{\psi}{\hamil}{\psi} = 0,
            \end{align}
            which is not solvable as the equations stand \cite{szabo1996modern}
            as they are unbounded.
            In order to construct equations which can be optimized, we use
            Lagrange's method of undetermined multipliers to construct a new
            function $G(\vfg{c})$ given by \cite{helgaker-molecular,
            szabo1996modern}
            \begin{align}
                G(\vfg{c})
                &=
                \mel{\psi}{\hamil}{\psi}
                - E(\vfg{c}) \para{\braket{\psi}{\psi} - 1},
            \end{align}
            where we've introduced the normalization condition for the trial
            wave function as a constraint.
            Optimizing this function for the first variation we find
            \begin{align}
                \dpd[]{G(\vfg{c})}{c_{\alpha}}
                &=
                \brak{
                    \dpd[]{}{c_{\alpha}}
                    \bra{\psi}
                }\hamil\ket{\psi}
                + \bra{\psi}\hamil
                \brak{
                    \dpd[]{}{c_{\alpha}}
                    \ket{\psi}
                }
                -
                \braket{\psi}{\psi}
                \dpd[]{E(\vfg{c})}{c_{\alpha}}
                \nonumber \\
                &\qquad
                -
                E(\vfg{c})
                \brak{
                    \dpd[]{}{c_{\alpha}}
                    \bra{\psi}
                }\ket{\psi}
                -
                \bra{\psi}
                \brak{
                    \dpd[]{}{c_{\alpha}}
                    \ket{\psi}
                }
                \\
                &=
                2 \mel{\chi_{\alpha}}{\hamil}{\psi}
                - 0
                - 2E(\vfg{c})\braket{\chi_{\alpha}}{\psi}
                = 0,
            \end{align}
            where the first variation in the energy function becomes zero due to
            the conditions imposed on the stationary point.
            Furthermore, due to the hermiticity of the Hamiltonian, we have
            collected equal terms.
            Inserting the expansion for the trial wave function on both sides,
            we find
            \begin{gather}
                \mel{\chi_{\alpha}}{\hamil}{\chi_{\beta}} c_{\beta}
                - E(\vfg{c})\braket{\chi_{\alpha}}{\chi_{\beta}} c_{\beta}
                = 0
                \\
                \implies
                \hamilten_{\alpha\beta} c_{\beta}
                = E(\vfg{c}) \overlapten_{\alpha\beta} c_{\beta}
                \\
                \implies
                \hamilmat \vfg{c}
                = E(\vfg{c}) \overlapmat \vfg{c},
                \label{eq:variational-eigenvalue}
            \end{gather}
            where we've defined the overlap matrix $\overlapmat$ from the
            overlap integrals between the known basis elements
            $\ket{\chi_{\alpha}}$ by
            \begin{align}
                \overlapten_{\alpha\beta}
                \equiv
                \braket{\chi_{\alpha}}{\chi_{\beta}}
            \end{align}
            We have now reduced the task of optimizing the energy function to
            the problem of solving the generalized eigenvalue equation in
            \autoref{eq:variational-eigenvalue}.
            In other words, by creating the Hamiltonian matrix $\hamilmat$ and
            the overlap matrix $\overlapmat$ from the basis of known basis
            elements, we can diagonalize the matrices, i.e., solve the
            generalized eigenvalue equation, and extract the optimal eigenpair
            $(E, \vf{c})$ as the eigenvalue and eigenvector respectively.
            This procedure will be used extensively as a procedure to transform
            from a known to an unknown basis.
            See \citetitle{helgaker-molecular} \cite{helgaker-molecular} for a
            derivation of the Hessian matix elements.

    \section{The Hellmann-Feynman theorem}
        The Hellmann-Feynman theorem provides us with a method of calculating
        first-order change (also known as a first order property) in the energy
        due to a perturbation \cite{helgaker-molecular}.
        \begin{theorem}
            If $\ket{\psi}$ is a normalized eigenstate of the Hamiltonian
            $\hamil$, or $\ket{\psi}$ is variationally determined from the
            Hamiltonian $\hamil$, the Hellmann-Feynman theorem \cite{feynman}
            states that
            \begin{align}
                \left.\dod[]{E(\alpha)}{\alpha}\right\rvert_{\alpha = 0}
                =
                \left.
                \dpd[]{}{\alpha}
                \bra{\psi_{\alpha}}\hamil + \alpha\hat{V}\ket{\psi_{\alpha}}
                \right\rvert_{\alpha = 0}
                = \bra{\psi}\hat{V}\ket{\psi},
                \label{eq:hellmann-feynman}
            \end{align}
            where $\alpha$ is a perturbational parameter and $\hat{V}$ the
            perturbation operator.
            The wave function $\ket{\psi_{\alpha}}$ is given by
            \begin{align}
                \ket{\psi_{\alpha}} = N(\ket{\psi} + \alpha\ket{\delta\psi}),
            \end{align}
            where $N$ is a normalization factor.
            For approximate wave functions, they must be optimized with respect
            to the same variational parameter $\alpha$ as in the theorem.
        \end{theorem}
        \begin{proof}
            The underlying assumption is that
            \begin{align}
                \hamil\ket{\psi} = E\ket{\psi},
            \end{align}
            regardless if we have an exact or a variationally determined wave
            function $\ket{\psi}$.
            Furthermore, both perturbed and unperturbed wave functions are
            normalized, viz.
            \begin{align}
                \braket{\psi}{\psi} = \braket{\psi_{\alpha}}{\psi_{\alpha}} = 1
                \implies
                \dpd[]{}{\alpha} \braket{\psi_{\alpha}}{\psi_{\alpha}} = 0.
            \end{align}
            See reference \cite{helgaker-molecular} for a proof where the
            normalization of the perturbed wave function is relaxed.
            We now prove \autoref{eq:hellmann-feynman} directly.
            \begin{align}
                \left.\dod[]{E(\alpha)}{\alpha}\right\rvert_{\alpha = 0}
                &=
                \left.
                \dpd[]{}{\alpha}
                \bra{\psi_{\alpha}}\hamil + \alpha\hat{V}\ket{\psi_{\alpha}}
                \right\rvert_{\alpha = 0}
                \\
                &=
                \bra{\delta\psi}\hamil\ket{\psi}
                + \bra{\psi}\hat{V}\ket{\psi}
                + \bra{\psi}\hamil\ket{\delta\psi}
                \\
                &=
                \energy \brak{
                    \braket{\delta\psi}{\psi}
                    + \braket{\psi}{\delta\psi}
                }
                + \bra{\psi}\hat{V}\ket{\psi}
                \\
                &=
                \energy \dpd[]{}{\alpha}\braket{\psi}{\psi}
                + \bra{\psi}\hat{V}\ket{\psi}
                \\
                &=
                \mel{\psi}{\hat{V}}{\psi},
            \end{align}
            which is what we wanted to show.
        \end{proof}
        A consequence of the Hellmann-Feynman theorem is that it provides us
        with a technique for computing expectation values of other quantities
        than the energy once we have variatonally determined the optimal state
        for a given system.
        For example, the expectation value of the operator $\hat{O}$ can be
        found by
        \begin{align}
            \expval{O}
            = \mel{\psi}{\hat{O}}{\psi}
            = \dpd[]{}{\alpha}
            \left.
            \mel{\psi_{\alpha}}{\hamil + \alpha\hat{O}}{\psi_{\alpha}}
            \right\rvert_{\alpha = 0}
            =
            \left.
            \dpd[]{E(\alpha)}{\alpha}
            \right\rvert_{\alpha = 0}.
        \end{align}
        This avoids the need of having to variationally determine every
        expectation value that we wish to measure as finding the optimal
        parameters for the energy is enough.

    \section{The time-dependent Schrödinger equation}
        Moving to the time domain, the dynamics of an isolated quantum system is
        described by the Schrödinger equation
        \begin{align}
            i\hslash \dpd{}{t}\ket{\psi(t)}
            = \hamil(t)\ket{\psi(t)}.
            \label{eq:tdse}
        \end{align}
        Given $\ket{\psi(t_0)}$ the time-dependent Schrödinger equation will
        determine $\ket{\psi(t)}$ for all earlier and later times.
        A point to note is that the time-dependent Schrödinger equation does not
        require that the initial state $\ket{\psi(t)}$ be a ``meaningfull
        state'', the equation will evolve this state -- whatever it is -- in
        time governed by the time-dependent Hamiltonian $\hamil(t)$.
        This is an important point in the sense that solving the time-dependent
        Schrödinger equation for a specific Hamiltonian can prove a great
        challenge.
        The approach that we take in this thesis is to choose an initial state,
        most often the approximation we have for the ground state of the
        time-independent Schrödinger equation, and evolve this state in time.
        This can be thought of as ``preparing'' a system in the ground state and
        then ``disturbing'' the system with an external interaction which
        triggers a reaction in the dynamics of the system.

    \section{The time evolution operators}
        \label{sec:time-evolution-operators}
        %We will in the following subsection stay close to the derivation of
        %\citeauthor{ullrich2011time} \cite{ullrich2011time}, section 3.1.2, and
        %\citeauthor{joachain2012atoms} \cite{joachain2012atoms}, section 5.1.
        Any solution to the time-dependent Schrödinger equation is given by
        \begin{align}
            \ket{\psi(t)} = \hat{U}(t, t_0)\ket{\psi(t_0)},
        \end{align}
        where $\hat{U}(t, t_0)$ is the time evolution operator that acts on the
        initial state $\ket{\psi(t_0)}$, and yields the state $\ket{\psi(t)}$ at
        some other time $t$.
        The time evolution operators are unitary at common times, viz.
        \begin{align}
            \hat{U}^{\dagger}(t, t_0)\hat{U}(t, t_0) = \1.
        \end{align}
        This means that we can go ``backwards in time'' in the sense that going
        from $\ket{\psi(t)} \to \ket{\psi(t_0)}$ is done by
        \begin{align}
            \ket{\psi(t_0)} = \hat{U}^{\dagger}(t, t_0)\ket{\psi(t)}.
        \end{align}
        Furthermore, we can compose a time evolution operator from other time
        evolution operators as
        \begin{align}
            \hat{U}(t_2, t_0) = \hat{U}(t_2, t_1)\hat{U}(t_1, t_0).
        \end{align}
        One way to realise this is that we are allowed to have intermediate
        states between two points in time $t_2$ and $t_0$.
        This seemingly benign result is important for our numerical time
        propagation schemes as we make use of this property to move between
        two time points using many small intermediate steps, where smaller steps
        in general yields lower errors.
        The time evolution operator can be found by solving the time-dependent
        Schrödinger equation
        \begin{gather}
            i\dpd{}{t}\hat{U}(t, t_0) = \hamil(t)\hat{U}(t, t_0),
        \end{gather}
        where we've inserted $\ket{\psi(t)} = \hat{U}(t, t_0)\ket{\psi(t_0)}$
        into \autoref{eq:tdse} and removed the initial state $\ket{\psi(t_0)}$
        as it is independent of the time-derivative.
        Now, if the Hamiltonian is time-independent, i.e., $\hat{H}(t) =
        \hat{H}$, the time evolution operator takes on the closed form solution
        \begin{align}
            \hat{U}(t, t_0) = \exp\brac{
                \frac{-i \hamil}{\hslash} (t - t_0)
            }.
            \label{eq:ti-evolution}
        \end{align}
        Assuming we have found the spectrum of our time-independent Hamiltonian,
        where $(E_n, \psi_n)$ are the eigenpairs, and we use this as our initial
        state $\ket{\psi(t_0)} = \ket{\psi_n}$, we find that
        \begin{gather}
            \ket{\psi_n(t)}
            = \exp{\frac{-i\hamil}{\hslash}(t - t_0)}\ket{\psi_n}
            = \exp{\frac{-i E_n}{\hslash} (t - t_0)}\ket{\psi_n},
        \end{gather}
        where $\ket{\psi_n(t)}$ is a \emph{stationary state}.
        This means that all expectation values are stationary in time as can be
        seen from
        \begin{align}
            \expval{O(t)}
            &= \mel{\psi_n(t)}{\hat{O}}{\psi_n(t)}
            = \mel{\psi_n}{\hat{O}}{\psi_n}
            = \expval{O},
        \end{align}
        where the time dependence cancel.
        However, a general time-dependent state $\ket{\psi(t)}$ can be written
        as a linear combination of the eigenstates.
        \begin{align}
            \ket{\psi(t)} = \sum_{n = 1}^{\infty}c_n(t)\ket{\psi_n},
        \end{align}
        where we've absorbed the time-dependency into the coefficients and
        require that the coefficients squared sum up to unity.
        This means that even though the Hamiltonian is time-independent, the
        states themselves are not stationary.
        Now, all observables that commute with the time-independent Hamiltonian
        will still be time-indepedent for $\ket{\psi(t)}$.
        We can see this by looking at a time-independent observable $\hat{Q}$
        which commute with the time-independent Hamiltonian such that the
        eigenstate of the Hamiltonian will be eigenstates of $\hat{Q}$ with
        eigenvalues $q_n$.
        We then have
        \begin{align}
            \expval{Q(t)}
            &= \mel{\psi(t)}{\hat{Q}}{\psi(t)}
            = \sum_{n, m = 1}^{\infty} c^{*}_n(t) c_m(t)
            \mel{\psi_n}{\hat{Q}}{\psi_m}
            \\
            &= \sum_{n, m = 1}^{\infty}
            c^{*}_n(t) c_m(t) q_n \delta_{nm}
            = \sum_{n = 1}^{\infty}
            \abs{c_n(t)}^2 q_n
            = \sum_{n = 1}^{\infty}
            q_n,
        \end{align}
        which shows that the observables that commute with the time-independent
        Hamiltonian are time-independent.


        In the case of a time-dependent Hamiltonian, the time evolution operator
        takes on a more complicated shape.
        \begin{align}
            \hat{U}(t, t_0) =
            \mathcal{T}\exp\brac{
                -\frac{i}{\hslash} \int_{t_0}^{t} \dd\tau
                \hat{H}(\tau)
            },
            \label{eq:td-evolution}
        \end{align}
        where $\mathcal{T}$ is the time-ordering operator.
        As an extra complicating factor, the time-dependent Hamiltonian might
        not commute with itself at different times.
        The time-evolution operator shown in \autoref{eq:td-evolution} serve as
        a theoretical foundation for the time-evolution.
        However, we will see later in the thesis how one can approximate this
        operator using known numerical integration methods.

    \section{The time-dependent variational principle}
        We now wish to extend the variational principle to the time-domain.
        This is done by the time-dependent variational principle by considering
        the action functional
        \begin{align}
            S = \int_{t_1}^{t_2}\dd t L[\psi, \psi^{*}],
        \end{align}
        where $L[\psi, \psi^{*}]$ is a Lagrangian functional of the trial wave
        function $\psi$ and its adjoint.
        In the case of normalized trial wave functions in time, the Lagrangian
        is given by
        \begin{align}
            L[\psi, \psi^{*}]
            =
            \mel{\psi(t)}{
                \para{
                    i\hslash\partial_t - \hamil(t)
                }
            }{\psi(t)},
        \end{align}
        % TODO: Confirm that the Hamiltonian is time-dependent.
        where for the sake of brevity have introduced the short-hand notation
        $\partial_t$ for the partial derivative with respect to time.
        If the wave functions are not required to be normalized in time the
        Lagrangian takes on a more complicated form to ensure that the action
        stays real as the differential operator $i\partial_t$ does not stay
        Hermitian along with the normalization condition
        \cite{kramer1981geometry}.
        By requiring that the first order variations
        \begin{align}
            \delta S
            = \int_{t_1}^{t_2}\dd t \delta L
            = 0,
            \label{eq:stationary-action}
        \end{align}
        are stationary we recover the time-dependent Schrödinger equation.
        \begin{proof}
            Performing variations in a similar fashion as in
            \autoref{sec:variational-principle},
            \begin{gather}
                \tilde{\psi}(x, t) = \psi(x, t) + \epsilon\eta(x, t),
            \end{gather}
            and the adjoint equation with $\epsilon \in \mathbb{R}$ and $\eta(x,
            t) \in \mathbb{C}$.
            Note that we now include time-dependent variations as we no longer
            restrict ourselves to stationary states.
            The stationary condition in \autoref{eq:stationary-action} results
            in the requirements that
            \begin{gather}
                \delta L[\tilde{\psi}, \psi^{*}]
                = \mel{\psi(t)}{i\hslash\partial_t - \hamil(t)}{\delta\psi(t)}
                = 0, \\
                \delta L[\psi, \tilde{\psi}^{*}]
                = \mel{\delta\psi(t)}{i\hslash\partial_t - \hamil(t)}{\psi(t)}
                = 0.
            \end{gather}
            The latter of these two equations is known as the Dirac-Frenkel
            variational principle.
            Letting $\delta\psi(x, t)$ be arbitrary variations vanishing at the
            endpoints $t = t_1$ and $t = t_2$, we recover the
            time-dependent Schrödinger equation and the complex conjugate
            formulation from the fundamental lemma of calculus \cite{frenkel,
            wiki:fundamental-lemma}.
        \end{proof}

    \section{Electrodynamics}
        In vacuum the classical electromagnetic field is described by Maxwell's
        equations without sources.
        \begin{gather}
            \vfg{\nabla}\cdot\vfg{E} = 0, \\
            \vfg{\nabla}\cdot\vfg{B} = 0, \\
            \vfg{\nabla}\times\vfg{E} = -\dpd{}{t}\vfg{B}, \\
            \vfg{\nabla}\times\vfg{B} = \frac{1}{c^2}\dpd{}{t}\vfg{E},
        \end{gather}
        where $\vfg{E}(\vfg{r}, t)$ and $\vfg{B}(\vfg{r}, t)$ are the
        electric and magnetic fields respectively.
        Here ``without sources'' translates to the charge density $\rho$ and the
        current density $\vfg{j}$ being set to zero.
        The electric and magnetic fields can be described by the scalar and
        vector potentials $\phi(\vfg{r}, t)$ and $\vfg{A}(\vfg{r}, t)$,
        respectively, by the relations
        \begin{gather}
            \vfg{E} = -\vfg{\nabla}\phi - \dpd{}{t}\vfg{A}, \\
            \vfg{B} = \vfg{\nabla}\times\vfg{A}.
        \end{gather}
        In addition to Maxwell's equations, the potentials and the electric and
        magnetic fields satisfies the homogeneous wave equation
        \cite{joachain2012atoms}
        \begin{align}
            \vfg{\nabla}^2\vfg{A} = \frac{1}{c^2}\dpd[2]{}{t}\vfg{A}.
            \label{eq:wave-equation}
        \end{align}
        % TODO: This might be a result of the Coulomb gauge condition...
        Maxwell's equations and the wave equation does not uniquely define the
        scalar and vector potentials as the electric and magnetic field are
        invariant under the gauge transformations
        \begin{gather}
            \vfg{A} \to \vfg{A}' = \vfg{A} + \vfg{\nabla} f, \\
            \phi \to \phi' = \phi + \dpd{f}{t},
        \end{gather}
        where $f$ is a differentiable, real function of $\vfg{r}$ and $t$.
        To go from here we choose a gauge fixing condition such that we are able
        to remove non-physical degrees of freedom \cite{modern-qm}.
        We will strictly be working in the non-relativistic limit such that the
        Lorenz gauge\footnote{%
            Yes, this is \emph{actually} spelled ``the Lorenz gauge'', named
            after the Danish physicist Ludvig Lorenz and is \emph{not} to be
            confused with the wrongly named ``Lorentz gauge'' after Hendrik
            Lorentz.
            However, the Lorenz gauge is a Lorentz invariant condition where the
            two terms frequently coincide and is therefore quite often misnamed
            \cite{wiki:lorenz}.
        } is unnecessary.
        We will be using the Coulomb gauge given by
        \begin{align}
            \vfg{\nabla}\cdot\vfg{A} = 0.
        \end{align}
        Without sources this means that $\phi = 0$ and the electric and magnetic
        fields are found by
        \begin{gather}
            \vfg{E} = -\dpd{}{t}\vfg{A}, \\
            \vfg{B} = \vfg{\nabla}\times \vfg{A}.
        \end{gather}

        A solution to the wave equation in \autoref{eq:wave-equation} for the
        vector potential $\vfg{A}$ is the monochromatic plane wave solutions
        with wave number $\vfg{k} = 2\pi / \lambda$, and angular frequency
        $\omega_k = \abs{\vfg{k}} c$.
        That is,
        \begin{align}
            \vfg{A}(\vfg{r}, t)
            &= \vfg{A}_0 \exp[i(\vfg{k}\cdot \vfg{r} - \omega_k t)]
            + \vfg{A}^{*}_0 \exp[-i(\vfg{k}\cdot\vfg{r} - \omega_k t)],
            \label{eq:plane-wave}
        \end{align}
        where the first term represents the positive frequency solution and the
        second term the negative frequency solution.
        The amplitudes $\vfg{A}_0$ are given by
        \begin{align}
            \vfg{A}_0 = \vfg{\epsilon} A_0,
        \end{align}
        where $\vfg{\epsilon}$ is the polarization vector, and we have that
        $\vfg{\epsilon}\cdot\vfg{\epsilon}^{*} = 1$.
        If the polarization vector is real and time-independent, we say that the
        electromagnetic field is linearly polarized.
        The Coulomb gauge condition is satisfied if
        \begin{align}
            \vfg{k}\cdot \vfg{\epsilon} = 0,
        \end{align}
        that is, the wave is transversal and the polarization is perpendicular
        to the propagation direction.
        In three dimensions we can in general write the plane wave solution as a
        linear combination of two linearly independent polarization vectors
        $\vfg{\epsilon}_1$ and $\vfg{\epsilon}_2$ which both are orthogonal to
        the wave vector $\vfg{k}$ thus forming a three-dimensional coordinate
        system.
        This opens up for elliptical polarization of the electromagnetic field.
        However, we will in this thesis limit ourselves to the case of linearly
        polarized fields with real polarization vectors.

        Now, \autoref{eq:plane-wave} describes a classical plane wave solution
        to the wave equation.
        It is possible to quantize the plane wave solution and introduce Fock
        states for the electromagnetic vector potential.
        The laser fields we will be working with will have such a high
        photoncount that the classical description of the field will dominate
        over quantum fluctuations arising from quantization of the fields
        \cite{joachain2012atoms}.
        An alternative is to describe the radiation from the laser field as a
        coherent state, which will allow us to use the quantum mechanical
        description of the electromagnetic field \cite{joachain2012atoms,
        modern-qm}.
        However, we will in the following remain in the classical description of
        the electromagnetic fields yielding a semi-classical approach to
        describing interactions between particles and electromagnetic fields.

        \subsection{Particle-field interactions}
            In a quantum description of the electromagnetic field, we have the
            full Hamiltonian of a particle in a potential $v(\vfg{r})$ given by
            \begin{align}
                \hamil
                = \hamil_{p} + \hamil_{e} + \hamil_{ep},
            \end{align}
            where we've denoted the particle (or, matter) contribution by
            $\hamil_p$, the electromagnetic contribution by $\hamil_e$ and
            finally the interaction between particles and the electromagnetic
            field by $\hamil_{ep}$.
            In the classical description of the electromagnetic field, the
            free-field Hamiltonian takes on the form
            \begin{align}
                \hamilten(t)
                = \half\int\dd^3\vfg{r} \brak{
                    \epsilon_0 \vfg{E}^2(\vfg{r}, t)
                    + \frac{1}{\mu_0}\vfg{B}^2(\vfg{r}, t)
                },
            \end{align}
            where $\epsilon_0$ is the vacuum permittivity and $\mu_0$ the
            magnetic permeability.
            The free-field contribution to the total Hamiltonian is necessary to
            include in order to keep the system of particles and fields
            conservative.
            This term allows for interchange of energy between the particles and
            the electromagnetic fields thus conserving the total energy.
            In the semi-classical description we use, the free-field Hamiltonian
            will reduce to a time-dependent constant as the electric and
            magnetic fields are classical quantities which do not act as
            operators on the quantum states.
            We will ignore the free-field Hamiltonian in the rest of this text
            and treat the particle-field interaction $\hamil_{ep}$ as a
            perturbation to the particle Hamiltonian $\hamil_{p}$, viz
            \begin{align}
                \hamil = \hamil_{p} + \hamil_{ep},
                \label{eq:field-free-hamiltonian}
            \end{align}
            which means that the total energy will \emph{not} be conserved
            while the field is active.
            The particle contribution to the Hamiltonian is given by
            \begin{align}
                \hamil_{p}
                = \frac{\momentumvec^2}{2m}
                + v(\vfg{r}),
            \end{align}
            where we've ignored particle-particle interaction in this
            discussion.
            However, when we do include the Coulomb interaction in the chapter
            on many-body quantum mechanics, this will be included to $\hamil_p$
            without any extra concern for the particle-field interaction
            $\hamil_{ep}$.
            The particle-field interaction term $\hamil_{ep}$ is typically found
            by describing a system with free particles subject to an external
            electromagnetic field in terms of a Lagrangian.
            This Hamiltonian is given by
            \begin{align}
                \hamil_{f}
                = \frac{1}{2m}\brak{
                    \momentumvec
                    - q\vfg{A}
                }^2
                + q\phi,
            \end{align}
            where $q$ is the charge of the particles.
            To get rid of the extra kinetic term we therefore find $\hamil_{ep}$
            by
            \begin{align}
                \hamil_{ep}
                = \hamil_{f}
                - \frac{\momentumvec^2}{2m}
                =
                -\frac{q}{2m}\brak{
                    \momentumvec\cdot \vfg{A}
                    + \vfg{A}\cdot\momentumvec
                }
                + \frac{q^2}{2m}
                \vfg{A}^2
                + q\phi.
                \label{eq:hamil-ep-pre}
            \end{align}
            However, this form comes from a classical description of the
            Hamiltonian and therefore does not include spin.
            This can be added by
            \begin{align}
                \hamil_s
                = \frac{g q}{2m}\hat{\vfg{S}}\cdot\vfg{B},
                \label{eq:hamil-s}
            \end{align}
            where $g$ is the g-factor of the particle.
            The spin-coupling term is small compared to the other two
            particle-field interactions \cite{modern-qm}.
            Furthermore, when we move to the dipole approximation, this term
            will disappear completely and we will therefore ignore the
            spin-coupling.
            Working in the Coulomb gauge, we can make the first term in
            \autoref{eq:hamil-ep-pre} a little shorter.
            If we consider a test function $f = f(\vfg{r})$, we see that
            \begin{align}
                \momentumvec\cdot\vfg{A}f
                = -i\hslash\vfg{\nabla}\cdot\para{
                    \vfg{A}f
                }
                = -i\hslash\para{
                    f\vfg{\nabla}\cdot\vfg{A}
                    + \vfg{A}\vfg{\nabla}f
                }
                = -i\hslash\vfg{A}\vfg{\nabla}f
                = \vfg{A}\cdot \momentumvec f,
            \end{align}
            where going from the second to the third equality we used the
            Coulomb gauge condition that the divergence of $\vfg{A}$ should be
            zero.
            Thus, the particle-field interaction term should be
            \begin{align}
                \hamil_{ep}
                =
                -\frac{q}{m}\vfg{A}\cdot\momentumvec
                + \frac{q^2}{2m}
                \vfg{A}^2
                + q\phi
                \label{eq:hamil-ep}
            \end{align}
            without the spin-coupling.
            Now, the full Hamiltonian in \autoref{eq:field-free-hamiltonian},
            without the spin-coupling, will be invariant under the the quantum
            mechanical gauge transformations
            \cite{joachain2012atoms}
            \begin{gather}
                \vfg{A} \to \vfg{A}' = \vfg{A} + \vfg{\nabla}f,
                \label{eq:gauge-invariant-vector-potential}
                \\
                \phi \to \phi' = \phi - \dpd{f}{t},
                \label{eq:gauge-invariant-scalar-potential}
                \\
                \psi \to \psi' = \exp[\frac{iq}{\hslash}f]\psi,
                \label{eq:gauge-invariant-wave-function}
            \end{gather}
            where $f = f(\vfg{r}, t)$ is a real, differentiable function and
            $\psi = \psi(\vfg{r}, t)$ is a solution to the time-dependent
            Schrödinger equation.
            A demonstration of the invariance of the Hamiltonian from
            \autoref{eq:field-free-hamiltonian} under the listed gauge
            transformations is shown in
            \autoref{app:gauge-invariant-electromagnetic-hamiltonian}.

        \subsection{The dipole approximation}
            If we assume that the wavelength $\lambda$ of the field is much
            larger than the size of the quantum system, that is, if
            \begin{align}
                \lambda \gg \abs{\vfg{r}}
                \implies
                \vfg{k}\cdot\vfg{r} \ll 1,
            \end{align}
            the spatial variations of the electromagnetic field will be small
            compared to the time variations.
            When looking at atomic particles with sizes on the order of
            nanometers, this approximation can be good when the electromagnetic
            field is in the low frequency domain, e.g., infrared light with wave
            lengths of the order of $\SI{700}{\nano\meter}$ to
            $\SI{1}{\milli\meter}$.
            Looking at the plane wave solution in \autoref{eq:plane-wave} we see
            that
            \begin{align}
                \exp[\pm i\vfg{k}\cdot\vfg{r}]
                = 1 \pm i\vfg{k}\cdot\vfg{r}
                - \frac{1}{2}\para{
                    \vfg{k}\cdot\vfg{r}
                }^2
                + \dots,
            \end{align}
            where the first term will dominate.
            The \emph{dipole approximation} consists of choosing this term as
            the only contribution to the spatial variations, i.e.,
            \begin{align}
                \exp[\pm i\vfg{k}\cdot\vfg{r}] \approx 1.
            \end{align}
            This means that $\vfg{A}(\vfg{r}, t) \approx \vfg{A}(t)$ and we get
            a spatially uniform electromagnetic field.
            In the Coulomb gauge this means that
            \begin{gather}
                \vfg{E} = -\dod{}{t}\vfg{A},
                \label{eq:electric-field-dipole}
                \\
                \vfg{B} = \vfg{\nabla}\times \vfg{A} = \vfg{0},
            \end{gather}
            and as promised, the magnetic field and therefore the spin-coupling
            disappears in the particle-field interaction from
            \autoref{eq:hamil-s}.

            In the dipole approximation we can write the vector potential as
            \begin{align}
                \vfg{A}(t)
                = \vfg{A}_0 e^{-i\omega_k t}
                + \vfg{A}^{*}_{0} e^{i\omega_k t}
                = \vfg{\epsilon}\brak{
                    A_0 e^{-i\omega_k t}
                    + A^{*}_0 e^{i\omega_k t}
                }.
            \end{align}
            In the case of a quantized electromagnetic field where the
            amplitudes are photon creation and annihilation operators and all
            operators are expressed in the interaction picture, it is common to
            split the particle-field interaction Hamiltonian into an emission
            part related to the negative frequency solutions and an absorption
            part for the positive frequency solutions.
            It is then possible to observe quantum effects such as spontaneous
            emission where a system will be able to emit a photon even if there
            are no electromagnetic fields present.
            However, as stated, we work in the semi-classical approximation and
            we will assume that quantum fluctuations such as spontaneous
            emission are small compared to classical effects.
            We will therefore collect both the positive and the negative
            frequency solutions into a single term.
            \begin{align}
                \vfg{A}(t)
                = \vfg{\epsilon}\mathcal{A}_0\sin(\omega_k t + \phi),
            \end{align}
            where $\phi$ is a phase factor and we've defined
            \begin{gather}
                2\Re(A_0) = \mathcal{A}_0\sin(\phi), \\
                2\Im(A_0) = \mathcal{A}_0\cos(\phi),
            \end{gather}
            and used the exponential identities for the sine and cosine.
            To go from here we introduce the gauge function
            \begin{align}
                f \equiv - \vfg{A}(t)\cdot\vfg{r},
            \end{align}
            and insert it into the gauge transformations from
            \autoref{eq:gauge-invariant-vector-potential} to
            \autoref{eq:gauge-invariant-wave-function} with $\phi = 0$.
            This yields
            \begin{gather}
                \vfg{A}'
                = \vfg{A} + \vfg{\nabla}f
                = \vfg{0}, \\
                \phi'
                = -\dpd{f}{t}
                = -\vfg{r}\cdot\vfg{E}, \\
                \psi'
                = \exp[-\frac{iq}{\hslash}\vfg{A}(t)\cdot\vfg{r}]\psi,
            \end{gather}
            % TODO: Should care be taken to handle the modified wave function,
            % or is this included in the evolution of the
            % coefficients/amplitudes?
            % I think maybe this gets included...
            where we've inserted the electric field $-\vfg{E} = \dot{\vfg{A}}$
            in the dipole approximation in the gauge transformation for the
            scalar potential.
            Inserted into the time-dependent Schrödinger equation we then find
            \begin{align}
                i\hslash\dpd{}{t}\psi'
                = \brak{
                    \hamil_p
                    - q\positionvec\cdot\vfg{E}
                }\psi'.
            \end{align}
            This transformation is known as the \emph{length gauge} due to the
            position operator $\positionvec$ \cite{joachain2012atoms}.
            Collecting the position operator and the electric charge $q$ we find
            the \emph{electric dipole moment} given by
            \begin{align}
                \hat{\vfg{d}}
                \equiv
                q\positionvec,
            \end{align}
            For the plane-wave solution of the vector potential we get the
            corresponding electric field
            \begin{align}
                \vfg{E}
                = \vfg{\epsilon}\mathcal{E}_0\cos(\omega_k t + \phi),
            \end{align}
            where we've defined the constant
            \begin{align}
                \mathcal{E}_0 = -\omega_k \mathcal{A}_0.
                \label{eq:electric-field-constant}
            \end{align}
            This in total yields the particle-field interaction of the
            Hamiltonian to be
            \begin{align}
                \hamil_{ep}
                = -\hat{\vfg{d}}\cdot\vfg{\epsilon}
                \mathcal{E}_0\cos(\omega_k t + \phi).
            \end{align}
            % TODO: Consider postponing the explicit plane wave solutions to the
            % section on lasers.
            % This might be more natural in terms of introducing the envelope
            % function.



        \subsection{Selection rules}
            Computing the matrix elements of the particle-field interaction term
            of the Hamiltonian between two states $\ket{\psi_{a}}$ and
            $\ket{\psi_{b}}$ we find
            \begin{align}
                \mel{\psi_{a}}{\hamil_{ep}(t)}{\psi_{b}}
                =
                - \mel{\psi_{a}}{\hat{\vfg{d}}}{\psi_b}
                \cdot \vfg{E}(t),
            \end{align}
            where the matrix elements of the dipole moment decides which
            transitions are allowed as a consequence of conservation of spin and
            parity \cite{modern-qm}.
            In short, only non-zero matrix elements for the dipole moment will
            contribute to the total Hamiltonian.
            We will discuss selection rules in more detail when we look at
            specific quantum systems.
            It is worth mentioning that the selection rules only apply for the
            dipole effects.
            This means that an experiment might yield transitions which are
            deemed forbidden in the dipole regime occuring from higher multipole
            expansions in the exponential plane wave solution.

    \section{Laser fields}
        The topic of lasers and laser fields interacting with atomic systems is
        a vast subject, and we will skim lightly at the edge of this field,
        discussing the more important aspects that are related to our
        simulations.
        Now, a linearly polarized, monochromatic laser field in the dipole
        approximation can be described by a vector potential on the form
        \cite{joachain2012atoms}
        \begin{align}
            \vfg{A}(t) = \vfg{\epsilon}\mathcal{A}_0
            \sin(\omega_k t + \phi). \\
        \end{align}
        From \autoref{eq:electric-field-dipole} we find the corresponding
        expression for the electric field to be
        \begin{align}
            \vfg{E}(t) = \vfg{\epsilon} \mathcal{E}_0
            \cos(\omega_k t + \phi),
        \end{align}
        where $\mathcal{E}_0$ is defined as in
        \autoref{eq:electric-field-constant}.
        These two equations describe a spatially homogeneous electric field
        oscillating in time.
        Furthermore, they describe a laser that is ``switched on'' for the
        entire simulation.
        However, we are often interested in firing a short laser pulse at the
        system before turning it off.
        Thus we can observe how the system reacts after the laser is switched
        off and we avoid driving the system in time.

        A linearly polarized laser pulse in the dipole approximation can be
        described by the vector potential \cite{joachain2012atoms}
        \begin{align}
            \vfg{A}(t)
            = \vfg{\epsilon} \mathcal{A}_0
            \int_{-\infty}^{t}
            \dd t'
            F(t')
            \cos(\omega_k t' + \phi),
        \end{align}
        where we treat $\omega_k$ and $\phi$ constant in time over the response
        of the system.
        The function $F(t) \in \brak{0, 1}$ is known as the \emph{envelope} of the
        pulse and we'll discuss it in due time.
        We find the electric field of the dipole laser from
        \autoref{eq:electric-field-dipole}.
        \begin{align}
            \vfg{E}(t)
            = \vfg{\epsilon} \mathcal{E}_0 F(t) \cos(\omega_k t + \phi).
        \end{align}
        We see that by setting $F(t) = 1$ we recover the expression for the
        monochromatic, linearly polarized, laser field.

        \subsection{Envelope}
            The purpose of the envelope function $F(t)$ is to ensure that the
            laser pulse goes smoothly to zero outside some defined region such
            that the electric field disappears at $t \to \pm \infty$.
            Furthermore it defines the temporal shape of the field.
            % TODO: Check if the purpose of the envelope function can be
            % summarized a little better.
            There are different models for the envelope function defined to
            replicate real laser pulses.
            Common choices are the cosine-squared, Gaussian, and the hyperbolic
            secant functions \cite{joachain2012atoms}.
            The cosine-squared envelope function is given by
            \begin{gather}
                F(t) = \begin{cases}
                    \cos^2\brak{
                        \pi t / T
                    }, & t \in [-T / 2, T / 2], \\
                    0, & \text{else},
                \end{cases}
            \end{gather}
            where $T$ decides how long the envelope should last.
            We define a cycle of the laser pulse to be $2\pi/\omega_k$.
            It is common to choose $T = 2n\pi\omega_k$, where $n \in
            \mathbb{N}\setminus{0}$ to ensure that we fit $n$ cycles of the
            laser pulse inside the envelope, but this is strictly not necessary
            and we can choose any value for $T$.
            As the cosine-squared envelope is a periodic function, we have to
            define a region where the function should be cut off and yield zero.
            This is solved by the cases-statement above.
            On the other hand, the hyperbolic secant and the Guassian envelopes
            will remove the pulse gradually.
            \begin{gather}
                F(t) = \sech\brak{
                    \frac{\pi t}{T}
                }, \\
                F(t) = \exp[
                    -\frac{(\pi t)^2}{2T^2}
                ].
            \end{gather}
            These functions are however harder to turn on and off as they are
            small, but non-zero, for a wide range of time points $t$ both before
            and after the peak intensity.
            In accordance with much of the litterature, we will almost
            exclusively use the sine-squared envelope function given by
            \begin{align}
                F(t) = \begin{cases}
                    \sin^2\brak{
                        \pi t / T
                    }, & t \in [0, T], \\
                    0, & \text{else}.
                \end{cases}
            \end{align}
            This envelope shares the same properties as the cosine-squared
            envelope, but it is often more convenient to start in $t_0 = 0$ by
            turning on the laser instead of setting $t_0 < 0$.
            A plot of the different envelope functions during a pulse is shown
            in \autoref{fig:envelope-functions}.

            \begin{figure}
                \centering
                \begin{tikzpicture}
                    \begin{groupplot}[
                            group style={
                                group size=2 by 2,
                                vertical sep=50pt,
                                horizontal sep=50pt,
                                xlabels at=edge bottom,
                                ylabels at=edge left,
                            },
                            width=6cm,
                            height=6cm,
                            xlabel={$t$ $[\text{a.u.}]$},
                            ylabel={$E(t)$ $[\text{a.u.}]$},
                        ]

                        % Sine
                        \nextgroupplot[
                            title={Sine-squared envelope},
                            grid=major,
                            enlarge x limits=false,
                            xmin=-10,
                            xmax=10,
                        ]
                            \addplot+[
                                mark=none,
                                thick,
                            ]
                            table
                            {theory/quantum-mechanics/dat/env_sine.dat};
                            \addplot+[
                                mark=none,
                                thick,
                            ]
                            table
                            {theory/quantum-mechanics/dat/laser_sine.dat};

                        % Sech
                        \nextgroupplot[
                            title={Hyperbolic secant envelope},
                            grid=major,
                            enlarge x limits=false,
                        ]
                            \addplot+[
                                mark=none,
                                thick,
                            ]
                            table
                            {theory/quantum-mechanics/dat/env_sech.dat};
                            \addplot+[
                                mark=none,
                                thick,
                            ]
                            table
                            {theory/quantum-mechanics/dat/laser_sech.dat};

                        % Cosine
                        \nextgroupplot[
                            title={Cosine-squared envelope},
                            grid=major,
                            enlarge x limits=false,
                            xmin=-10,
                            xmax=10,
                        ]
                            \addplot+[
                                mark=none,
                                thick,
                            ]
                            table
                            {theory/quantum-mechanics/dat/env_cosine.dat};
                            \addplot+[
                                mark=none,
                                thick,
                            ]
                            table
                            {theory/quantum-mechanics/dat/laser_cosine.dat};

                        % Exp
                        \nextgroupplot[
                            title={Gaussian envelope},
                            grid=major,
                            enlarge x limits=false,
                        ]
                            \addplot+[
                                mark=none,
                                thick,
                            ]
                            table
                            {theory/quantum-mechanics/dat/env_gauss.dat};
                            \addplot+[
                                mark=none,
                                thick,
                            ]
                            table
                            {theory/quantum-mechanics/dat/laser_gauss.dat};
                    \end{groupplot}
                \end{tikzpicture}
                \caption{In these figures we plot the sine-squared,
                cosine-squared, hyperbolic secant, and Gaussian envelope
                functions $F(t)$ along with an electric field $E(t)$ under the
                envelope.
                We have used $\mathcal{E}_0 = 1$, $\omega_k = 4$, $T =
                10\pi/\omega_k$ (giving five cycles of the laser pulse inside
                the envelope) on a grid with $t \in [-30, 30]$.
                The choice of the strength $\mathcal{E}_0$ and the frequency is
                such that the figures look presentable.
                Note that the figures for the sine- and cosine-squared envelopes
                are plotted in a shorter time-scale than the other two functions
                to bring out more details.
                The phase factor in the cosine of the electric field is set to
                $\phi = -\pi/2$ to give a sine function.}
                \label{fig:envelope-functions}
            \end{figure}

        \subsection{Measuring energy transitions}
            To see the usefulness of laser induced dynamics, we consider a
            system described by a Hamiltonian
            \begin{align}
                \hamil(t)
                = \hamil_p + \hamil_{ep}(t),
            \end{align}
            where the time-dependence is kept in the particle-field interaction.
            The scenarios we will consider consists of describing the system
            before and after the laser pulse is active.
            At $t = 0$ we have the time-independent Schrödinger equation
            \begin{align}
                \hamil(0)\ket{\phi_k}
                = \hamil_p\ket{\phi_k}
                = \energy_k\ket{\phi_k},
            \end{align}
            where $(\energy_k, \ket{\phi_k})$ are the eigenpairs of the
            time-independent Hamiltonian, and the eigenstates are orthonormal.
            Now, if we choose a particular $\ket{\phi_k}$ as our initial
            state\footnote{%
                We will always choose the ground state as our starting point in
                this thesis.
            } we can ``turn on'' the laser pulse for $t \in \brak{0, T}$ using
            an envelope function.
            We can in some ways conider the laser as a thermalization step where
            we move from an initial state that might be slightly unphysical in
            the sense that the pure eigenstates are very unlikely states to find
            a system in.
            For $t \geq T$ we have $\hamil(t \geq T) = \hamil(0)$, that is,
            $\hamil(t \geq T)$ has the same spectrum as $\hamil(0)$.
            At $t = T$ we can then write the evolved state as
            \begin{align}
                \ket{\psi(T)} = \sum_{k = 0}^{\infty}
                c_k(T) \ket{\phi_k},
            \end{align}
            In terms of measurements, the state $\ket{\psi(T)}$ will be our
            initial state as this is a thermalized state from some initial
            eigenstate $\ket{\phi_k}$.
            The time-evolution of an arbitrary state $\ket{\psi(t)}$ can be
            described by
            \begin{align}
                \ket{\psi(t \geq T)}
                = \sum_{k = 0}^{\infty} c_k(t)\ket{\phi_k}
                = \sum_{k = 0}^{\infty}
                c_k(T) e^{-i\energy_k t/\hslash}\ket{\phi_k},
            \end{align}
            where the time-evolution is kept in the coefficients.
            There are now two quantities which are of interest as they provide
            information on the energy transitions in a system and we will look
            at these separately.

            \subsubsection{Autocorrelation}
                We define the autocorrelation as the overlap between the
                time-evolved stated $\ket{\psi(t)}$ with the initial state
                $\ket{\psi(T)}$ for $t \geq T$ \cite{robinett20041,
                pedersen2018symplectic},\footnote{%
                    Note that the autocorrelation can be computed for arbitrary
                    states, but we use it in a specific way for measurements.
                }
                \begin{align}
                    A(t)
                    &= \braket{\psi(t)}{\psi(T)}
                    =
                    \sum_{i = 0}^{\infty}
                    \sum_{j = 0}^{\infty}
                    c^{*}_i(t) c_j(T)
                    \braket{\phi_i}{\phi_j}
                    \\
                    &=
                    \sum_{i = 0}^{\infty}
                    \sum_{j = 0}^{\infty}
                    c^{*}_i(T) c_j(T) e^{-i E_i t/\hslash}
                    \delta_{ij}
                    =
                    \sum_{i = 0}^{\infty}
                    \abs{c_i(T)}^2 e^{-i E_i t/\hslash},
                \end{align}
                where we've used that the eigenstates of the time-independent
                Hamiltonian are orthonormal.

            \subsubsection{Dipole moment}
                Computing the expectation value of the dipole moment for $t \geq
                T$ we have
                \begin{align}
                    \expval{\vfg{d}(t)}
                    &= \mel{\psi(t)}{\hat{\vfg{d}}}{\psi(t)}
                    =
                    c^{*}_i(t)c_j(t)\mel{\phi_i}{\hat{\vfg{d}}}{\phi_j}
                    \\
                    &=
                    c^{*}_i(T)c_j(T) e^{i E_i t/\hslash}
                    e^{-i E_j t / \hslash}
                    \vfg{d}_{ij}
                    =
                    c^{*}_i(0)c_j(0) e^{i \omega_{ij} t}
                    \vfg{d}_{ij},
                \end{align}
                where we've defined the energy transition frequency by
                \begin{align}
                    \omega_{ij} \equiv \frac{E_i - E_j}{\hslash},
                \end{align}
                and the matrix elements of the dipole moment using the
                eigenstates by $\vfg{d}_{ij}$.
                Note that we typically measure along one direction in the dipole
                moment, but we've kept the vector notation to demonstrate that
                we can choose whichever axis of the dipole moment we like.
                % TODO: Consider moving the subsubsections into defintions.

                Now, we often do not have access to the spectrum of the
                time-independent system and the right-hand sides of the
                expectation value of the dipole moment and the autocorrelation
                is not something we can evaluate directly.
                However, by inspection we see that the right-hand sides bears
                resemblance to the \emph{discrete Fourier transform}.
                % TODO: Consider adding the formulas for the Fourier transform.
                This inspires the calculation of the Fourier transform of the
                autocorrelation and the dipole moment.
                The peaks in the Fourier spectrum can then be interpreted as the
                excited state $\energy_i$ from the autocorrelation.
                The peaks in the Fourier spectrum of the dipole moment yields
                the energy transitions $\omega_{ij}$.
                Note that we often sample the dipole moment and the
                autocorrelation from $T = 0$ while the laser is active, but we
                ignore all values of the autocorrelation that are generated
                while the laser is active.

            \subsubsection{Time-dependent overlap}
                In this thesis we will not compute the autocorrelation as it
                stands, but rather compute the time-dependent overlap.
                This is given by
                \begin{align}
                    P(t, t_0)
                    &= \abs{\braket{\psi(t)}{\psi(t_0)}}^2.
                \end{align}
                Instead of taking the Fourier transform of this quantity we use
                it as a measure of the transition probability to go from state
                $\ket{\psi(t_0)}$ to $\ket{\psi(t)}$.
                It can also be interpreted as the ``amount'' of
                $\ket{\psi(t_0)}$ in state $\ket{\psi(t)}$.
                This can be used as a measure of how much the state changes in
                time from some initial configuration.


    \section{Atomic units}
        When doing large calculations on quantum mechanical systems, the
        constants related to the physical units quickly becomes quite populous
        and can lead to numerical errors and might overshadow the important
        concepts of the theory.
        As a consequence, we tend to set most units to unity thus eliminating
        them from the equations.
        However, this means that in order to recover physical expressions, we
        must re-insert the constants to get the proper units and magnitudes.
        We will for the most part use Hartree atomic units \cite{hartree_1928}
        where we set the following quantities to unity:
        \begin{itemize}
            \item Reduced Planck's constant $\hslash = \SI{1}{\text{a.u.}}$.
            \item The magnitude of the charge of the electron
                $e = \SI{1}{\text{a.u.}}$.
            \item The electron rest mass $m_e = \SI{1}{\text{a.u.}}$.
            \item The Coulomb force constant
                $k_e = (4\pi \epsilon_0)^{-1} = \SI{1}{\text{a.u.}}$.
        \end{itemize}
        We've denoted the units by $\si{\text{a.u.}}$ for atomic units.
        As a consequence of converting to atomic units, we now measure various
        physical units in a scaled system.
        A list of the derived units and how they can be scaled are shown in
        \autoref{tab:atomic-units}.
        \begin{table}
            \centering
            \caption{In this table we list some of derived atomic units from
            working in Hartree atomic units \cite{wiki:units,
            joachain2012atoms}.}
            \renewcommand{\arraystretch}{1.3}
            \begin{tabular}{@{}ll@{}}
                \toprule
                Dimension & Expression
                \\
                \midrule
                Length
                &
                $a_0 = 4\pi\epsilon_0\hslash^2/\para{m_e e^2}$
                \\
                Energy
                &
                $E_h = m_e e^4/\para{4\pi \epsilon_0 \hslash}^2$
                \\
                Time
                &
                $t_a = \hslash/E_h$
                \\
                Momentum
                &
                $p_a = \hslash/a_0$
                \\
                Electric field
                &
                $\mathcal{E}_a = E_h/\para{e a_0}$
                \\
                Intensity
                &
                $I_a = \half\epsilon_0 c \mathcal{E}^2_a$
                \\
                \bottomrule
            \end{tabular}
            \label{tab:atomic-units}
        \end{table}
        % TODO: Remember to include atomic units for the intensity.
