\chapter{Quantum mechanics}
    \epigraph{The underlying physical laws necessary for the mathematical
    theory of a large part of physics and the whole of chemistry are thus
    completely known, and the difficulty is only that the exact application of
    these laws leads to equations much too complicated to be soluble.}
    {--- P. A. M. Dirac}

    We start our journey by reviewing parts of quantum mechanics that we deem
    necessary in order to understand the thesis.

    \section{The postulates of quantum mechanics}
        In order to make sure that we have a common understanding of how to
        understand and interpret quantum mechanics we begin by introducing the
        postulates of quantum mechanics.
        The postulates were originally developed by Dirac
        \cite{dirac1981principles} and Neumann \cite{von2018mathematical},
        but have since been subject to interpretation.
        This has lead to many versions of the postulates, both in the number of
        postulates, and in the accuracy in their description.
        We will base our description of the postulates of quantum mechanics from
        the book \citetitle{salasnich2017quantum} by
        \citeauthor{salasnich2017quantum} \cite{salasnich2017quantum}.

        \begin{enumerate}
            \item A state $\ket{\psi}$ is a unitary vector defined on a
                separably complex Hilbert space.
                The state can be expanded in any complete set of basis vectors
                on the Hilbert space by
                \begin{align}
                    \ket{\psi} = c_i\ket{i},
                \end{align}
                where $c_i \in \mathbb{C}$.
            \item An observable is described by a Hermitian operator $\hat{Q}$
                acting on the Hilbert space of state vectors.
            \item The eigenvalue $q$ of the observable $\hat{Q}$ represents its
                measurable values.
                That is,
                \begin{align}
                    \hat{Q}\ket{q} = q\ket{q},
                \end{align}
                where $\para{q, \ket{q}}$ are the eigenpairs of the observable.
            \item We can measure the probability $p$ of finding the normalized
                state $\ket{\psi}$ in the normalized eigenstate $\ket{q}$ by
                \begin{align}
                    p = \abs{\braket{g}{\psi}}^2.
                \end{align}
                As a consequence $p$ also gives the probability of measuring the
                eigenvalue $g$.
                We can also measure the expectation value of the observable
                $\hat{Q}$ for the state $\ket{\psi}$ by
                \begin{align}
                    \expv{Q} = \mel{\psi}{\hat{Q}}{\psi}.
                \end{align}
            \item The time-evolution of a state $\ket{\psi(t)}$ is defined by
                the Schrödinger equation
                \begin{align}
                    i\hslash \dod[]{}{t}\ket{\psi(t)} = \hamil\ket{\psi(t)},
                \end{align}
                where we work in the Schrödinger picture.
        \end{enumerate}

    \section{Canonical quantization}
        \subsection{The Schrödinger equation}

    \section{Time-independent Schrödinger equation}
    \section{The variational principle}
        The variational principle tells us that the ``true'' ground state energy
        $\energy_1$, i.e., the lowest energy eigenvalue of the Hamiltonian
        $\hamil$, will always be the lower bound on the energy of the system.
        This means that all approximate wave functions to the Hamiltonian will
        serve as an upper bound to the ground state energy
        \cite{griffiths2017introduction}.
        \begin{theorem}
            Given a Hamiltonian $\hamil$ describing the system we are looking
            at and a normalized wave function $\ket{\psi}$, we have that
            \begin{align}
                \energy_1
                \leq
                E[\psi]
                = \bra{\psi}\hamil\ket{\psi},
                \label{eq:variational-principle}
            \end{align}
            where $E[\psi]$ is an energy functional dependent on the shape of
            the wave function $\ket{\psi}$.
            Here $\energy_1$ represents the true ground state of the
            Hamiltonian.
        \end{theorem}
        The variational principle guarantees that any wave function $\ket{\psi}$
        will overestimate the ground state energy unless we happen upon the true
        ground state.
        \begin{proof}
            The eigenstates of the Hamiltonian will form a complete basis set
            for the Hilbert space they are apart of.
            This means that we can construct any state $\ket{\psi}$ as a linear
            combination of these basis functions
            \begin{align}
                \ket{\psi} = \sum_{i = 1}^{N} c_{i}\ket{\phi_i},
            \end{align}
            where the basis functions $\brac{\ket{\phi_i}}_{i = 1}^{N}$ are
            eigenstates of the Hamiltonian
            \begin{align}
                \hamil\ket{\phi_i} = \energy_i\ket{\phi_i},
            \end{align}
            such that $\energy_1 \leq \energy_2 \leq \dots \leq \energy_N$.
            Furthermore, in our formulation of the variational principle, i.e.,
            for normalized wave functions, we require that the basis functions
            are orthornomal.
            \begin{align}
                \braket{\phi_i}{\phi_j} = \delta_{ij},
            \end{align}
            and that the coefficients yield
            \begin{align}
                \abs{\vfg{c}}^2 = \sum_{i = 1}^{N} c^{*}_{i}c_{i} = 1.
            \end{align}
            Inserting the state $\ket{\psi}$ into the energy funtional in
            \autoref{eq:variational-principle} we find
            \begin{align}
                E[\psi]
                &= \sum_{i = 1}^{N}\sum_{j = 1}^{N}
                c^{*}_i\bra{\phi_i}\hamil\ket{\phi_j}c_j
                = \sum_{i = 1}^{N}\sum_{j = 1}^{N}
                c^{*}_i c_j \energy_j\delta_{ij}
                = \sum_{i = 1}^{N}
                \abs{c_i}^2 \energy_i.
            \end{align}
            However, by definition $\energy_1$ is the smallest eigenvalue of the
            Hamiltonian, i.e., $\energy_1 \leq \energy_i$, for all $i \in
            \brac{1, \dots, N}$.
            From whence, we find
            \begin{align}
                E[\psi]
                &= \sum_{i = 1}^{N}
                \abs{c_i}^2 \energy_i
                \geq
                \energy_1 \sum_{i = 1}^{N}
                \abs{c_i}^{2}
                = \energy_1,
            \end{align}
            which shows that $\energy_1$ serves as a lower bound for the energy
            of the system under observation.
        \end{proof}

        \subsection{The variational method}
            The variational principle tells us that the energy we find from a
            trial wave function $\ket{\psi}$ will be an upper bound to the
            ground state energy, but it does not provide us with a method of
            finding the \emph{best} upper bound energy.

    \section{The Hellmann-Feynman theorem}
        The Hellmann-Feynman theorem provides us with a method of calculating
        first-order change (also known as a first order property) in the energy
        due to a perturbation \cite{helgaker-molecular}.
        \begin{theorem}
            If $\ket{\psi}$ is a normalized eigenstate of the Hamiltonian
            $\hamil$, or $\ket{\psi}$ is variationally determined from the
            Hamiltonian $\hamil$, the Hellmann-Feynman theorem \cite{feynman}
            states that
            \begin{align}
                \left.\dod[]{E(\alpha)}{\alpha}\right\rvert_{\alpha = 0}
                =
                \left.
                \dpd[]{}{\alpha}
                \bra{\psi_{\alpha}}\hamil + \alpha\hat{V}\ket{\psi_{\alpha}}
                \right\rvert_{\alpha = 0}
                = \bra{\psi}\hat{V}\ket{\psi},
                \label{eq:hellmann-feynman}
            \end{align}
            where $\alpha$ is a perturbational parameter and $\hat{V}$ the
            perturbation operator.
            The wave function $\ket{\psi_{\alpha}}$ is given by
            \begin{align}
                \ket{\psi_{\alpha}} = N(\ket{\psi} + \alpha\ket{\delta\psi}),
            \end{align}
            where $N$ is a normalization factor.
            For approximate wave functions, they must be optimized with respect
            to the same variational parameter $\alpha$ as in the theorem.
        \end{theorem}
        \begin{proof}
            The underlying assumption is that
            \begin{align}
                \hamil\ket{\psi} = E\ket{\psi},
            \end{align}
            regardless if we have an exact or a variationally determined wave
            function $\ket{\psi}$.
            Furthermore, both perturbed and unperturbed wave functions are
            normalized, viz.
            \begin{align}
                \braket{\psi}{\psi} = \braket{\psi_{\alpha}}{\psi_{\alpha}} = 1
                \implies
                \dpd[]{}{\alpha} = \braket{\psi_{\alpha}}{\psi_{\alpha}} = 0.
            \end{align}
            See reference \cite{helgaker-molecular} for a proof where the
            normalization of the perturbed wave function is relaxed.
            We now prove \autoref{eq:hellmann-feynman} directly.
            \begin{align}
                \left.\dod[]{E(\alpha)}{\alpha}\right\rvert_{\alpha = 0}
                &=
                \left.
                \dpd[]{}{\alpha}
                \bra{\psi_{\alpha}}\hamil + \alpha\hat{V}\ket{\psi_{\alpha}}
                \right\rvert_{\alpha = 0}
                \\
                &=
                \bra{\delta\psi}\hamil\ket{\psi}
                + \bra{\psi}\hat{V}\ket{\psi}
                + \bra{\psi}\hamil\ket{\delta\psi}
                \\
                &=
                \energy \brak{
                    \braket{\delta\psi}{\psi}
                    + \braket{\psi}{\delta\psi}
                }
                + \bra{\psi}\hat{V}\ket{\psi}
                \\
                &=
                \energy \dpd[]{}{\alpha}\braket{\psi}{\psi}
                + \bra{\psi}\hat{V}\ket{\psi}
                \\
                &=
                \bra{\psi}\hat{V}\ket{\psi},
            \end{align}
            which is what we wanted to show.
        \end{proof}
    \section{Time-dependent Schrödinger equation}
    \section{The time evolution operators}
        We will in the following subsection stay close to the derivation of
        \citeauthor{ullrich2011time} \cite{ullrich2011time}, section 3.1.2, and
        \citeauthor{joachain2012atoms} \cite{joachain2012atoms}, section 5.1.
        Any solution to the time-dependent Schrödinger equation is given by
        \begin{align}
            \ket{\Psi(t)} = \hat{U}(t, t_0)\ket{\Psi(t_0)},
        \end{align}
        where $\hat{U}(t, t_0)$ is the time evolution operator that acts on the
        initial state $\ket{\Psi(t_0)}$, and yields the state $\ket{\Psi(t)}$ at
        some later time $t$. The time evolution operators are unitary at common
        times, viz.
        \begin{align}
            \hat{U}^{\dagger}(t, t_0)\hat{U}(t, t_0) = \1.
        \end{align}
        This means that we can go ``backwards in time'' in the sense that going
        from $\ket{\Psi(t)} \to \ket{\Psi(t_0)}$ is done by
        \begin{align}
            \ket{\Psi(t_0)} = \hat{U}^{\dagger}(t, t_0)\ket{\Psi(t)}.
        \end{align}
        Furthermore, we can compose a time evolution operator from other time
        evolution operators as
        \begin{align}
            \hat{U}(t_2, t_0) = \hat{U}(t_2, t_1)\hat{U}(t_1, t_0),
        \end{align}
        where $t_2 \geq t_1 \geq t_0$. If the Hamiltonian is time-independent,
        i.e., $\hat{H}(t) = \hat{H}$, the time evolution operator takes on the
        form
        \begin{align}
            \hat{U}(t, t_0) = \exp\brac{
                \frac{-i \hat{H} (t - t_0)}{\hslash}
            }.
            \label{eq:ti-evolution}
        \end{align}
        However, if the Hamiltonian is time-dependent, the time evolution
        operator takes on a much more complicated shape.
        \begin{align}
            \hat{U}(t, t_0) =
            \mathcal{T}\exp\brac{
                -\frac{i}{\hslash} \int_{t_0}^{t} \dd\tau
                \hat{H}(\tau)
            },
            \label{eq:td-evolution}
        \end{align}
        where $\mathcal{T}$ is the time-ordering operator. As an extra
        complicating factor, the time-dependent Hamiltonian might not commute
        with itself at different times. Here \autoref{eq:ti-evolution} and
        \autoref{eq:td-evolution} serve as the theoretical foundation for how
        the time-evolution is practically done in a numerical scheme.

    \section{The time-dependent variational principle}


    \section{Density operators}
        When working with many-body quantum mechanics, computing expectation
        values can at times prove easier when done using density matrices. A
        general density matrix of a \emph{pure state} is on the form
        \begin{align}
            \densitymatrix = \ket{\psi}\bra{\psi},
        \end{align}
        that is, a pure state is a quantum state $\ket{\psi}$ containing the
        maximum amount of information about a given system. For a \emph{mixed
        state}, i.e., a linear combination of pure states $\ket{\psi_k}$ with a
        classical probability $p_k$ associated with the state, we get a density
        matrix on the form
        \begin{align}
            \densitymatrix = \sum_{k} p_k \ket{\psi_k}\bra{\psi_k}.
        \end{align}
        Any density operator must satisfy the following
        properties \cite{modern-qm}:
        \begin{enumerate}
            \item Hermiticity, that is
                \begin{align}
                    p_k = p_k^{*} \implies \densitymatrix = \densitymatrix^{\dagger}.
                \end{align}
                This translates to the probabilities being real, $p_k \in
                \mathbb{R}$.
            \item Positivity,
                \begin{align}
                    p_k \geq 0 \implies \bra{\chi}\densitymatrix\ket{\chi} \geq 0.
                \end{align}
                In other words, density matrices are \emph{positive
                semidefinite}.
            \item Normalization of the probabilities,
                \begin{align}
                    \sum_{k} p_k = 1 \implies \tr(\densitymatrix),
                \end{align}
                that is, the probabilities must sum up to one.
        \end{enumerate}
        Furthermore, by squaring the density matrix and taking the trace we can
        infer if the system we are perusing is in a mixed state or a pure state
        \cite{modern-qm}.
        \begin{align}
            \tr(\densitymatrix^2) = \sum_{k} p_k^2 \leq 1,
        \end{align}
        with equality if, and only if, the system is in a pure state, viz.
        \begin{align}
            \densitymatrix = \ket{\psi}\bra{\psi}
            \implies \densitymatrix^2 = \densitymatrix
            \implies \tr(\densitymatrix^2) = 1.
        \end{align}
        Using density matrices, we can compute the expectation value of any
        operator $\hat{O}$, by \cite{modern-qm}
        \begin{align}
            \expv{\hat{O}} = \tr(\hat{O}\densitymatrix).
        \end{align}
        % TODO: Work this out further

    \section{Quantizing the electromagnetic field}
        \subsection{Dipole moment}
        \subsection{Energy spectrum}

    \section{Laser field}
        \subsection{Polarization}
        \subsection{Envelope}

    \section{Physical units}
        \subsection{Atomic units}
