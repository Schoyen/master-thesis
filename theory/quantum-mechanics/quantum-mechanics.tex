\chapter{Quantum mechanics}
    \epigraph{The underlying physical laws necessary for the mathematical
    theory of a large part of physics and the whole of chemistry are thus
    completely known, and the difficulty is only that the exact application of
    these laws leads to equations much too complicated to be soluble.}
    {--- P. A. M. Dirac}

    We start our journey by reviewing parts of quantum mechanics that we deem
    necessary in order to understand the thesis.

    \section{The postulates of quantum mechanics}
        In order to make sure that we have a common understanding of how to
        understand and interpret quantum mechanics we begin by introducing the
        postulates of quantum mechanics.
        The postulates were originally developed by Dirac
        \cite{dirac1981principles} and Neumann \cite{von2018mathematical},
        but have since been subject to interpretation.
        This has lead to many versions of the postulates, both in the number of
        postulates, and in the accuracy in their description.
        We will base our description of the postulates of quantum mechanics from
        the book \citetitle{salasnich2017quantum} by
        \citeauthor{salasnich2017quantum} \cite{salasnich2017quantum}.

        \begin{enumerate}
            \item A state $\ket{\psi}$ is a unitary vector defined on a
                separably complex Hilbert space.
                The state can be expanded in any complete set of basis vectors
                on the Hilbert space by
                \begin{align}
                    \ket{\psi} = c_i\ket{i},
                \end{align}
                where $c_i \in \mathbb{C}$.
            \item An observable is described by a Hermitian operator $\hat{Q}$
                acting on the Hilbert space of state vectors.
            \item The eigenvalue $q$ of the observable $\hat{Q}$ represents its
                measurable values.
                That is,
                \begin{align}
                    \hat{Q}\ket{q} = q\ket{q},
                \end{align}
                where $\para{q, \ket{q}}$ are the eigenpairs of the observable.
            \item We can measure the probability $p$ of finding the normalized
                state $\ket{\psi}$ in the normalized eigenstate $\ket{q}$ by
                \begin{align}
                    p = \abs{\braket{g}{\psi}}^2.
                \end{align}
                As a consequence $p$ also gives the probability of measuring the
                eigenvalue $g$.
                We can also measure the expectation value of the observable
                $\hat{Q}$ for the state $\ket{\psi}$ by
                \begin{align}
                    \expv{Q} = \mel{\psi}{\hat{Q}}{\psi}.
                \end{align}
            \item The time-evolution of a state $\ket{\psi(t)}$ is defined by
                the Schrödinger equation
                \begin{align}
                    i\hslash \dod[]{}{t}\ket{\psi(t)} = \hamil\ket{\psi(t)},
                \end{align}
                where we work in the Schrödinger picture.
        \end{enumerate}

    \section{Canonical quantization}
        \subsection{The Schrödinger equation}

    \section{Time-independent Schrödinger equation}

    \section{Density operators}
        \label{sec:density-operators}
        When working with many-body quantum mechanics, computing expectation
        values can at times prove easier when done using density matrices. A
        general density matrix of a \emph{pure state} is on the form
        \begin{align}
            \densitymatrix = \ket{\psi}\bra{\psi},
        \end{align}
        that is, a pure state is a quantum state $\ket{\psi}$ containing the
        maximum amount of information about a given system. For a \emph{mixed
        state}, i.e., a linear combination of pure states $\ket{\psi_k}$ with a
        classical probability $p_k$ associated with the state, we get a density
        matrix on the form
        \begin{align}
            \densitymatrix = \sum_{k} p_k \ket{\psi_k}\bra{\psi_k}.
        \end{align}
        Any density operator must satisfy the following
        properties \cite{modern-qm}:
        \begin{enumerate}
            \item Hermiticity, that is
                \begin{align}
                    p_k = p_k^{*} \implies \densitymatrix = \densitymatrix^{\dagger}.
                \end{align}
                This translates to the probabilities being real, $p_k \in
                \mathbb{R}$.
            \item Positivity,
                \begin{align}
                    p_k \geq 0 \implies \bra{\chi}\densitymatrix\ket{\chi} \geq 0.
                \end{align}
                In other words, density matrices are \emph{positive
                semidefinite}.
            \item Normalization of the probabilities,
                \begin{align}
                    \sum_{k} p_k = 1 \implies \tr(\densitymatrix),
                \end{align}
                that is, the probabilities must sum up to one.
        \end{enumerate}
        Furthermore, by squaring the density matrix and taking the trace we can
        infer if the system we are perusing is in a mixed state or a pure state
        \cite{modern-qm}.
        \begin{align}
            \tr(\densitymatrix^2) = \sum_{k} p_k^2 \leq 1,
        \end{align}
        with equality if, and only if, the system is in a pure state, viz.
        \begin{align}
            \densitymatrix = \ket{\psi}\bra{\psi}
            \implies \densitymatrix^2 = \densitymatrix
            \implies \tr(\densitymatrix^2) = 1.
        \end{align}
        Using density matrices, we can compute the expectation value of any
        operator $\hat{O}$, by \cite{modern-qm}
        \begin{align}
            \expv{O} = \tr(\hat{O}\densitymatrix).
        \end{align}
        % TODO: Work this out further

    \section{The variational principle}
        The variational principle tells us that the ``true'' ground state energy
        $\energy_1$, i.e., the lowest energy eigenvalue of the Hamiltonian
        $\hamil$, will always be the lower bound on the energy of the system.
        This means that all approximate wave functions to the Hamiltonian will
        serve as an upper bound to the ground state energy
        \cite{griffiths2017introduction}.
        \begin{theorem}
            Given a Hamiltonian $\hamil$ describing the system we are looking
            at and a normalized wave function $\ket{\psi}$, we have that
            \begin{align}
                \energy_1
                \leq
                E[\psi]
                = \bra{\psi}\hamil\ket{\psi},
                \label{eq:variational-principle}
            \end{align}
            where $E[\psi]$ is an energy functional dependent on the shape of
            the wave function $\ket{\psi}$.
            Here $\energy_1$ represents the true ground state of the
            Hamiltonian.
        \end{theorem}
        The variational principle guarantees that any wave function $\ket{\psi}$
        will overestimate the ground state energy unless we happen upon the true
        ground state.
        \begin{proof}
            The eigenstates of the Hamiltonian will form a complete basis set
            for the Hilbert space they are apart of.
            This means that we can construct any state $\ket{\psi}$ as a linear
            combination of these basis functions
            \begin{align}
                \ket{\psi} = \sum_{i = 1}^{N} c_{i}\ket{\phi_i},
            \end{align}
            where the basis functions $\brac{\ket{\phi_i}}_{i = 1}^{N}$ are
            eigenstates of the Hamiltonian
            \begin{align}
                \hamil\ket{\phi_i} = \energy_i\ket{\phi_i},
            \end{align}
            such that $\energy_1 \leq \energy_2 \leq \dots \leq \energy_N$.
            Furthermore, in our formulation of the variational principle, i.e.,
            for normalized wave functions, we require that the basis functions
            are orthornomal.
            \begin{align}
                \braket{\phi_i}{\phi_j} = \delta_{ij},
            \end{align}
            and that the coefficients yield
            \begin{align}
                \abs{\vfg{c}}^2 = \sum_{i = 1}^{N} c^{*}_{i}c_{i} = 1.
            \end{align}
            Inserting the state $\ket{\psi}$ into the energy funtional in
            \autoref{eq:variational-principle} we find
            \begin{align}
                E[\psi]
                &= \sum_{i = 1}^{N}\sum_{j = 1}^{N}
                c^{*}_i\bra{\phi_i}\hamil\ket{\phi_j}c_j
                = \sum_{i = 1}^{N}\sum_{j = 1}^{N}
                c^{*}_i c_j \energy_j\delta_{ij}
                = \sum_{i = 1}^{N}
                \abs{c_i}^2 \energy_i.
            \end{align}
            However, by definition $\energy_1$ is the smallest eigenvalue of the
            Hamiltonian, i.e., $\energy_1 \leq \energy_i$, for all $i \in
            \brac{1, \dots, N}$.
            From whence, we find
            \begin{align}
                E[\psi]
                &= \sum_{i = 1}^{N}
                \abs{c_i}^2 \energy_i
                \geq
                \energy_1 \sum_{i = 1}^{N}
                \abs{c_i}^{2}
                = \energy_1,
            \end{align}
            which shows that $\energy_1$ serves as a lower bound for the energy
            of the system under observation.
        \end{proof}

        \subsection{The variational method}
            The variational principle tells us that the energy we find from a
            trial wave function $\ket{\psi}$ will be an upper bound to the
            ground state energy, but it does not provide us with a method of
            finding the \emph{best} upper bound energy.
            Given a Hermitian model Hamiltonian $\hamil$ and a normalized trial
            wave function $\ket{\psi}$ that is constructed as a linear
            combination of known normalized wave functions
            $\ket{\chi_{\alpha}}$, viz.
            \begin{align}
                \ket{\psi} = c_{\alpha}\ket{\chi_{\alpha}},
            \end{align}
            we construct the variational energy functional $E[\psi]$ by
            \begin{align}
                E[\psi]
                &= \mel{\psi}{\hamil}{\psi}.
            \end{align}
            Optimizing the energy functional is now done by finding the
            stationary points with respect to the variational parameters
            $c_{\alpha}$.
            We do this by
            \begin{align}
                \dpd[]{E[c_{\alpha}]}{c_{\beta}} &= 0,
                \\
                \dmd{E[c_{\alpha}]}{2}{c_{\beta}}{}{c_{\gamma}}{} &= 0,
            \end{align}
            where the first variation locates the stationary point and the
            second variation categorizes the point as the Hessian matrix.
            However, looking at the first variation we have
            \begin{align}
                \dpd{}{c_{\beta}}\mel{\psi}{\hamil}{\psi} = 0,
            \end{align}
            which is not solvable as the equations stand \cite{szabo1996modern}.
            In order to construct equations which can be optimized, we use
            Lagrange's method of undetermined multipliers to construct a new
            functional $G[\psi]$ given by \cite{helgaker-molecular,
            szabo1996modern}
            \begin{align}
                G[\psi]
                &=
                \mel{\psi}{\hamil}{\psi}
                - E[\psi] \para{\braket{\psi}{\psi} - 1},
            \end{align}
            where we've introduced the normalization condition for the trial
            wave function as a constraint.
            Optimizing this functional for the first variation we find
            \begin{align}
                \dpd[]{G[\psi]}{c_{\alpha}}
                &=
                \brak{
                    \dpd[]{}{c_{\alpha}}
                    \bra{\psi}
                }\hamil\ket{\psi}
                + \bra{\psi}\hamil
                \brak{
                    \dpd[]{}{c_{\beta}}
                    \ket{\psi}
                }
                -
                \braket{\psi}{\psi}
                \dpd[]{E[\psi]}{c_{\alpha}}
                \nonumber \\
                &\qquad
                -
                E[\psi]
                \brak{
                    \dpd[]{}{c_{\alpha}}
                    \bra{\psi}
                }\ket{\psi}
                -
                \bra{\psi}
                \brak{
                    \dpd[]{}{c_{\beta}}
                    \ket{\psi}
                }
                \\
                &=
                2 \mel{\chi_{\alpha}}{\hamil}{\psi}
                - 0
                - 2E[\psi]\braket{\chi_{\alpha}}{\psi}
                = 0,
            \end{align}
            where the first variation in the energy functional becomes zero due
            to the conditions imposed on the stationary point of the energy
            functional.
            Furthermore, due to the hermiticity of the Hamiltonian, we have
            collected equal terms.
            Inserting the expansion for the trial wave function on both sides,
            we find
            \begin{gather}
                \mel{\chi_{\alpha}}{\hamil}{\chi_{\beta}} c_{\beta}
                - E[\psi]\braket{\chi_{\alpha}}{\chi_{\beta}} c_{\beta}
                = 0
                \\
                \implies
                \hamilten_{\alpha\beta} c_{\beta}
                = E[\psi] S_{\alpha\beta} c_{\beta}
                \\
                \implies
                \hamilmat \vfg{c}
                = E[\psi] \overlapmat \vfg{c},
                \label{eq:variational-eigenvalue}
            \end{gather}
            where we've defined the overlap matrix $\overlapmat$ from the
            overlap integrals between the known basis elements
            $\ket{\chi_{\alpha}}$.
            We have now reduced the task of optimizing the energy functional to
            the problem of solving the generalized eigenvalue equation in
            \autoref{eq:variational-eigenvalue}.
            In other words, by creating the Hamiltonian matrix $\hamilmat$ and
            the overlap matrix $\overlapmat$ from the basis of known basis
            elements, we can diagonalize the matrices, i.e., solve the
            generalized eigenvalue equation, and extract the optimal eigenpair
            $(E, \vf{c})$ as the eigenvalue and eigenvector respectively.
            This procedure will be used extensively as a procedure to transform
            from a known to an unknown basis.
            See \citetitle{helgaker-molecular} \cite{helgaker-molecular} for a
            derivation of the Hessian elements.

    \section{The Hellmann-Feynman theorem}
        The Hellmann-Feynman theorem provides us with a method of calculating
        first-order change (also known as a first order property) in the energy
        due to a perturbation \cite{helgaker-molecular}.
        \begin{theorem}
            If $\ket{\psi}$ is a normalized eigenstate of the Hamiltonian
            $\hamil$, or $\ket{\psi}$ is variationally determined from the
            Hamiltonian $\hamil$, the Hellmann-Feynman theorem \cite{feynman}
            states that
            \begin{align}
                \left.\dod[]{E(\alpha)}{\alpha}\right\rvert_{\alpha = 0}
                =
                \left.
                \dpd[]{}{\alpha}
                \bra{\psi_{\alpha}}\hamil + \alpha\hat{V}\ket{\psi_{\alpha}}
                \right\rvert_{\alpha = 0}
                = \bra{\psi}\hat{V}\ket{\psi},
                \label{eq:hellmann-feynman}
            \end{align}
            where $\alpha$ is a perturbational parameter and $\hat{V}$ the
            perturbation operator.
            The wave function $\ket{\psi_{\alpha}}$ is given by
            \begin{align}
                \ket{\psi_{\alpha}} = N(\ket{\psi} + \alpha\ket{\delta\psi}),
            \end{align}
            where $N$ is a normalization factor.
            For approximate wave functions, they must be optimized with respect
            to the same variational parameter $\alpha$ as in the theorem.
        \end{theorem}
        \begin{proof}
            The underlying assumption is that
            \begin{align}
                \hamil\ket{\psi} = E\ket{\psi},
            \end{align}
            regardless if we have an exact or a variationally determined wave
            function $\ket{\psi}$.
            Furthermore, both perturbed and unperturbed wave functions are
            normalized, viz.
            \begin{align}
                \braket{\psi}{\psi} = \braket{\psi_{\alpha}}{\psi_{\alpha}} = 1
                \implies
                \dpd[]{}{\alpha} \braket{\psi_{\alpha}}{\psi_{\alpha}} = 0.
            \end{align}
            See reference \cite{helgaker-molecular} for a proof where the
            normalization of the perturbed wave function is relaxed.
            We now prove \autoref{eq:hellmann-feynman} directly.
            \begin{align}
                \left.\dod[]{E(\alpha)}{\alpha}\right\rvert_{\alpha = 0}
                &=
                \left.
                \dpd[]{}{\alpha}
                \bra{\psi_{\alpha}}\hamil + \alpha\hat{V}\ket{\psi_{\alpha}}
                \right\rvert_{\alpha = 0}
                \\
                &=
                \bra{\delta\psi}\hamil\ket{\psi}
                + \bra{\psi}\hat{V}\ket{\psi}
                + \bra{\psi}\hamil\ket{\delta\psi}
                \\
                &=
                \energy \brak{
                    \braket{\delta\psi}{\psi}
                    + \braket{\psi}{\delta\psi}
                }
                + \bra{\psi}\hat{V}\ket{\psi}
                \\
                &=
                \energy \dpd[]{}{\alpha}\braket{\psi}{\psi}
                + \bra{\psi}\hat{V}\ket{\psi}
                \\
                &=
                \mel{\psi}{\hat{V}}{\psi},
            \end{align}
            which is what we wanted to show.
        \end{proof}
        A consequence of the Hellmann-Feynman theorem is that it provides us
        with a technique for computing expectation values of other quantities
        than the energy once we have variatonally determined the optimal state
        for a given system.
        For example, the expectation value of the operator $\hat{O}$ can be
        found by
        \begin{align}
            \expv{O}
            = \mel{\psi}{\hat{O}}{\psi}
            = \dpd[]{}{\alpha}
            \left.
            \mel{\psi_{\alpha}}{\hamil + \alpha\hat{O}}{\psi_{\alpha}}
            \right\rvert_{\alpha = 0}
            =
            \left.
            \dpd[]{E(\alpha)}{\alpha}
            \right\rvert_{\alpha = 0}.
        \end{align}
        This avoids the need of having to variationally determine every
        expectation value that we wish to measure as finding the optimal
        parameters for the energy is enough.

    \section{The time-dependent Schrödinger equation}
        Moving to the time domain, the dynamics of an isolated quantum system is
        described by the Schrödinger equation
        \begin{align}
            i\hslash \dpd{}{t}\ket{\psi(t)}
            = \hamil(t)\ket{\psi(t)}.
            \label{eq:tdse}
        \end{align}
        Given $\ket{\psi(t_0)}$ the time-dependent Schrödinger equation will
        determine $\ket{\psi(t)}$ for all earlier and later times.
        A point to note is that the time-dependent Schrödinger equation does not
        require that the initial state $\ket{\psi(t)}$ be a ``meaningfull
        state'', the equation will evolve this state -- whatever it is -- in
        time governed by the time-dependent Hamiltonian $\hamil(t)$.
        This is an important point in the sense that solving the time-dependent
        Schrödinger equation for a specific Hamiltonian can prove a great
        challenge.
        The approach that we take in this thesis is to choose an initial state,
        most often the approximation we have for the ground state of the
        time-independent Schrödinger equation, and evolve this state in time.
        This can be thought of as ``preparing'' a system in the ground state and
        then ``disturbing'' the system with an external interaction which
        triggers a reaction in the dynamics of the system.

    \section{The time evolution operators}
        %We will in the following subsection stay close to the derivation of
        %\citeauthor{ullrich2011time} \cite{ullrich2011time}, section 3.1.2, and
        %\citeauthor{joachain2012atoms} \cite{joachain2012atoms}, section 5.1.
        Any solution to the time-dependent Schrödinger equation is given by
        \begin{align}
            \ket{\psi(t)} = \hat{U}(t, t_0)\ket{\psi(t_0)},
        \end{align}
        where $\hat{U}(t, t_0)$ is the time evolution operator that acts on the
        initial state $\ket{\psi(t_0)}$, and yields the state $\ket{\psi(t)}$ at
        some other time $t$.
        The time evolution operators are unitary at common times, viz.
        \begin{align}
            \hat{U}^{\dagger}(t, t_0)\hat{U}(t, t_0) = \1.
        \end{align}
        This means that we can go ``backwards in time'' in the sense that going
        from $\ket{\psi(t)} \to \ket{\psi(t_0)}$ is done by
        \begin{align}
            \ket{\psi(t_0)} = \hat{U}^{\dagger}(t, t_0)\ket{\psi(t)}.
        \end{align}
        Furthermore, we can compose a time evolution operator from other time
        evolution operators as
        \begin{align}
            \hat{U}(t_2, t_0) = \hat{U}(t_2, t_1)\hat{U}(t_1, t_0).
        \end{align}
        One way to realise this is that we are allowed to have intermediate
        states between two points in time $t_2$ and $t_0$.
        This seemingly benign result is important for our numerical time
        propagation schemes as we make use of this property to move between
        two time points using many small intermediate steps, where smaller steps
        in general yields lower errors.
        The time evolution operator can be found by solving the time-dependent
        Schrödinger equation
        \begin{gather}
            i\dpd{}{t}\hat{U}(t, t_0) = \hamil(t)\hat{U}(t, t_0),
        \end{gather}
        where we've inserted $\ket{\psi(t)} = \hat{U}(t, t_0)\ket{\psi(t_0)}$
        into \autoref{eq:tdse} and removed the initial state $\ket{\psi(t_0)}$
        as it is independent of the time-derivative.
        Now, if the Hamiltonian is time-independent, i.e., $\hat{H}(t) =
        \hat{H}$, the time evolution operator takes on the closed form solution
        \begin{align}
            \hat{U}(t, t_0) = \exp\brac{
                \frac{-i \hamil}{\hslash} (t - t_0)
            }.
            \label{eq:ti-evolution}
        \end{align}
        Assuming we have found the spectrum of our time-independent Hamiltonian,
        where $(E_n, \psi_n)$ are the eigenpairs, and we use this as our initial
        state $\ket{\psi(t_0)} = \ket{\psi_n}$, we find that
        \begin{gather}
            \ket{\psi_n(t)}
            = \exp{\frac{-i\hamil}{\hslash}(t - t_0)}\ket{\psi_n}
            = \exp{\frac{-i E_n}{\hslash} (t - t_0)}\ket{\psi_n},
        \end{gather}
        where $\ket{\psi_n(t)}$ is a \emph{stationary state}.
        This means that all expectation values are stationary in time as can be
        seen from
        \begin{align}
            \expv{O(t)}
            &= \mel{\psi_n(t)}{\hat{O}}{\psi_n(t)}
            = \mel{\psi_n}{\hat{O}}{\psi_n}
            = \expv{O},
        \end{align}
        where the time dependence cancel.
        However, a general time-dependent state $\ket{\psi(t)}$ can be written
        as a linear combination of the spectrum of stationary states.
        \begin{align}
            \ket{\psi(t)} = \sum_{n = 1}^{\infty}c_n(t)\ket{\psi_n},
        \end{align}
        where we've absorved the time-dependency into the coefficients and
        require that the coefficients squared sum up to unity.
        This means that even though the Hamiltonian is time-independent, the
        states themselves are not stationary.
        Now, all observables that commute with the time-independent Hamiltonian
        will still be time-indepedent for $\ket{\psi(t)}$.
        We can see this by looking at a time-independent observable $\hat{Q}$
        which commute with the time-independent Hamiltonian such that the
        eigenstate of the Hamiltonian will be eigenstates of $\hat{Q}$ with
        eigenvalues $q_n$.
        We then have
        \begin{align}
            \expv{Q(t)}
            &= \mel{\psi(t)}{\hat{Q}}{\psi(t)}
            = \sum_{n, m = 1}^{\infty} c^{*}_n(t) c_m(t)
            \mel{\psi_n}{\hat{Q}}{\psi_m}
            \\
            &= \sum_{n, m = 1}^{\infty}
            c^{*}_n(t) c_m(t) q_n \delta_{nm}
            = \sum_{n = 1}^{\infty}
            \abs{c_n(t)}^2 q_n
            = \sum_{n = 1}^{\infty}
            q_n,
        \end{align}
        which shows that the observables that commute with the time-independent
        Hamiltonian are time-independent.


        In the case of a time-dependent Hamiltonian, the time evolution operator
        takes on a more complicated shape.
        \begin{align}
            \hat{U}(t, t_0) =
            \mathcal{T}\exp\brac{
                -\frac{i}{\hslash} \int_{t_0}^{t} \dd\tau
                \hat{H}(\tau)
            },
            \label{eq:td-evolution}
        \end{align}
        where $\mathcal{T}$ is the time-ordering operator.
        As an extra complicating factor, the time-dependent Hamiltonian might
        not commute with itself at different times.
        The time-evolution operator shown in \autoref{eq:td-evolution} serve as
        a theoretical foundation for the time-evolution.
        However, we will see later in the thesis how can approximate this
        operator using known numerical integration methods.

    \section{The time-dependent variational principle}

    \section{Electrodynamics}
        In vacuum the classical electromagnetic field is described by Maxwell's
        equations without sources.
        \begin{gather}
            \vfg{\nabla}\cdot\vfg{E} = 0, \\
            \vfg{\nabla}\cdot\vfg{B} = 0, \\
            \vfg{\nabla}\times\vfg{E} = -\dpd{}{t}\vfg{B}, \\
            \vfg{\nabla}\times\vfg{B} = \frac{1}{c^2}\dpd{}{t}\vfg{E},
        \end{gather}
        where $\vfg{E}(\vfg{r}, t)$ and $\vfg{B}(\vfg{r}, t)$ are the
        electric and magnetic fields respectively.
        Without sources translates to the charge density $\rho$ and the current
        density $\vfg{j}$ being zero.
        The electric and magnetic fields can be described by the scalar and
        vector potentials $\phi(\vfg{r}, t)$ and $\vfg{A}(\vfg{r}, t)$,
        respectively, by the relations
        \begin{gather}
            \vfg{E} = -\vfg{\nabla}\phi - \dpd{}{t}\vfg{A}, \\
            \vfg{B} = \vfg{\nabla}\times\vfg{A}.
        \end{gather}
        In addition to Maxwell's equations, the potentials and the electric and
        magnetic fields satisfies the homogeneous wave equation
        \cite{joachain2012atoms}
        \begin{align}
            \vfg{\nabla}^2\vfg{A} = \frac{1}{c^2}\dpd[2]{}{t}\vfg{A}.
            \label{eq:wave-equation}
        \end{align}
        % TODO: This might be a result of the Coulomb gauge condition...
        Maxwell's equations and the wave equation does not uniquely define the
        scalar and vector potentials as the electric and magnetic field are
        invariant under the gauge transformations
        \begin{gather}
            \vfg{A} \to \vfg{A}' = \vfg{A} + \vfg{\nabla} f, \\
            \phi \to \phi' = \phi + \dpd{f}{t},
        \end{gather}
        where $f$ is a differentiable, real function of $\vfg{r}$ and $t$.
        To go from here we choose a gauge fixing condition such that we are able
        to remove non-physical degrees of freedom \cite{modern-qm}.
        We will strictly be working in the non-relativistic limit such that the
        Lorenz gauge is unnecessary.
        We will be using the Coulomb gauge given by
        \begin{align}
            \vfg{\nabla}\cdot\vfg{A} = 0.
        \end{align}
        Without sources, this means that $\phi = 0$ and the electric and
        magnetic fields are found by
        \begin{gather}
            \vfg{E} = -\dpd{}{t}\vfg{A}, \\
            \vfg{B} = \vfg{\nabla}\times \vfg{A}.
        \end{gather}

        A solution to the wave equation in \autoref{eq:wave-equation} for the
        vector potential $\vfg{A}$ is the monochromatic plane wave solutions
        with wave number $\vfg{k} = 2\pi / \lambda$, and angular frequency
        $\omega_k = \abs{\vfg{k}} c$.
        That is,
        \begin{align}
            \vfg{A}(\vfg{r}, t)
            &= \vfg{A}_0 \exp[i(\vfg{k}\cdot \vfg{r} - \omega_k t)]
            + \vfg{A}^{*}_0 \exp[-i(\vfg{k}\cdot\vfg{r} - \omega_k t)],
            \label{eq:plane-wave}
        \end{align}
        where the first term represents the positive frequency solution and the
        second term the negative frequency solution.
        The amplitudes $\vfg{A}_0$ are given by
        \begin{align}
            \vfg{A}_0 = \vfg{\epsilon} A_0,
        \end{align}
        where $\vfg{\epsilon}$ is the polarization vector.
        The Coulomb gauge condition is satisfied if
        \begin{align}
            \vfg{k}\cdot \vfg{\epsilon} = 0,
        \end{align}
        that is, the wave is transversal and the polarization is perpendicular
        to the propagation direction.
        Now, \autoref{eq:plane-wave} describes a classical plane wave solution
        to the wave equation.
        It is possible to quantize the plane wave solution and introduce Fock
        states for the electromagnetic vector potential.
        The laser fields we will be working with will have such a high
        photoncount that the classical description of the field will dominate
        over quantum fluctuations arising from quantization of the fields
        \cite{joachain2012atoms}.
        An alternative is to describe the radiation from the laser field as a
        coherent state, which will allow us to use the quantum mechanical
        description of the electromagnetic field \cite{joachain2012atoms,
        modern-qm}.
        However, we wil in the following remain in the classical description of
        the electromagnetic fields yielding a semi-classical approach to
        describing interactions between particles and electromagnetic fields.

        \subsection{Electromagnetic field and particle interactions}
            In a quantum description of the electromagnetic field, we have the
            full Hamiltonian of a particle in a potential $v(\vfg{r})$ given by
            \begin{align}
                \hamil
                = \hamil_{p} + \hamil_{e} + \hamil_{ep},
            \end{align}
            where we've denoted the particle (or, matter) contribution by
            $\hamil_p$, the electromagnetic contribution by $\hamil_e$ and
            finally the interaction between particles and the electromagnetic
            field by $\hamil_{ep}$.
            % TODO: Discuss that we ignore interaction between charges in the
            % Coulomb interactin.

        \subsection{The dipole approximation}
            If we assume that the wavelength $\lambda$ of the field is larger
            than the size of the quantum system, that is, if
            \begin{align}
                \lambda \gg \abs{\vfg{r}}
                \implies
                \vfg{k}\cdot\vfg{r} \ll 1,
            \end{align}
            the spatial variations of the electromagnetic field will be small
            compared to the time variations.
            When looking at atomic particles with sizes on the order of
            nanometers, this approximation can be good when the electromagnetic
            field is in the low frequency domain, e.g., infrared light, etc.
            Looking at the plane wave solution in \autoref{eq:plane-wave} we see
            that
            \begin{align}
                \exp[\pm i\vfg{k}\cdot\vfg{r}]
                = 1 \pm i\vfg{k}\cdot\vfg{r}
                - \frac{1}{2}\para{
                    \vfg{k}\cdot\vfg{r}
                }^2
                + \dots,
            \end{align}
            where the first term will dominate.
            The \emph{dipole approximation} consists of choosing this term as
            the only contribution to the spatial variations, i.e.,
            \begin{align}
                \exp[\pm i\vfg{k}\cdot\vfg{r}] \approx 1.
            \end{align}
            This means that $\vfg{A}(\vfg{r}, t) \approx \vfg{A}(t)$ and we get
            a spatially uniform electromagnetic field.
            In the Coulomb gauge this means that
            \begin{gather}
                \vfg{E} = -\dod{}{t}\vfg{A}, \\
                \vfg{B} = \vfg{\nabla}\times \vfg{A} = \vfg{0}.
            \end{gather}
        \subsection{Selection rules}

    \section{Laser field}
        \subsection{Envelope}
            % TODO: Add plots of the envelope functions.
        \subsection{Dipole laser}

    \section{Physical units}
        \subsection{Atomic units}
